{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# hw4pr1iris_cleaner:  data-cleaning for iris modeling and classification\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# We don't need any data at all to create a predictive model!\n",
    "#\n",
    "import random\n",
    "\n",
    "def predictive_model( Features ):\n",
    "    \"\"\" input: a list of four features \n",
    "                [ sepallen, sepalwid, petallen, petalwid ]\n",
    "        output: the predicted species of iris, from\n",
    "                  setosa (0), versicolor (1), virginica (2)\n",
    "    \"\"\"\n",
    "    [ sepallen, sepalwid, petallen, petalwid ] = Features # unpacking!\n",
    "    \n",
    "    if petalwid < 1.0:\n",
    "        return 'setosa (0)'\n",
    "    else:\n",
    "        return random.choice( ['versicolor (1)', 'virginica (2)'] )\n",
    "    \n",
    "#\n",
    "# Try it!\n",
    "# \n",
    "# Features = eval(input(\"Enter new Features: \"))\n",
    "#\n",
    "Features = [ 4.6, 3.6, 3.0, 0.2 ] \n",
    "result = predictive_model( Features )\n",
    "print(f\"I predict {result} from Features {Features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# (now, to explore how we _can_ use data to do better... :-) \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries!\n",
    "import numpy as np      # numpy is Python's \"array\" library\n",
    "import pandas as pd     # Pandas is Python's \"data\" library (\"dataframe\" == spreadsheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's read in our flower data...\n",
    "# \n",
    "# for read_csv, use header=0 when row 0 is a header row\n",
    "# \n",
    "filename = 'iris.csv'\n",
    "df = pd.read_csv(filename, header=0)   # encoding=\"latin1\" et al.\n",
    "print(f\"{filename} : file read into a pandas dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# a dataframe is a \"spreadsheet in Python\"   (seems to have an extra column!)\n",
    "#\n",
    "pd.set_option('display.max_rows', 10)  # None for no limit; default: 10\n",
    "pd.set_option('display.min_rows', 10)  # None for no limit; default: 10\n",
    "# let's view it!\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# let's look at our pandas dataframe   (Aargh: that extra column!)\n",
    "#\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# let's drop that last column (dropping is usually by _name_):\n",
    "#\n",
    "#   if you want a list of the column names use df.columns\n",
    "name_to_drop = df.columns[5]  # get column name at index 5\n",
    "print(f\"dropping {name_to_drop}\")\n",
    "df_clean = df.drop(columns=[name_to_drop])  # drop by name is typical\n",
    "df_clean.info()                         # should be happier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# let's keep our column names in variables, for reference\n",
    "#\n",
    "COLUMNS = df_clean.columns            # \"list\" of columns\n",
    "print(f\"COLUMNS is {COLUMNS}\\n\")  \n",
    "  # It's a \"pandas\" list, called an Index\n",
    "  # use it just as a Python list of strings:\n",
    "print(f\"COLUMNS[0] is {COLUMNS[0]}\\n\")\n",
    "\n",
    "# let's create a dictionary to look up any column index by name\n",
    "COL_INDEX = {}\n",
    "for i, name in enumerate(COLUMNS):\n",
    "    COL_INDEX[name] = i  # using the name (as key), look up the value (i)\n",
    "print(f\"COL_INDEX is {COL_INDEX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# let's look at our cleaned-up dataframe...\n",
    "#\n",
    "df_clean.info()   \n",
    "#\n",
    "# notice that the non-null is _different_ for irisname!\n",
    "df_clean   # show a table! (the problem rows are the last two...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# typically, after dropping columns we don't want, \n",
    "#   we drop rows with missing data (other approaches are possible, too)\n",
    "#\n",
    "df_full = df_clean.dropna()   # this removes all rows with nan items\n",
    "df_full.info()                # it's \"full\" because it has no nan items\n",
    "df_full\n",
    "#\n",
    "# notice that _all_ of the rows now have 142 non-null items\n",
    "#    also, the last row isn't real data... we'll handle it next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# get rid of last 2 rows!\n",
    "#\n",
    "df_final = df_full.iloc[0:-2]   # not the syntax I would choose\n",
    "# careful:  don't run this again!\n",
    "print(df_final.shape)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# all of scikit-learn's ML routines need numbers, not strings\n",
    "#   ... even for categories/classifications (like species!)\n",
    "#   so, we will convert the flower-species to numbers:\n",
    "\n",
    "SPECIES = ['setosa','versicolor','virginica']   # int to str\n",
    "SPECIES_INDEX = {'setosa':0,'versicolor':1,'virginica':2}  # str to int\n",
    "\n",
    "def convert_species(speciesname):\n",
    "    \"\"\" return the species index (a unique integer/category) \"\"\"\n",
    "    #print(f\"converting {speciesname}...\")\n",
    "    return SPECIES_INDEX[speciesname]\n",
    "\n",
    "# Let's try it out...\n",
    "for name in SPECIES:\n",
    "    print(f\"{name} maps to {convert_species(name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# we can \"apply\" to a whole column\n",
    "#   it may give a warning, but this is ok...\n",
    "#\n",
    "\n",
    "df_final['irisname_numeric'] = df_final['irisname'].apply(convert_species)\n",
    "\n",
    "# Don't run this twice!   Why?!  What's \"KeyError: 0\"?\n",
    "#   (for sure, you can always go back and re-establish definitions)\n",
    "\n",
    "# don't worry about the (possible)  \"SettingWithCopyWarning\" here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# let's see it!  (this is safe to run many times...)\n",
    "#\n",
    "df_final         # print(df_final.tostring())  # for _all_ rows..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tidy = df_final.drop(columns=[\"irisname\"])  # drop the named species\n",
    "df_tidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# That's it!  Let's write it to \n",
    "cleaned_filename = filename[:-4] + \"_cleaned.csv\"  # name-creating\n",
    "print(f\"cleaned_filename is {cleaned_filename}\")\n",
    "\n",
    "# Now, save\n",
    "df_tidy.to_csv(cleaned_filename, index_label=False)  # no \"index\" column..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Let's make sure this worked!\n",
    "#\n",
    "\n",
    "# let's read in our flower data...\n",
    "# \n",
    "# for read_csv, use header=0 when row 0 is a header row\n",
    "# \n",
    "df_tidy = pd.read_csv(cleaned_filename, header=0)   # encoding=\"utf-8\" et al.\n",
    "print(f\"{filename} : file read into a pandas dataframe.\")\n",
    "df_tidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Let's make sure we have all of our helpful variables in one place \n",
    "#      (for the next file, the modeler! ...)\n",
    "#\n",
    "\n",
    "#\n",
    "# let's keep our column names in variables, for reference\n",
    "#\n",
    "COLUMNS = df_tidy.columns            # \"list\" of columns\n",
    "print(f\"COLUMNS is {COLUMNS}\\n\")  \n",
    "  # It's a \"pandas\" list, called an Index\n",
    "  # use it just as a Python list of strings:\n",
    "print(f\"COLUMNS[0] is {COLUMNS[0]}\\n\")\n",
    "\n",
    "# let's create a dictionary to look up any column index by name\n",
    "COL_INDEX = {}\n",
    "for i, name in enumerate(COLUMNS):\n",
    "    COL_INDEX[name] = i  # using the name (as key), look up the value (i)\n",
    "print(f\"COL_INDEX is {COL_INDEX}\\n\\n\")\n",
    "\n",
    "\n",
    "#\n",
    "# and our species names\n",
    "#\n",
    "\n",
    "\n",
    "# all of scikit-learn's ML routines need numbers, not strings\n",
    "#   ... even for categories/classifications (like species!)\n",
    "#   so, we will convert the flower-species to numbers:\n",
    "\n",
    "SPECIES = ['setosa','versicolor','virginica']   # int to str\n",
    "SPECIES_INDEX = {'setosa':0,'versicolor':1,'virginica':2}  # str to int\n",
    "\n",
    "def convert_species(speciesname):\n",
    "    \"\"\" return the species index (a unique integer/category) \"\"\"\n",
    "    #print(f\"converting {speciesname}...\")\n",
    "    return SPECIES_INDEX[speciesname]\n",
    "\n",
    "# Let's try it out...\n",
    "for name in SPECIES:\n",
    "    print(f\"{name} maps to {convert_species(name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# now, we're starting to model...\n",
    "#\n",
    "\n",
    "#\n",
    "# So this cell - and the two cells above -- should be in the next notebook!\n",
    "#\n",
    "\n",
    "#\n",
    "# let's convert our dataframe to a numpy array, named A\n",
    "#    Our ML library, scikit-learn operates entirely on numpy arrays.\n",
    "#\n",
    "A = df_more_final.values    # .values gets the numpy array\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# That's it!  Welcome to the world of data-cleaning workflows!!    \n",
    "#\n",
    "#             Our prediction?  You'll be headed to the \"modeler\" next! \n",
    "#\n",
    "\n",
    "#\n",
    "# And, the rest of the hw is to run more ML workflows:   Digits, Titanic, Housing, ...\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
