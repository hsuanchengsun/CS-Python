{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# hw4pr1iris:  iris clasification via nearest neighbors\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# We don't need any data at all to create a predictive model!\n",
    "#\n",
    "import random\n",
    "\n",
    "def predictive_model( Features ):\n",
    "    \"\"\" input: a list of four features \n",
    "                [ sepallen, sepalwid, petallen, petalwid ]\n",
    "        output: the predicted species of iris, from\n",
    "                  setosa (0), versicolor (1), virginica (2)\n",
    "    \"\"\"\n",
    "    [ sepallen, sepalwid, petallen, petalwid ] = Features # unpacking!\n",
    "    if petalwid < 1.0:\n",
    "        return 'setosa (0)'\n",
    "    else:\n",
    "        return random.choice( ['versicolor (1)', 'virginica (2)'] )\n",
    "    \n",
    "#\n",
    "# Try it!\n",
    "# \n",
    "# Features = eval(input(\"Enter new Features: \"))\n",
    "#\n",
    "Features = [ 4.6, 3.6, 3.0, 0.2 ] \n",
    "result = predictive_model( Features )\n",
    "print(f\"I predict {result} from Features {Features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# (now, to explore how we _can_ use data to do better... :-) \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries!\n",
    "import numpy as np      # numpy is Python's \"array\" library\n",
    "import pandas as pd     # Pandas is Python's \"data\" library (\"dataframe\" == spreadsheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's read in our flower data...\n",
    "# \n",
    "# for read_csv, use header=0 when row 0 is a header row\n",
    "# \n",
    "filename = 'iris.csv'\n",
    "df = pd.read_csv(filename, header=0)   # encoding=\"latin1\" et al.\n",
    "print(f\"{filename} : file read into a pandas dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# a dataframe is a \"spreadsheet in Python\"   (seems to have an extra column!)\n",
    "#\n",
    "pd.set_option('display.max_rows', 10)  # None for no limit; default: 10\n",
    "pd.set_option('display.min_rows', 10)  # None for no limit; default: 10\n",
    "# let's view it!\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# let's look at our pandas dataframe   (Aargh: that extra column!)\n",
    "#\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# let's drop that last column (dropping is usually by _name_):\n",
    "#\n",
    "#   if you want a list of the column names use df.columns\n",
    "col5name = df.columns[5]  # get column name at index 5\n",
    "df_clean = df.drop(columns=[col5name])  # drop by name is typical\n",
    "df_clean.info()                         # should be happier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# let's keep our column names in variables, for reference\n",
    "#\n",
    "COLUMNS = df_clean.columns            # \"list\" of columns\n",
    "print(f\"COLUMNS is {COLUMNS}\\n\")  \n",
    "  # It's a \"pandas\" list, called an Index\n",
    "  # use it just as a Python list of strings:\n",
    "print(f\"COLUMNS[0] is {COLUMNS[0]}\\n\")\n",
    "\n",
    "# let's create a dictionary to look up any column index by name\n",
    "COL_INDEX = {}\n",
    "for i, name in enumerate(COLUMNS):\n",
    "    COL_INDEX[name] = i  # using the name (as key), look up the value (i)\n",
    "print(f\"COL_INDEX is {COL_INDEX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# let's look at our cleaned-up dataframe...\n",
    "#\n",
    "df_clean.info()   \n",
    "#\n",
    "# notice that the non-null is _different_ for irisname!\n",
    "df_clean   # show a table! (the problem rows are the last two...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# typically, after dropping columns we don't want, \n",
    "#   we drop rows with missing data (other approaches are possible, too)\n",
    "#\n",
    "df_full = df_clean.dropna()   # this removes all rows with nan items\n",
    "df_full.info()                # it's \"full\" because it has no nan items\n",
    "df_full\n",
    "#\n",
    "# notice that _all_ of the rows now have 142 non-null items\n",
    "#    also, the last row isn't real data... we'll handle it next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# get rid of last row!\n",
    "#\n",
    "df_final = df_full.iloc[0:-1]   # not the syntax I would choose\n",
    "# careful:  don't run this again!\n",
    "print(df_final.shape)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all of scikit-learn's ML routines need numbers, not strings\n",
    "#   ... even for categories/classifications (like species!)\n",
    "#   so, we will convert the flower-species to numbers:\n",
    "\n",
    "SPECIES = ['setosa','versicolor','virginica']   # int to str\n",
    "SPECIES_INDEX = {'setosa':0,'versicolor':1,'virginica':2}  # str to int\n",
    "\n",
    "def convert_species(speciesname):\n",
    "    \"\"\" return the species index (a unique integer/category) \"\"\"\n",
    "    #print(f\"converting {speciesname}...\")\n",
    "    return SPECIES_INDEX[speciesname]\n",
    "\n",
    "# Let's try it out...\n",
    "for name in SPECIES:\n",
    "    print(f\"{name} maps to {convert_species(name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# we can \"apply\" to a whole column\n",
    "#   it may give a warning, but this is ok...\n",
    "#\n",
    "\n",
    "df_final['irisname'] = df_final['irisname'].apply(convert_species)\n",
    "\n",
    "# Don't run this twice!   Why?!  What's \"KeyError: 0\"?\n",
    "#   (for sure, you can always go back and re-establish definitions)\n",
    "\n",
    "# don't worry about the (possible)  \"SettingWithCopyWarning\" here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# let's see it!  (this is safe to run many times...)\n",
    "#\n",
    "df_final         # print(df_final.tostring())  # for _all_ rows..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# let's convert our dataframe to a numpy array, named A\n",
    "#    Our ML library, scikit-learn operates entirely on numpy arrays.\n",
    "#\n",
    "A = df_final.values    # .values gets the numpy array\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# let's make sure it's all floating-point, so we can multiply and divide\n",
    "#\n",
    "A = A.astype('float64')  # so many:  www.tutorialspoint.com/numpy/numpy_data_types.htm\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# nice to have NUM_ROWS and NUM_COLS around\n",
    "#\n",
    "NUM_ROWS, NUM_COLS = A.shape\n",
    "print(f\"\\nThe dataset has {NUM_ROWS} rows and {NUM_COLS} cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's use all of our variables, to reinforce names...\n",
    "\n",
    "# choose a row index, n:\n",
    "n = 132\n",
    "print(f\"flower #{n} is {A[n]}\")\n",
    "\n",
    "for i in range(len(COLUMNS)):\n",
    "    colname = COLUMNS[i]\n",
    "    if colname != 'irisname':\n",
    "        print(f\"  Its {colname} is {A[n][i]}\")\n",
    "    else:\n",
    "        species_num = int(A[n][i])\n",
    "        species = SPECIES[species_num]\n",
    "        print(f\"  Its {colname} is {species} ({species_num})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# We don't have to use scikit-learn to implement n.n.!\n",
    "#\n",
    "\n",
    "#\n",
    "# data-driven predictive model (1-nearest-neighbor)\n",
    "#\n",
    "dist = np.linalg.norm  # built in to numpy\n",
    "NUM_ROWS, NUM_COLS = A.shape  # data size\n",
    "\n",
    "def predictive_model( Features ):\n",
    "    \"\"\" input: a list of four features \n",
    "                [ sepallen, sepalwid, petallen, petalwid ]\n",
    "        output: the predicted species of iris, from\n",
    "                  setosa (0), versicolor (1), virginica (2)\n",
    "    \"\"\"\n",
    "    our_features = np.asarray(Features)   # make a numpy array\n",
    "    \n",
    "    closest_flower   = A[0]\n",
    "    closest_features = A[0,0:4]         \n",
    "    closest_distance = dist(our_features-closest_features)\n",
    "    \n",
    "    for i in range(NUM_ROWS):\n",
    "        current_flower   = A[i]\n",
    "        current_features = A[i,0:4] \n",
    "        current_distance = dist(our_features-current_features)\n",
    "        \n",
    "        if current_distance < closest_distance:\n",
    "            closest_distance = current_distance  # remember closest!\n",
    "            closest_flower = current_flower\n",
    "    \n",
    "    # done comparing with every flower in the dataset\n",
    "    predicted_species = int(round(closest_flower[4]))\n",
    "    name = SPECIES[predicted_species]\n",
    "    return f\"{name} ({predicted_species})\"\n",
    "    \n",
    "#\n",
    "# Try it!\n",
    "# \n",
    "# Features = eval(input(\"Enter new Features: \"))\n",
    "#\n",
    "Features = [ 4.6, 3.6, 3.0, 1.2 ] \n",
    "result = predictive_model( Features )\n",
    "print(f\"I predict {result} from Features {Features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# but, we don't have to write our own ... because\n",
    "#\n",
    "#     we want knn for any k!\n",
    "#     we want an already-debugged algorithm!\n",
    "#     we want to ask iris q'ns instead of implementation ones... (?)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"+++ Start of data definitions +++\\n\")\n",
    "\n",
    "X_all = A[:,0:4]  # X (features) ... is all rows, columns 0, 1, 2, 3\n",
    "y_all = A[:,4]    # y (labels) ... is all rows, column 4 only\n",
    "\n",
    "print(f\"X_all (just features) is \\n {X_all}\")\n",
    "print(f\"y_all (just labels)   is \\n {y_all}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# we can re-weight different features here...\n",
    "#\n",
    "\n",
    "COL_WEIGHT = {              # could be called Feature weight...\n",
    "    'sepallen':1.0,\n",
    "    'sepalwid':1.0,\n",
    "    'petallen':1.0,\n",
    "    'petalwid':1.0,\n",
    "}\n",
    "\n",
    "for colname in COL_WEIGHT:\n",
    "    i = COL_INDEX[colname]    # get the column index, i, of the colname\n",
    "    weight = COL_WEIGHT[colname]  # from the dictionary above\n",
    "    print(\"Weighting\", colname, \"by\", weight)   \n",
    "    # weighting == \"multiplying\"\n",
    "    X_all[:,i] *= weight   # multiply by the weight to give this column (\"feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# we scramble the data, to give a different TRAIN/TEST split each time...\n",
    "# \n",
    "indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "# we scramble both X and y, necessarily with the same permutation\n",
    "X_labeled = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "y_labeled = y_all[indices]              # again...\n",
    "print(X_labeled)\n",
    "print(y_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# We next separate into test data and training data ... \n",
    "#    + We will train on the training data...\n",
    "#    + We will _not_ look at the testing data to build the model\n",
    "#\n",
    "# Then, afterward, we will test on the testing data -- and see how well we do!\n",
    "#\n",
    "\n",
    "#\n",
    "# a common convention:  train on 80%, test on 20%    Let's define the TEST_PERCENT\n",
    "#\n",
    "NUM_ROWS = X_labeled.shape[0]     # the number of labeled rows\n",
    "TEST_PERCENT = 0.20\n",
    "TEST_SIZE = int(TEST_PERCENT*NUM_ROWS)   # no harm in rounding down\n",
    "\n",
    "X_test = X_labeled[:TEST_SIZE]    # first section are for testing\n",
    "y_test = y_labeled[:TEST_SIZE]\n",
    "\n",
    "X_train = X_labeled[TEST_SIZE:]   # all the rest are for training\n",
    "y_train = y_labeled[TEST_SIZE:]\n",
    "\n",
    "print(f\"training with {len(y_train)} rows;  testing with {len(y_test)} rows\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# +++ This is the \"Model-building and Model-training Cell\"\n",
    "#       \n",
    "# Create a kNN model and train it! \n",
    "#\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k = 84   # we don't know what k to use, so we guess!  (this will _not_ be a good value)\n",
    "knn_model = KNeighborsClassifier(n_neighbors=k)       # here, k is the \"k\" in kNN\n",
    "\n",
    "# we train the model (it's one line!)\n",
    "knn_model.fit(X_train, y_train)                              # yay!  trained!\n",
    "print(\"Created and trained a knn classifier with k =\", k)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# +++ This is the \"Model-testing Cell\"\n",
    "#\n",
    "# Now, let's see how well we did on our \"held-out data\" (the testing data)\n",
    "#\n",
    "\n",
    "# We run our test set!\n",
    "predicted_labels = knn_model.predict(X_test)\n",
    "actual_labels = y_test\n",
    "\n",
    "# Let's print them so we can compare...\n",
    "print(\"Predicted labels:\", predicted_labels)\n",
    "print(\"Actual  labels  :\", actual_labels)\n",
    "\n",
    "# And, some overall results\n",
    "num_correct = sum(predicted_labels == actual_labels)\n",
    "total = len(actual_labels)\n",
    "print(f\"\\nResults on test set:  {num_correct} correct out of {total} total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Let's print these more helpfully, in a vertical table\n",
    "#\n",
    "\n",
    "def compare_labels(predicted_labels, actual_labels):\n",
    "    \"\"\" a more neatly formatted comparison \"\"\"\n",
    "    NUM_LABELS = len(predicted_labels)\n",
    "    num_correct = 0\n",
    "    \n",
    "    for i in range(NUM_LABELS):\n",
    "        p = int(round(predicted_labels[i]))         # round protects from fp error \n",
    "        a = int(round(actual_labels[i]))\n",
    "        result = \"incorrect\"\n",
    "        if p == a:  # if they match,\n",
    "            result = \"\"       # no longer incorrect\n",
    "            num_correct += 1  # and we count a match!\n",
    "\n",
    "        print(f\"row {i:>3d} : {SPECIES[p]:>12s} {SPECIES[a]:<12s}   {result}\")   \n",
    "\n",
    "    print()\n",
    "    print(\"Correct:\", num_correct, \"out of\", NUM_LABELS)\n",
    "    return num_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# let's try it out!\n",
    "#\n",
    "\n",
    "compare_labels(predicted_labels,actual_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Ok!  We have our knn model, we could just use it...\n",
    "#\n",
    "\n",
    "#\n",
    "# data-driven predictive model (k-nearest-neighbor), using scikit-learn\n",
    "#\n",
    "\n",
    "def predictive_model( Features ):\n",
    "    \"\"\" input: a list of four features \n",
    "                [ sepallen, sepalwid, petallen, petalwid ]\n",
    "        output: the predicted species of iris, from\n",
    "                  setosa (0), versicolor (1), virginica (2)\n",
    "    \"\"\"\n",
    "    our_features = np.asarray([Features])                 # extra brackets needed\n",
    "    predicted_species = knn_model.predict(our_features)\n",
    "    \n",
    "    predicted_species = int(round(predicted_species[0]))  # unpack one element\n",
    "    name = SPECIES[predicted_species]\n",
    "    return f\"{name} ({predicted_species})\"\n",
    "    \n",
    "#\n",
    "# Try it!\n",
    "# \n",
    "# Features = eval(input(\"Enter new Features: \"))\n",
    "#\n",
    "Features = [6.7,3.3,5.7,2.1]  # [5.8,2.7,4.1,1.0] [4.6,3.6,3.0,2.2] [6.7,3.3,5.7,2.1]\n",
    "result = predictive_model( Features )\n",
    "print(f\"I predict {result} from Features {Features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Except, we didn't really explore whether this was the BEST model we could build!\n",
    "#\n",
    "#\n",
    "# We used k = 84  (a neighborhood size of 84 flowers)\n",
    "# In a dataset of only 140ish flowers, with three species, this seems like a bad idea!\n",
    "#\n",
    "# Perhaps we should try ALL the neighborhood sizes in their own TRAIN/TEST split\n",
    "# and see which neighborhood size works the best, for irises, at least...\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# to do this, we use \"cross validation\"\n",
    "#\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#\n",
    "# cross-validation splits the training set into two pieces:\n",
    "#   + model-building and model-validation. We'll use \"build\" and \"validate\"\n",
    "#\n",
    "for k in range(1,85):\n",
    "    knn_cv_model = KNeighborsClassifier(n_neighbors=k)   # build knn_model for every k!\n",
    "    cv_scores = cross_val_score( knn_cv_model, X_train, y_train, cv=5 )  # 5 means 80/20 split\n",
    "    # print(cv_scores)  # just to see the five scores... \n",
    "    average_cv_accuracy = cv_scores.mean()  # mean() is numpy's built-in average function \n",
    "    print(f\"k: {k:2d}  cv accuracy: {average_cv_accuracy:7.4f}\")\n",
    "\n",
    "    \n",
    "    \n",
    "# assign best value of k to best_k\n",
    "best_k = k      # at the moment this is incorrect   TO DO for hw4pr1: fix this...\n",
    "# you'll need to use the loop above to find and remember the real best_k\n",
    "\n",
    "print(f\"best_k = {best_k}   yields the highest average cv accuracy.\")  # print the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Now, we re-create and re-run the  \"Model-building and -training Cell\"\n",
    "#\n",
    "# Now, using best_k instead of the original, randomly-guessed value    How does it do?!\n",
    "#\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_model_tuned = KNeighborsClassifier(n_neighbors=best_k)   # here, we use the best_k\n",
    "\n",
    "# we train the model (one line!)\n",
    "knn_model_tuned.fit(X_train, y_train)                              # yay!  trained!\n",
    "print(f\"Created + trained a knn classifier, now tuned with a (best) k of {best_k}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Re-create and re-run the  \"Model-testing Cell\"     How does it do with best_k?!\n",
    "#\n",
    "predicted_labels = knn_model_tuned.predict(X_test)\n",
    "actual_labels = y_test\n",
    "\n",
    "# Let's print them so we can compare...\n",
    "print(\"Predicted labels:\", predicted_labels)\n",
    "print(\"Actual labels:\", actual_labels)\n",
    "print()\n",
    "# and, we'll print our nicer table...\n",
    "compare_labels(predicted_labels,actual_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Ok!  We have tuned knn to use the \"best\" value of k...\n",
    "#\n",
    "# And, we should really use ALL available data to train our final predictive model:\n",
    "#\n",
    "\n",
    "knn_model_final = KNeighborsClassifier(n_neighbors=best_k)   # here, we use the best_k\n",
    "knn_model_final.fit(X_all, y_all)                              # yay!  trained!\n",
    "print(f\"Created + trained a 'final' knn classifier, with a (best) k of {best_k}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# final predictive model (k-nearest-neighbor), with tuned k + ALL data incorporated\n",
    "#\n",
    "\n",
    "def predictive_model( Features ):\n",
    "    \"\"\" input: a list of four features \n",
    "                [ sepallen, sepalwid, petallen, petalwid ]\n",
    "        output: the predicted species of iris, from\n",
    "                  setosa (0), versicolor (1), virginica (2)\n",
    "    \"\"\"\n",
    "    our_features = np.asarray([Features])                 # extra brackets needed\n",
    "    predicted_species = knn_model_final.predict(our_features)\n",
    "    \n",
    "    predicted_species = int(round(predicted_species[0]))  # unpack one element\n",
    "    name = SPECIES[predicted_species]\n",
    "    return f\"{name} ({predicted_species})\"\n",
    "    \n",
    "#\n",
    "# Try it!\n",
    "# \n",
    "# Features = eval(input(\"Enter new Features: \"))\n",
    "#\n",
    "Features = [6.7,3.3,5.7,2.1]  # [5.8,2.7,4.1,1.0] [4.6,3.6,3.0,2.2] [6.7,3.3,5.7,2.1]\n",
    "result = predictive_model( Features )\n",
    "print(f\"I predict {result} from Features {Features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# try it on new data!  (grab it from the problem statement)\n",
    "#\n",
    "#\n",
    "# TO DO for hw4pr1:\n",
    "#       write a loop that will handle _multiple_ new flowers and predict their species...\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Be sure your results from trying this on the unknown data are here - or above!\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# That's it!  Welcome to the world of model-building workflows!!    \n",
    "#\n",
    "#             Our prediction?  We'll be back for more ML! \n",
    "#\n",
    "\n",
    "#\n",
    "# In fact, the rest of the hw is to run more ML workflows:   Digits, Titanic, Housing, ...\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
