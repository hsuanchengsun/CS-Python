{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "####  Modeling with both proximity and conditionals:  Neural Nets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   We demonstrate _both_ clasification + regression for bitwise functions:\n",
    "\n",
    "#     + function #1:  MAJ, the \"majority\" function\n",
    "#                     three bits input, the most-appearing bit is the output \n",
    "\n",
    "#     + function #2:  XOR, the \"xor\" or \"odd # of 1's\" function \n",
    "#                     three bits input, output is their sum%2 \n",
    "#                     that is, 1 if there is an odd # of 1's, 0 if an even # of 1's\n",
    "#   \n",
    "#   From here, we'll use NNets for the births and iris datasets\n",
    "#     + births is complete\n",
    "#     + iris has been started.  Your task: to complete its analysis.\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries!\n",
    "import numpy as np      # numpy is Python's \"array\" library\n",
    "import pandas as pd     # Pandas is Python's \"data\" library (\"dataframe\" == spreadsheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maj_cleaned.csv : file read into a pandas dataframe.\n"
     ]
    }
   ],
   "source": [
    "# let's read in our data...\n",
    "# \n",
    "# for read_csv, use header=0 when row 0 is a header row\n",
    "# \n",
    "filename = 'xor_cleaned.csv'\n",
    "filename = 'maj_cleaned.csv'\n",
    "df = pd.read_csv(filename, header=0)   # encoding=\"latin1\" et al.\n",
    "print(f\"{filename} : file read into a pandas dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8 entries, 0 to 7\n",
      "Data columns (total 4 columns):\n",
      "bit1         8 non-null int64\n",
      "bit2         8 non-null int64\n",
      "bit3         8 non-null int64\n",
      "outputbit    8 non-null int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 384.0 bytes\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's look at our pandas dataframe  \n",
    "#\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bit1</th>\n",
       "      <th>bit2</th>\n",
       "      <th>bit3</th>\n",
       "      <th>outputbit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bit1  bit2  bit3  outputbit\n",
       "0     0     0     0          0\n",
       "1     0     0     1          0\n",
       "2     0     1     0          0\n",
       "3     0     1     1          1\n",
       "4     1     0     0          0\n",
       "5     1     0     1          1\n",
       "6     1     1     0          1\n",
       "7     1     1     1          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMNS is Index(['bit1', 'bit2', 'bit3', 'outputbit'], dtype='object')\n",
      "\n",
      "COLUMNS[0] is bit1\n",
      "\n",
      "COL_INDEX is {'bit1': 0, 'bit2': 1, 'bit3': 2, 'outputbit': 3}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's keep our column names in variables, for reference\n",
    "#\n",
    "COLUMNS = df.columns            # \"list\" of columns\n",
    "print(f\"COLUMNS is {COLUMNS}\\n\")  \n",
    "  # It's a \"pandas\" list, called an Index\n",
    "  # use it just as a Python list of strings:\n",
    "print(f\"COLUMNS[0] is {COLUMNS[0]}\\n\")\n",
    "\n",
    "# let's create a dictionary to look up any column index by name\n",
    "COL_INDEX = {}\n",
    "for i, name in enumerate(COLUMNS):\n",
    "    COL_INDEX[name] = i  # using the name (as key), look up the value (i)\n",
    "print(f\"COL_INDEX is {COL_INDEX}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero maps to 0\n",
      "one maps to 1\n"
     ]
    }
   ],
   "source": [
    "# all of scikit-learn's ML routines need numbers, not strings\n",
    "#   ... even for categories/classifications (like species!)\n",
    "#   so, we will convert the flower-species to numbers:\n",
    "\n",
    "SPECIES = ['zero','one']   # int to str\n",
    "SPECIES_INDEX = {'zero':0,'one':1}  # str to int\n",
    "\n",
    "def convert_species(speciesname):\n",
    "    \"\"\" return the species index (a unique integer/category) \"\"\"\n",
    "    #print(f\"converting {speciesname}...\")\n",
    "    return SPECIES_INDEX[speciesname]\n",
    "\n",
    "# Let's try it out...\n",
    "for name in SPECIES:\n",
    "    print(f\"{name} maps to {convert_species(name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 1. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 1. 1.]\n",
      " [1. 1. 0. 1.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's convert our dataframe to a numpy array, named A\n",
    "#    Our ML library, scikit-learn operates entirely on numpy arrays.\n",
    "#\n",
    "A = df.to_numpy()    \n",
    "A = A.astype('float64')   # and make things floating-point\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 1. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 1. 1.]\n",
      " [1. 1. 0. 1.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# This is a small table of data... \n",
    "# \n",
    "# Alternaively, we could just define any bitwise function, by hand! :-)\n",
    "#\n",
    "\n",
    "\n",
    "# print(\"+++ Start of MAJ (majority) example +++\\n\")\n",
    "\n",
    "# A = np.asarray( [ \n",
    "#                     [0,0,0,  0],  # three input bits, one output bit (MAJ)\n",
    "#                     [0,0,1,  0],   \n",
    "#                     [0,1,0,  0],  \n",
    "#                     [0,1,1,  1],   \n",
    "#                     [1,0,0,  0],  \n",
    "#                     [1,0,1,  1],   \n",
    "#                     [1,1,0,  1],  \n",
    "#                     [1,1,1,  1],\n",
    "#                 ])\n",
    "\n",
    "\n",
    "# print(\"+++ Start of XOR (exclusive or == odd #of 1's) example +++\\n\")\n",
    "\n",
    "# A = np.asarray( [ \n",
    "#                     [0,0,0,  0],  # three input bits, one output bit (XOR) odd # of 1's\n",
    "#                     [0,0,1,  1],   \n",
    "#                     [0,1,0,  1],  \n",
    "#                     [0,1,1,  0],   \n",
    "#                     [1,0,0,  1],  \n",
    "#                     [1,0,1,  0],   \n",
    "#                     [1,1,0,  0],  \n",
    "#                     [1,1,1,  1],\n",
    "#                 ])\n",
    "\n",
    "A = A.astype('float64')   # and make things floating-point\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            input  -> pred  des. \n",
      "        [0. 0. 0.] -> ?     0    \n",
      "        [0. 0. 1.] -> ?     0    \n",
      "        [0. 1. 0.] -> ?     0    \n",
      "        [0. 1. 1.] -> ?     1    \n",
      "        [1. 0. 0.] -> ?     0    \n",
      "        [1. 0. 1.] -> ?     1    \n",
      "        [1. 1. 0.] -> ?     1    \n",
      "        [1. 1. 1.] -> ?     1    \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Let's split into features and labels/categories\n",
    "#\n",
    "\n",
    "# Here, we call it X_def and y_def, because it's data \"from definition,\" not observation\n",
    "\n",
    "X_def = A[:,0:3].copy()   # We make a copy so we don't change A\n",
    "y_def = A[:,3].copy()\n",
    "\n",
    "def ascii_table(X,y):\n",
    "    \"\"\" print a table of binary inputs and outputs \"\"\"\n",
    "    print(f\"{'input ':>18s} -> {'pred':<5s} {'des.':<5s}\") \n",
    "    for i in range(len(y)):\n",
    "        print(f\"{X[i,:]!s:>18s} -> {'?':<5s} {y[i]:<5.0f}\")   # !s is str ...\n",
    "        \n",
    "ascii_table(X_def,y_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            input  -> pred  des. \n",
      "        [0. 0. 0.] -> ?     0    \n",
      "        [0. 0. 1.] -> ?     0    \n",
      "        [0. 1. 0.] -> ?     0    \n",
      "        [0. 1. 1.] -> ?     1    \n",
      "        [1. 0. 0.] -> ?     0    \n",
      "        [1. 0. 1.] -> ?     1    \n",
      "        [1. 1. 0.] -> ?     1    \n",
      "        [1. 1. 1.] -> ?     1    \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we can scramble the remaining data if we want to...\n",
    "# \n",
    "\n",
    "# Here, we switch to X_all (features, inputs) and y_all (output / species)\n",
    "#       to match our other modeling workflows\n",
    "\n",
    "SCRAMBLE = False\n",
    "if SCRAMBLE == True:\n",
    "    NUM_ROWS = len(y_def)\n",
    "    indices = np.random.permutation(NUM_ROWS)  # this scrambles the data each time\n",
    "    X_all = X_def[indices]\n",
    "    y_all = y_def[indices]\n",
    "else:\n",
    "    X_all = X_def  # don't scramble\n",
    "    y_all = y_def\n",
    "\n",
    "ascii_table(X_all,y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            input  -> pred  des. \n",
      "        [0. 0. 0.] -> ?     0    \n",
      "        [0. 0. 1.] -> ?     0    \n",
      "        [0. 1. 0.] -> ?     0    \n",
      "        [0. 1. 1.] -> ?     1    \n",
      "        [1. 0. 0.] -> ?     0    \n",
      "        [1. 0. 1.] -> ?     1    \n",
      "        [1. 1. 0.] -> ?     1    \n",
      "        [1. 1. 1.] -> ?     1    \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# here, we _cheat_ by letting the full dataset \n",
    "# be _both_ the training and testing sets.  (There are too few otherwise!)\n",
    "#\n",
    "X_train = X_all.copy()\n",
    "y_train = y_all.copy()\n",
    "\n",
    "X_test = X_all.copy()\n",
    "y_test = y_all.copy()\n",
    "\n",
    "ascii_table(X_train,y_train)    # same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            input  -> pred  des. \n",
      "     [-1. -1. -1.] -> ?     0    \n",
      "     [-1. -1.  1.] -> ?     0    \n",
      "     [-1.  1. -1.] -> ?     0    \n",
      "     [-1.  1.  1.] -> ?     1    \n",
      "     [ 1. -1. -1.] -> ?     0    \n",
      "     [ 1. -1.  1.] -> ?     1    \n",
      "     [ 1.  1. -1.] -> ?     1    \n",
      "        [1. 1. 1.] -> ?     1    \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# for NNets, it's important to keep the feature values near 0, say -1. to 1. or so\n",
    "#    This is done through the \"StandardScaler\" in scikit-learn\n",
    "# \n",
    "USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "# we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "if USE_SCALER == True:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)  # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "else:\n",
    "    # this one does no scaling!  We still create it to be consistent:\n",
    "    scaler = StandardScaler(copy=True, with_mean=False, with_std=False)\n",
    "    scaler.fit(X_train)  # still need to fit, though it does not change...\n",
    "\n",
    "scaler   # is now defined and ready to use...\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Here are our scaled training and testing sets:\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train) # scale!\n",
    "X_test_scaled = scaler.transform(X_test) # scale!\n",
    "\n",
    "y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "y_test_scaled = y_test  # not using the scaler\n",
    "    \n",
    "ascii_table(X_train_scaled,y_train_scaled)\n",
    "\n",
    "#\n",
    "# Note that the zeros have become -1's\n",
    "# and the 1's have stayed 1's\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "Iteration 1, loss = 0.44105959\n",
      "Iteration 2, loss = 0.40441556\n",
      "Iteration 3, loss = 0.35685680\n",
      "Iteration 4, loss = 0.30391629\n",
      "Iteration 5, loss = 0.25088272\n",
      "Iteration 6, loss = 0.20200191\n",
      "Iteration 7, loss = 0.15976763\n",
      "Iteration 8, loss = 0.12485218\n",
      "Iteration 9, loss = 0.09674138\n",
      "Iteration 10, loss = 0.07450450\n",
      "Iteration 11, loss = 0.05720233\n",
      "Iteration 12, loss = 0.04396376\n",
      "Iteration 13, loss = 0.03397792\n",
      "Iteration 14, loss = 0.02651418\n",
      "Iteration 15, loss = 0.02095357\n",
      "Iteration 16, loss = 0.01680234\n",
      "Iteration 17, loss = 0.01368433\n",
      "Iteration 18, loss = 0.01132145\n",
      "Iteration 19, loss = 0.00951173\n",
      "Iteration 20, loss = 0.00810964\n",
      "Iteration 21, loss = 0.00701045\n",
      "Iteration 22, loss = 0.00613857\n",
      "Iteration 23, loss = 0.00543909\n",
      "Iteration 24, loss = 0.00487178\n",
      "Iteration 25, loss = 0.00440694\n",
      "Iteration 26, loss = 0.00402236\n",
      "Iteration 27, loss = 0.00370131\n",
      "Iteration 28, loss = 0.00343103\n",
      "Iteration 29, loss = 0.00320172\n",
      "Iteration 30, loss = 0.00300574\n",
      "Iteration 31, loss = 0.00283710\n",
      "Iteration 32, loss = 0.00269108\n",
      "Iteration 33, loss = 0.00256389\n",
      "Iteration 34, loss = 0.00245248\n",
      "Iteration 35, loss = 0.00235439\n",
      "Iteration 36, loss = 0.00226759\n",
      "Iteration 37, loss = 0.00219044\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 38, loss = 0.00212155\n",
      "Iteration 39, loss = 0.00206276\n",
      "Iteration 40, loss = 0.00201110\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 41, loss = 0.00196555\n",
      "Iteration 42, loss = 0.00192576\n",
      "Iteration 43, loss = 0.00189070\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 44, loss = 0.00185972\n",
      "Iteration 45, loss = 0.00183239\n",
      "Iteration 46, loss = 0.00180818\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 47, loss = 0.00178671\n",
      "Iteration 48, loss = 0.00176765\n",
      "Iteration 49, loss = 0.00175070\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 50, loss = 0.00173561\n",
      "Iteration 51, loss = 0.00172216\n",
      "Iteration 52, loss = 0.00171017\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 53, loss = 0.00169946\n",
      "Iteration 54, loss = 0.00168989\n",
      "Iteration 55, loss = 0.00168133\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 56, loss = 0.00167367\n",
      "Iteration 57, loss = 0.00166682\n",
      "Iteration 58, loss = 0.00166068\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 59, loss = 0.00165517\n",
      "Iteration 60, loss = 0.00165024\n",
      "Iteration 61, loss = 0.00164581\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "The analog prediction error (the loss) is 0.0016458137134802593\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# import our NNet library (within scikit-learn)\n",
    "#\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#\n",
    "# Here's where you can change the number of layers, neurons, and other parameters:\n",
    "#\n",
    "nn_classifier = MLPClassifier(hidden_layer_sizes=(6,7),  # 3 input -> 6 -> 7 -> 1 output\n",
    "                    max_iter=500,      # how many times to train\n",
    "                    activation=\"tanh\", # the \"activation function\" input -> output\n",
    "                    solver='sgd',      # the algorithm for optimizing weights\n",
    "                    verbose=True,      # False to \"mute\" the training\n",
    "                    shuffle=True,      # reshuffle the training epochs?\n",
    "                    random_state=None, # set for reproduceability\n",
    "                    learning_rate_init=.1,       # learning rate: % of error to backprop\n",
    "                    learning_rate = 'adaptive')  # soften feedback as it converges\n",
    "\n",
    "# documentation:\n",
    "# scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html \n",
    "#     Try verbose / activation \"relu\" / other network sizes ...\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_classifier.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"\\n++++++++++  TRAINING:   end  +++++++++++++++\")\n",
    "print(f\"The analog prediction error (the loss) is {nn_classifier.loss_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            input  ->  pred   des. \n",
      "        [0. 0. 0.] ->   0      0      correct \n",
      "        [0. 0. 1.] ->   0      0      correct \n",
      "        [0. 1. 0.] ->   0      0      correct \n",
      "        [0. 1. 1.] ->   1      1      correct \n",
      "        [1. 0. 0.] ->   0      0      correct \n",
      "        [1. 0. 1.] ->   1      1      correct \n",
      "        [1. 1. 0.] ->   1      1      correct \n",
      "        [1. 1. 1.] ->   1      1      correct \n",
      "\n",
      "correct predictions: 8 out of 8\n",
      "\n",
      "\n",
      "+++++ parameters, weights, etc. +++++\n",
      "\n",
      "\n",
      "weights/coefficients:\n",
      "\n",
      "[[-0.08089331  0.52849942  0.70168487  0.85128627  0.89402718  0.32919981]\n",
      " [ 0.59594161  0.51640883 -0.24380113  0.79447509  0.91069355  0.73879972]\n",
      " [-0.61224498  0.75129231  0.73568727  0.91123375  0.96357882  0.40054251]]\n",
      "[[-0.26876569 -0.53246958  0.27720792 -0.40649254  0.1999074   0.49483574\n",
      "   0.12624405]\n",
      " [-0.41168771 -0.31666842 -0.35524665 -0.25967029  0.69546041  0.38830438\n",
      "  -0.96834342]\n",
      " [ 0.00480392 -0.74815599 -0.53079652 -0.27904107 -0.46070948 -0.21588615\n",
      "  -0.08493404]\n",
      " [-0.71825781 -0.32120327 -0.59003434 -0.58136246  0.82019378 -0.65982708\n",
      "  -0.66385463]\n",
      " [-0.65321966 -0.20207766 -1.21012129 -0.3701233   0.15255858 -0.24785653\n",
      "  -1.34575646]\n",
      " [-0.47063168 -0.75716028 -0.69041013  0.60936343  0.35531193 -0.1559655\n",
      "  -0.30956906]]\n",
      "[[-1.18216752]\n",
      " [-0.85609426]\n",
      " [-1.84795932]\n",
      " [-0.54759365]\n",
      " [ 0.64822489]\n",
      " [-0.40228987]\n",
      " [-2.18104338]]\n",
      "\n",
      "intercepts: [array([ 0.53570499, -0.0888445 ,  0.72818822,  0.08498794, -0.06720924,\n",
      "       -0.00951485]), array([-0.02319306,  0.4879504 ,  0.04273447,  0.13224146, -0.1553812 ,\n",
      "       -0.05063261, -0.11726084]), array([0.13295811])]\n",
      "\n",
      "all parameters: {'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (6, 7), 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_iter': 500, 'momentum': 0.9, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'sgd', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': True, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# what can we see about our nnet?\n",
    "#\n",
    "#\n",
    "# how did it do on the training data?   (It's the same as the testing data, in this case!)\n",
    "#\n",
    "\n",
    "#\n",
    "# which one do we want: classifier or regressor?\n",
    "#\n",
    "\n",
    "def ascii_table_for_classifier(Xsc,y,nn,scaler):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(Xsc)            # all predictions\n",
    "    prediction_probs = nn.predict_proba(Xsc) # all prediction probabilities\n",
    "    Xpr = scaler.inverse_transform(Xsc)      # Xpr is the \"X to print\": unscaled data!\n",
    "    # count correct\n",
    "    num_correct = 0\n",
    "    # printing\n",
    "    print(f\"{'input ':>18s} -> {'pred':^6s} {'des.':^6s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        pred_probs = prediction_probs[i,:]\n",
    "        desired = y[i]\n",
    "        if pred != desired: result = \"  incorrect: \" + str(pred_probs)\n",
    "        else: result = \"  correct\"; num_correct += 1\n",
    "        # Xpr = Xsc  # if you want to see the scaled versions\n",
    "        print(f\"{Xpr[i,:]!s:>18s} -> {pred:^6.0f} {desired:^6.0f} {result:^10s}\") \n",
    "    print(f\"\\ncorrect predictions: {num_correct} out of {len(y)}\")\n",
    "    \n",
    "\n",
    "\n",
    "#\n",
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_classifier(X_test_scaled,\n",
    "                           y_test_scaled,\n",
    "                           nn_classifier,\n",
    "                           scaler)   \n",
    "#\n",
    "# other things...\n",
    "#\n",
    "nn = nn_classifier  # less to type?\n",
    "print(\"\\n\\n+++++ parameters, weights, etc. +++++\\n\")\n",
    "print(f\"\\nweights/coefficients:\\n\")\n",
    "for wts in nn.coefs_:\n",
    "    print(wts)\n",
    "print(f\"\\nintercepts: {nn.intercepts_}\")\n",
    "print(f\"\\nall parameters: {nn.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input features are [1, 0, 1]\n",
      "nn.predict_proba ==  [[0.00179787 0.99820213]]\n",
      "prediction: [1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jason/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we have a predictive model!  Let's try it out...\n",
    "#\n",
    "\n",
    "def make_prediction( Features, nn, scaler ):\n",
    "    \"\"\" uses nn for predictions \"\"\"\n",
    "    print(\"input features are\", Features)\n",
    "    #  we make sure Features has the right shape (list-of-lists)\n",
    "    row = np.array( [Features] )  # makes an array-row\n",
    "    row = scaler.transform(row)   # scale according to scaler\n",
    "    print(\"nn.predict_proba == \", nn.predict_proba(row))   # probabilities of each\n",
    "    prediction = nn.predict(row)  # max!\n",
    "    return prediction\n",
    "    \n",
    "# our features -- note that the inputs don't have to be bits!\n",
    "Features = [ 1, 0, 1 ]      # whatever we'd like to test\n",
    "prediction = make_prediction(Features, nn_classifier, scaler)\n",
    "print(f\"prediction: {prediction}\")   # just takes the max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### From classification to regression\n",
    "+ NNets are more natural regressors than classifiers...\n",
    "+ That is, they naturally output continuous, floating-point values\n",
    "+ ... instead of a category or choice-among-labels.\n",
    "+ So, let's try to predict our binary function as a floating point output instead.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "Iteration 1, loss = 0.24078732\n",
      "Iteration 2, loss = 0.13522246\n",
      "Iteration 3, loss = 0.08314288\n",
      "Iteration 4, loss = 0.07133080\n",
      "Iteration 5, loss = 0.06724755\n",
      "Iteration 6, loss = 0.05450467\n",
      "Iteration 7, loss = 0.03702870\n",
      "Iteration 8, loss = 0.02422198\n",
      "Iteration 9, loss = 0.01971273\n",
      "Iteration 10, loss = 0.02068482\n",
      "Iteration 11, loss = 0.02252016\n",
      "Iteration 12, loss = 0.02252298\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 0.02051211\n",
      "Iteration 14, loss = 0.01993584\n",
      "Iteration 15, loss = 0.01910825\n",
      "Iteration 16, loss = 0.01806516\n",
      "Iteration 17, loss = 0.01686687\n",
      "Iteration 18, loss = 0.01558276\n",
      "Iteration 19, loss = 0.01428025\n",
      "Iteration 20, loss = 0.01301752\n",
      "Iteration 21, loss = 0.01183974\n",
      "Iteration 22, loss = 0.01077776\n",
      "Iteration 23, loss = 0.00984872\n",
      "Iteration 24, loss = 0.00905789\n",
      "Iteration 25, loss = 0.00840103\n",
      "Iteration 26, loss = 0.00786699\n",
      "Iteration 27, loss = 0.00744021\n",
      "Iteration 28, loss = 0.00710287\n",
      "Iteration 29, loss = 0.00683670\n",
      "Iteration 30, loss = 0.00662431\n",
      "Iteration 31, loss = 0.00645013\n",
      "Iteration 32, loss = 0.00630099\n",
      "Iteration 33, loss = 0.00616637\n",
      "Iteration 34, loss = 0.00603839\n",
      "Iteration 35, loss = 0.00591166\n",
      "Iteration 36, loss = 0.00578293\n",
      "Iteration 37, loss = 0.00565075\n",
      "Iteration 38, loss = 0.00551501\n",
      "Iteration 39, loss = 0.00537660\n",
      "Iteration 40, loss = 0.00523701\n",
      "Iteration 41, loss = 0.00509798\n",
      "Iteration 42, loss = 0.00496131\n",
      "Iteration 43, loss = 0.00482861\n",
      "Iteration 44, loss = 0.00470118\n",
      "Iteration 45, loss = 0.00457997\n",
      "Iteration 46, loss = 0.00446551\n",
      "Iteration 47, loss = 0.00435796\n",
      "Iteration 48, loss = 0.00425716\n",
      "Iteration 49, loss = 0.00416265\n",
      "Iteration 50, loss = 0.00407379\n",
      "Iteration 51, loss = 0.00398982\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 52, loss = 0.00390996\n",
      "Iteration 53, loss = 0.00384495\n",
      "Iteration 54, loss = 0.00378712\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 55, loss = 0.00373532\n",
      "Iteration 56, loss = 0.00369092\n",
      "Iteration 57, loss = 0.00365181\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 58, loss = 0.00361722\n",
      "Iteration 59, loss = 0.00358701\n",
      "Iteration 60, loss = 0.00356036\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 61, loss = 0.00353682\n",
      "Iteration 62, loss = 0.00351606\n",
      "Iteration 63, loss = 0.00349769\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 64, loss = 0.00348141\n",
      "Iteration 65, loss = 0.00346698\n",
      "Iteration 66, loss = 0.00345416\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 67, loss = 0.00344277\n",
      "Iteration 68, loss = 0.00343263\n",
      "Iteration 69, loss = 0.00342359\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 70, loss = 0.00341554\n",
      "Iteration 71, loss = 0.00340835\n",
      "Iteration 72, loss = 0.00340192\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "The (squared) prediction error (the loss) is 0.0034019241005944\n",
      "And, its square root: 0.05832601564134482\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# MLPRegressor predicts _floating-point_ outputs\n",
    "#\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "nn_regressor = MLPRegressor(hidden_layer_sizes=(6,7), \n",
    "                    max_iter=200,          # how many training epochs\n",
    "                    activation=\"tanh\",     # the activation function\n",
    "                    solver='sgd',          # the optimizer\n",
    "                    verbose=True,          # do we want to watch as it trains?\n",
    "                    shuffle=True,          # shuffle each epoch?\n",
    "                    random_state=None,     # use for reproducibility\n",
    "                    learning_rate_init=.1, # how much of each error to back-propagate\n",
    "                    learning_rate = 'adaptive')  # how to handle the learning_rate\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_regressor.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"++++++++++  TRAINING:   end  +++++++++++++++\")\n",
    "\n",
    "print(f\"The (squared) prediction error (the loss) is {nn_regressor.loss_}\")\n",
    "print(f\"And, its square root: {nn_regressor.loss_ ** 0.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            input  ->   pred    desr    absdiff  \n",
      "        [0. 0. 0.] ->  -0.090  +0.000    0.090   \n",
      "        [0. 0. 1.] ->  +0.050  +0.000    0.050   \n",
      "        [0. 1. 0.] ->  +0.038  +0.000    0.038   \n",
      "        [0. 1. 1.] ->  +0.910  +1.000    0.090   \n",
      "        [1. 0. 0.] ->  +0.039  +0.000    0.039   \n",
      "        [1. 0. 1.] ->  +0.963  +1.000    0.037   \n",
      "        [1. 1. 0.] ->  +0.954  +1.000    0.046   \n",
      "        [1. 1. 1.] ->  +1.167  +1.000    0.167   \n",
      "\n",
      "average abs error: 0.06982011385617114\n",
      "\n",
      "\n",
      "+++++ parameters, weights, etc. +++++\n",
      "\n",
      "\n",
      "weights/coefficients:\n",
      "\n",
      "[[ 0.63891575  0.17406195  0.68129526  0.53264087  0.11598044 -0.20226688]\n",
      " [ 0.679538   -0.76647803 -0.13154123 -0.76438126 -0.26852287 -0.09560018]\n",
      " [ 0.50994094 -0.04292635 -0.13266066 -0.22690533 -0.19834911 -0.75313677]]\n",
      "[[-0.87593371 -0.45195321  0.68037407 -0.53189539  0.47657602  0.03045833\n",
      "  -0.4899803 ]\n",
      " [ 0.24257548  0.26037806 -0.54383287  0.22580542 -0.28281168  0.54422811\n",
      "  -0.2705987 ]\n",
      " [-0.58852691 -0.52557957 -0.3429275   0.66368494  0.70732807  0.69786704\n",
      "   0.40465926]\n",
      " [ 0.41914508  0.03354939  0.16818474 -0.44615173 -0.37920414  0.12335768\n",
      "   0.1246365 ]\n",
      " [ 0.41945806 -0.30042831  0.60955297 -0.37620213  0.16276613  0.09081929\n",
      "   0.20108376]\n",
      " [ 0.24848075 -0.46351024 -0.25845615  0.34825382  0.42470591  0.20147632\n",
      "  -0.10543849]]\n",
      "[[-0.55396643]\n",
      " [ 0.14902712]\n",
      " [ 0.35261027]\n",
      " [-0.23006213]\n",
      " [-0.05404551]\n",
      " [ 0.29151755]\n",
      " [ 0.18279404]]\n",
      "\n",
      "intercepts: [array([ 0.12558938, -0.02214669,  0.1213287 ,  0.41496038,  0.28619962,\n",
      "        0.44310695]), array([ 0.00688002,  0.2483503 ,  0.03884268,  0.02139168, -0.43585963,\n",
      "       -0.10395421, -0.29150933]), array([0.49368768])]\n",
      "\n",
      "all parameters: {'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (6, 7), 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_iter': 200, 'momentum': 0.9, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'sgd', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': True, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# how did it do? now we're making progress (by regressing)\n",
    "#\n",
    "\n",
    "def ascii_table_for_regressor(Xsc,y,nn,scaler):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(Xsc) # all predictions\n",
    "    Xpr = scaler.inverse_transform(Xsc)  # Xpr is the \"X to print\": unscaled data!\n",
    "    # measure error\n",
    "    error = 0.0\n",
    "    # printing\n",
    "    print(f\"{'input ':>18s} ->  {'pred':^6s}  {'desr':^6s}  {'absdiff':^10s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        desired = y[i]\n",
    "        result = abs(desired - pred)\n",
    "        error += result\n",
    "        # Xpr = Xsc   # if you'd like to see the scaled values\n",
    "        print(f\"{Xpr[i,:]!s:>18s} ->  {pred:<+6.3f}  {desired:<+6.3f}  {result:^10.3f}\") \n",
    "    print(f\"\\naverage abs error: {error/len(y)}\")\n",
    "    \n",
    "#\n",
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_regressor(X_test_scaled,\n",
    "                          y_test_scaled,\n",
    "                          nn_regressor,\n",
    "                          scaler)   # this is our own f'n, above\n",
    "#\n",
    "# other things...\n",
    "#\n",
    "nn = nn_regressor  # less to type?\n",
    "print(\"\\n\\n+++++ parameters, weights, etc. +++++\\n\")\n",
    "print(f\"\\nweights/coefficients:\\n\")\n",
    "for wts in nn.coefs_:\n",
    "    print(wts)\n",
    "print(f\"\\nintercepts: {nn.intercepts_}\")\n",
    "print(f\"\\nall parameters: {nn.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input features are [1.0, 0.0, 1.0]\n",
      "nn.predict(row) ==  [0.96267974]\n",
      "prediction: [0.96267974]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# again, we have a predictive model, now a regressor.  Let's try it out...\n",
    "#\n",
    "\n",
    "def make_prediction( Features, nn, scaler ):\n",
    "    \"\"\" use a NNet regressor to make a prediction \"\"\"\n",
    "    print(\"input features are\", Features)\n",
    "    row = np.array( [Features] )  # a list-of-lists-style input is needed\n",
    "    row = scaler.transform(row)   # scale!\n",
    "    prediction = nn.predict(row)\n",
    "    print(\"nn.predict(row) == \", prediction)\n",
    "    return prediction\n",
    "    \n",
    "# our features\n",
    "Features = [ 1.0, 0.0, 1.0 ]\n",
    "prediction = make_prediction(Features, nn_regressor, scaler)\n",
    "print(f\"prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Welcome to NNets!  \n",
    "# \n",
    "# Onward to the births and iris data.... and beyond!\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
