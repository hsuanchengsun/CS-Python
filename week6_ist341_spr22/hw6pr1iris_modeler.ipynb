{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "####   hw6pr1iris_modeler  (IST341)\n",
    "+ iris clasification and regression via NNets\n",
    "+ [here is the piazza page](https://piazza.com/hmc/spring2022/ist341/resources)\n",
    "+ [here is the hw6 page, specifically](https://docs.google.com/document/d/1S-WDOmwD6hwhOjHHOzZDUAwr6Vw6L8Aotr5seyegMjE/edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# hw6pr1iris_modeler:  NNETS! \n",
    "#\n",
    "#   including _both_ clasification + regression for iris-species modeling\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries...\n",
    "import numpy as np      # numpy is Python's \"array\" library\n",
    "import pandas as pd     # Pandas is Python's \"data\" library (\"dataframe\" == spreadsheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris_cleaned.csv : file read into a pandas dataframe.\n"
     ]
    }
   ],
   "source": [
    "# let's read in our flower data...\n",
    "# \n",
    "# iris_cleaned.csv and hw4pr1iris_cleaner.ipynb should be in this folder\n",
    "# \n",
    "filename = 'iris_cleaned.csv'\n",
    "df_tidy = pd.read_csv(filename)      # encoding = \"utf-8\", \"latin1\"\n",
    "print(f\"{filename} : file read into a pandas dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_tidy.shape is (141, 6)\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 141 entries, 0 to 140\n",
      "Data columns (total 6 columns):\n",
      "sepallen    141 non-null float64\n",
      "sepalwid    141 non-null float64\n",
      "petallen    141 non-null float64\n",
      "petalwid    141 non-null float64\n",
      "irisname    141 non-null object\n",
      "irisnum     141 non-null int64\n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 7.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepallen</th>\n",
       "      <th>sepalwid</th>\n",
       "      <th>petallen</th>\n",
       "      <th>petalwid</th>\n",
       "      <th>irisname</th>\n",
       "      <th>irisnum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>virginica</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>virginica</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepallen  sepalwid  petallen  petalwid   irisname  irisnum\n",
       "0         4.6       3.6       1.0       0.2     setosa        0\n",
       "1         4.3       3.0       1.1       0.1     setosa        0\n",
       "2         5.0       3.2       1.2       0.2     setosa        0\n",
       "3         5.8       4.0       1.2       0.2     setosa        0\n",
       "4         4.4       3.0       1.3       0.2     setosa        0\n",
       "..        ...       ...       ...       ...        ...      ...\n",
       "136       7.9       3.8       6.4       2.0  virginica        2\n",
       "137       7.6       3.0       6.6       2.1  virginica        2\n",
       "138       7.7       3.8       6.7       2.2  virginica        2\n",
       "139       7.7       2.8       6.7       2.0  virginica        2\n",
       "140       7.7       2.6       6.9       2.3  virginica        2\n",
       "\n",
       "[141 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# different version vary on how to see all rows (adapt to suit your system!)\n",
    "#\n",
    "print(f\"df_tidy.shape is {df_tidy.shape}\\n\")\n",
    "df_tidy.info()  # prints column information\n",
    "\n",
    "# let's print the whole dataframe, too  (adapt # of lines, as desired)\n",
    "# pd.options.display.max_rows = 150   # None for no limit; default: 10\n",
    "# pd.options.display.min_rows = 150   # None for no limit; default: 10\n",
    "# pd.options.display.max_rows = 10   # None for no limit; default: 10\n",
    "# pd.options.display.min_rows = 10   # None for no limit; default: 10\n",
    "df_tidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepallen</th>\n",
       "      <th>sepalwid</th>\n",
       "      <th>petallen</th>\n",
       "      <th>petalwid</th>\n",
       "      <th>irisnum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepallen  sepalwid  petallen  petalwid  irisnum\n",
       "0         4.6       3.6       1.0       0.2        0\n",
       "1         4.3       3.0       1.1       0.1        0\n",
       "2         5.0       3.2       1.2       0.2        0\n",
       "3         5.8       4.0       1.2       0.2        0\n",
       "4         4.4       3.0       1.3       0.2        0\n",
       "..        ...       ...       ...       ...      ...\n",
       "136       7.9       3.8       6.4       2.0        2\n",
       "137       7.6       3.0       6.6       2.1        2\n",
       "138       7.7       3.8       6.7       2.2        2\n",
       "139       7.7       2.8       6.7       2.0        2\n",
       "140       7.7       2.6       6.9       2.3        2\n",
       "\n",
       "[141 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# All of the columns need to be numeric, we'll drop irisname\n",
    "ROW = 0\n",
    "COLUMN = 1\n",
    "df_model1 = df_tidy.drop( 'irisname', axis=COLUMN )\n",
    "df_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMNS is Index(['sepallen', 'sepalwid', 'petallen', 'petalwid', 'irisnum'], dtype='object')\n",
      "\n",
      "COLUMNS[0] is sepallen\n",
      "\n",
      "COL_INDEX is {'sepallen': 0, 'sepalwid': 1, 'petallen': 2, 'petalwid': 3, 'irisnum': 4}\n",
      "\n",
      "\n",
      "setosa maps to 0\n",
      "versicolor maps to 1\n",
      "virginica maps to 2\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# once we have all the columns we want, let's create an index of their names...\n",
    "\n",
    "#\n",
    "# Let's make sure we have all of our helpful variables in one place \n",
    "#       To be adapted if we drop/add more columns...\n",
    "#\n",
    "\n",
    "#\n",
    "# let's keep our column names in variables, for reference\n",
    "#\n",
    "COLUMNS = df_model1.columns            # \"list\" of columns\n",
    "print(f\"COLUMNS is {COLUMNS}\\n\")  \n",
    "  # It's a \"pandas\" list, called an Index\n",
    "  # use it just as a Python list of strings:\n",
    "print(f\"COLUMNS[0] is {COLUMNS[0]}\\n\")\n",
    "\n",
    "# let's create a dictionary to look up any column index by name\n",
    "COL_INDEX = {}\n",
    "for i, name in enumerate(COLUMNS):\n",
    "    COL_INDEX[name] = i  # using the name (as key), look up the value (i)\n",
    "print(f\"COL_INDEX is {COL_INDEX}\\n\\n\")\n",
    "\n",
    "\n",
    "#\n",
    "# and our \"species\" names\n",
    "#\n",
    "\n",
    "# all of scikit-learn's ML routines need numbers, not strings\n",
    "#   ... even for categories/classifications (like species!)\n",
    "#   so, we will convert the flower-species to numbers:\n",
    "\n",
    "SPECIES = ['setosa','versicolor','virginica']   # int to str\n",
    "SPECIES_INDEX = {'setosa':0,'versicolor':1,'virginica':2}  # str to int\n",
    "\n",
    "# Let's try it out...\n",
    "for name in SPECIES:\n",
    "    print(f\"{name} maps to {SPECIES_INDEX[name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# We _could_ reweight our columns...\n",
    "# For exmaple, if petalwid were \"worth\" 20x more than the others?\n",
    "# \n",
    "\n",
    "# df_model1['petalwid'] *= 20\n",
    "# df_model1\n",
    "\n",
    "#\n",
    "# But, with NNets, the goal is to have the network weight each feature itself!\n",
    "#\n",
    "#      So, let's see how it does:\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.6 3.6 1.  0.2 0. ]\n",
      " [4.3 3.  1.1 0.1 0. ]\n",
      " [5.  3.2 1.2 0.2 0. ]\n",
      " [5.8 4.  1.2 0.2 0. ]\n",
      " [4.4 3.  1.3 0.2 0. ]\n",
      " [4.4 3.2 1.3 0.2 0. ]\n",
      " [4.5 2.3 1.3 0.3 0. ]\n",
      " [4.7 3.2 1.3 0.2 0. ]\n",
      " [5.  3.5 1.3 0.3 0. ]\n",
      " [5.4 3.9 1.3 0.4 0. ]\n",
      " [5.5 3.5 1.3 0.2 0. ]\n",
      " [4.4 2.9 1.4 0.2 0. ]\n",
      " [4.6 3.4 1.4 0.3 0. ]\n",
      " [4.6 3.2 1.4 0.2 0. ]\n",
      " [4.8 3.  1.4 0.1 0. ]\n",
      " [4.8 3.  1.4 0.3 0. ]\n",
      " [4.9 3.  1.4 0.2 0. ]\n",
      " [5.  3.6 1.4 0.2 0. ]\n",
      " [5.  3.3 1.4 0.2 0. ]\n",
      " [5.1 3.5 1.4 0.2 0. ]\n",
      " [5.1 3.5 1.4 0.3 0. ]\n",
      " [5.2 3.4 1.4 0.2 0. ]\n",
      " [5.5 4.2 1.4 0.2 0. ]\n",
      " [4.6 3.1 1.5 0.2 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [5.  3.4 1.5 0.2 0. ]\n",
      " [5.1 3.8 1.5 0.3 0. ]\n",
      " [5.1 3.7 1.5 0.4 0. ]\n",
      " [5.1 3.4 1.5 0.2 0. ]\n",
      " [5.2 3.5 1.5 0.2 0. ]\n",
      " [5.3 3.7 1.5 0.2 0. ]\n",
      " [5.4 3.7 1.5 0.2 0. ]\n",
      " [5.7 4.4 1.5 0.4 0. ]\n",
      " [4.7 3.2 1.6 0.2 0. ]\n",
      " [4.8 3.4 1.6 0.2 0. ]\n",
      " [5.  3.  1.6 0.2 0. ]\n",
      " [5.  3.4 1.6 0.4 0. ]\n",
      " [5.  3.5 1.6 0.6 0. ]\n",
      " [5.1 3.8 1.6 0.2 0. ]\n",
      " [5.1 3.3 1.7 0.5 0. ]\n",
      " [5.4 3.9 1.7 0.4 0. ]\n",
      " [5.4 3.4 1.7 0.2 0. ]\n",
      " [5.7 3.8 1.7 0.3 0. ]\n",
      " [4.8 3.4 1.9 0.2 0. ]\n",
      " [5.1 3.8 1.9 0.4 0. ]\n",
      " [7.  3.2 4.7 1.4 1. ]\n",
      " [4.9 2.4 3.3 1.  1. ]\n",
      " [5.  2.3 3.3 1.  1. ]\n",
      " [5.  2.  3.5 1.  1. ]\n",
      " [5.7 2.6 3.5 1.  1. ]\n",
      " [5.6 2.9 3.6 1.3 1. ]\n",
      " [5.5 2.4 3.7 1.  1. ]\n",
      " [5.5 2.4 3.8 1.1 1. ]\n",
      " [5.2 2.7 3.9 1.4 1. ]\n",
      " [5.6 2.5 3.9 1.1 1. ]\n",
      " [5.8 2.7 3.9 1.2 1. ]\n",
      " [5.5 2.3 4.  1.3 1. ]\n",
      " [5.5 2.5 4.  1.3 1. ]\n",
      " [5.8 2.6 4.  1.2 1. ]\n",
      " [6.  2.2 4.  1.  1. ]\n",
      " [6.1 2.8 4.  1.3 1. ]\n",
      " [5.6 3.  4.1 1.3 1. ]\n",
      " [5.8 2.7 4.1 1.  1. ]\n",
      " [5.6 2.7 4.2 1.3 1. ]\n",
      " [5.7 3.  4.2 1.2 1. ]\n",
      " [5.9 3.  4.2 1.5 1. ]\n",
      " [6.4 2.9 4.3 1.3 1. ]\n",
      " [5.5 2.6 4.4 1.2 1. ]\n",
      " [6.3 2.3 4.4 1.3 1. ]\n",
      " [6.6 3.  4.4 1.4 1. ]\n",
      " [6.7 3.1 4.4 1.4 1. ]\n",
      " [5.4 3.  4.5 1.5 1. ]\n",
      " [5.6 3.  4.5 1.5 1. ]\n",
      " [5.7 2.8 4.5 1.3 1. ]\n",
      " [6.  2.9 4.5 1.5 1. ]\n",
      " [6.  3.4 4.5 1.6 1. ]\n",
      " [6.2 2.2 4.5 1.5 1. ]\n",
      " [6.4 3.2 4.5 1.5 1. ]\n",
      " [6.1 3.  4.6 1.4 1. ]\n",
      " [6.5 2.8 4.6 1.5 1. ]\n",
      " [6.6 2.9 4.6 1.3 1. ]\n",
      " [6.1 2.9 4.7 1.4 1. ]\n",
      " [6.1 2.8 4.7 1.2 1. ]\n",
      " [6.3 3.3 4.7 1.6 1. ]\n",
      " [6.7 3.1 4.7 1.5 1. ]\n",
      " [5.9 3.2 4.8 1.8 1. ]\n",
      " [6.8 2.8 4.8 1.4 1. ]\n",
      " [6.3 2.5 4.9 1.5 1. ]\n",
      " [6.9 3.1 4.9 1.5 1. ]\n",
      " [6.7 3.  5.  1.7 1. ]\n",
      " [6.  2.7 5.1 1.6 1. ]\n",
      " [4.9 2.5 4.5 1.7 2. ]\n",
      " [6.  3.  4.8 1.8 2. ]\n",
      " [6.2 2.8 4.8 1.8 2. ]\n",
      " [5.6 2.8 4.9 2.  2. ]\n",
      " [6.1 3.  4.9 1.8 2. ]\n",
      " [6.3 2.7 4.9 1.8 2. ]\n",
      " [5.7 2.5 5.  2.  2. ]\n",
      " [6.  2.2 5.  1.5 2. ]\n",
      " [6.3 2.5 5.  1.9 2. ]\n",
      " [5.8 2.8 5.1 2.4 2. ]\n",
      " [5.8 2.7 5.1 1.9 2. ]\n",
      " [5.9 3.  5.1 1.8 2. ]\n",
      " [6.3 2.8 5.1 1.5 2. ]\n",
      " [6.5 3.2 5.1 2.  2. ]\n",
      " [6.9 3.1 5.1 2.3 2. ]\n",
      " [6.5 3.  5.2 2.  2. ]\n",
      " [6.7 3.  5.2 2.3 2. ]\n",
      " [6.4 2.7 5.3 1.9 2. ]\n",
      " [6.4 3.2 5.3 2.3 2. ]\n",
      " [6.2 3.4 5.4 2.3 2. ]\n",
      " [6.9 3.1 5.4 2.1 2. ]\n",
      " [6.4 3.1 5.5 1.8 2. ]\n",
      " [6.5 3.  5.5 1.8 2. ]\n",
      " [6.8 3.  5.5 2.1 2. ]\n",
      " [6.1 2.6 5.6 1.4 2. ]\n",
      " [6.3 2.9 5.6 1.8 2. ]\n",
      " [6.3 3.4 5.6 2.4 2. ]\n",
      " [6.4 2.8 5.6 2.1 2. ]\n",
      " [6.4 2.8 5.6 2.2 2. ]\n",
      " [6.7 3.1 5.6 2.4 2. ]\n",
      " [6.7 3.3 5.7 2.1 2. ]\n",
      " [6.7 3.3 5.7 2.5 2. ]\n",
      " [6.9 3.2 5.7 2.3 2. ]\n",
      " [6.5 3.  5.8 2.2 2. ]\n",
      " [6.7 2.5 5.8 1.8 2. ]\n",
      " [7.2 3.  5.8 1.6 2. ]\n",
      " [6.8 3.2 5.9 2.3 2. ]\n",
      " [7.1 3.  5.9 2.1 2. ]\n",
      " [7.2 3.2 6.  1.8 2. ]\n",
      " [7.2 3.6 6.1 2.5 2. ]\n",
      " [7.4 2.8 6.1 1.9 2. ]\n",
      " [7.7 3.  6.1 2.3 2. ]\n",
      " [7.3 2.9 6.3 1.8 2. ]\n",
      " [7.9 3.8 6.4 2.  2. ]\n",
      " [7.6 3.  6.6 2.1 2. ]\n",
      " [7.7 3.8 6.7 2.2 2. ]\n",
      " [7.7 2.8 6.7 2.  2. ]\n",
      " [7.7 2.6 6.9 2.3 2. ]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's convert our dataframe to a numpy array, named A\n",
    "#\n",
    "A = df_model1.to_numpy()   \n",
    "A = A.astype('float64')    # many types:  www.tutorialspoint.com/numpy/numpy_data_types.htm\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The dataset has 141 rows and 5 cols\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# nice to have NUM_ROWS and NUM_COLS around\n",
    "#\n",
    "NUM_ROWS, NUM_COLS = A.shape\n",
    "print(f\"\\nThe dataset has {NUM_ROWS} rows and {NUM_COLS} cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flower #42 is [5.4 3.9 1.7 0.4 0. ]\n",
      "  Its sepallen is 5.4\n",
      "  Its sepalwid is 3.9\n",
      "  Its petallen is 1.7\n",
      "  Its petalwid is 0.4\n",
      "  Its irisnum is 0.0\n",
      "  Its species is setosa (i.e., 0)\n"
     ]
    }
   ],
   "source": [
    "# let's use all of our variables, to reinforce that we have\n",
    "# (1) names...\n",
    "# (2) access and control...\n",
    "\n",
    "# choose a row index, n:\n",
    "n = 42\n",
    "print(f\"flower #{n} is {A[n]}\")\n",
    "\n",
    "for i in range(len(COLUMNS)):\n",
    "    colname = COLUMNS[i]\n",
    "    value = A[n][i]\n",
    "    print(f\"  Its {colname} is {value}\")\n",
    "\n",
    "species_index = COL_INDEX['irisnum']\n",
    "species_num = int(round(A[n][species_index]))\n",
    "species = SPECIES[species_num]\n",
    "print(f\"  Its species is {species} (i.e., {species_num})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Start of data definitions +++\n",
      "\n",
      "y_all (just the labels/species)   are \n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      "X_all (just the features, first few rows) are \n",
      " [[4.6 3.6 1.  0.2]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [4.4 3.  1.3 0.2]]\n"
     ]
    }
   ],
   "source": [
    "print(\"+++ Start of data definitions +++\\n\")\n",
    "\n",
    "#\n",
    "# we could do this at the data-frame level, too!\n",
    "#\n",
    "\n",
    "X_all = A[:,0:4]  # X (features) ... is all rows, columns 0, 1, 2, 3\n",
    "y_all = A[:,4]    # y (labels) ... is all rows, column 4 only\n",
    "\n",
    "print(f\"y_all (just the labels/species)   are \\n {y_all}\")\n",
    "print(f\"X_all (just the features, first few rows) are \\n {X_all[0:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scrambled labels/species are \n",
      " [0. 2. 2. 1. 0. 1. 1. 2. 2. 1. 2. 0. 2. 2. 1. 0. 2. 1. 1. 0. 1. 1. 1. 2.\n",
      " 2. 2. 0. 0. 1. 0. 2. 1. 0. 1. 2. 0. 0. 0. 0. 1. 2. 2. 1. 2. 1. 0. 0. 2.\n",
      " 2. 2. 1. 2. 2. 0. 1. 1. 2. 2. 1. 2. 0. 0. 1. 1. 0. 1. 2. 0. 1. 1. 2. 1.\n",
      " 0. 0. 1. 2. 2. 0. 2. 1. 0. 1. 0. 2. 1. 1. 0. 2. 2. 1. 1. 1. 2. 0. 1. 1.\n",
      " 2. 2. 2. 2. 0. 0. 1. 2. 0. 0. 1. 2. 1. 0. 0. 0. 0. 2. 0. 2. 0. 0. 2. 2.\n",
      " 1. 0. 0. 0. 1. 0. 1. 0. 0. 2. 0. 2. 1. 2. 2. 1. 1. 0. 0. 2. 1.]\n",
      "The corresponding data rows are \n",
      " [[5.  3.5 1.3 0.3]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [4.8 3.4 1.6 0.2]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we can scramble the data, to remove (potential) dependence on its ordering: \n",
    "# \n",
    "indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "# we scramble both X and y, necessarily with the same permutation\n",
    "X_all = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "y_all = y_all[indices]              # again...\n",
    "print(f\"The scrambled labels/species are \\n {y_all}\")\n",
    "print(f\"The corresponding data rows are \\n {X_all[0:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with 112 rows;  testing with 29 rows\n",
      "\n",
      "Held-out data... (testing data: 29)\n",
      "y_test: [1. 0. 2. 0. 1. 1. 1. 1. 2. 1. 2. 1. 1. 0. 1. 2. 1. 2. 0. 0. 0. 1. 0. 1.\n",
      " 1. 1. 1. 2. 2.]\n",
      "\n",
      "X_test (few rows): [[6.  2.2 4.  1. ]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [6.2 2.2 4.5 1.5]]\n",
      "\n",
      "Data used for modeling... (training data: 112)\n",
      "y_train: [2. 1. 2. 0. 2. 1. 0. 2. 2. 1. 1. 2. 2. 0. 0. 1. 2. 0. 0. 0. 2. 2. 2. 2.\n",
      " 1. 0. 0. 0. 0. 0. 2. 1. 2. 2. 0. 1. 1. 0. 1. 2. 2. 1. 1. 0. 1. 1. 1. 1.\n",
      " 2. 2. 0. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 1. 2. 2. 0. 1. 2. 0. 1. 2.\n",
      " 0. 0. 2. 0. 2. 0. 0. 0. 2. 1. 1. 1. 1. 0. 0. 2. 2. 1. 2. 0. 0. 0. 2. 0.\n",
      " 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 2. 0. 1. 1. 2.]\n",
      "\n",
      "X_train (few rows): [[6.4 3.2 5.3 2.3]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [6.  3.  4.8 1.8]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We next separate into test data and training data ... \n",
    "#    + We will train on the training data...\n",
    "#    + We will _not_ look at the testing data to build the model\n",
    "#\n",
    "# Then, afterward, we will test on the testing data -- and see how well we do!\n",
    "#\n",
    "\n",
    "#\n",
    "# a common convention:  train on 80%, test on 20%    Let's define the TEST_PERCENT\n",
    "#\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
    "\n",
    "print(f\"training with {len(y_train)} rows;  testing with {len(y_test)} rows\\n\" )\n",
    "\n",
    "print(f\"Held-out data... (testing data: {len(y_test)})\")\n",
    "print(f\"y_test: {y_test}\\n\")\n",
    "print(f\"X_test (few rows): {X_test[0:5,:]}\")  # 5 rows\n",
    "print()\n",
    "print(f\"Data used for modeling... (training data: {len(y_train)})\")\n",
    "print(f\"y_train: {y_train}\\n\")\n",
    "print(f\"X_train (few rows): {X_train[0:5,:]}\")  # 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    input  -> pred  des. \n",
      "             [0.59884231 0.26836224 0.84179329 1.39711533] -> ?     2    \n",
      "         [-0.31213003 -0.4472704  -0.0873513   0.12287334] -> ?     1    \n",
      "             [1.50981466 1.22253908 1.2790378  1.65196373] -> ?     2    \n",
      "         [-1.67858855  0.26836224 -1.34442928 -1.27879286] -> ?     0    \n",
      "         [ 0.14335614 -0.20872618  0.56851547  0.75999433] -> ?     2    \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# for NNets, it's important to keep the feature values near 0, say -1. to 1. or so\n",
    "#    This is done through the \"StandardScaler\" in scikit-learn\n",
    "# \n",
    "USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "# we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "if USE_SCALER == True:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)  # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "else:\n",
    "    # this one does no scaling!  We still create it to be consistent:\n",
    "    scaler = StandardScaler(copy=True, with_mean=False, with_std=False) # no scaling\n",
    "    scaler.fit(X_train)  # still need to fit, though it does not change...\n",
    "\n",
    "scaler   # is now defined and ready to use...\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Here are our scaled training and testing sets:\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train) # scale!\n",
    "X_test_scaled = scaler.transform(X_test) # scale!\n",
    "\n",
    "y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "y_test_scaled = y_test  # not using the scaler\n",
    "\n",
    "def ascii_table(X,y):\n",
    "    \"\"\" print a table of binary inputs and outputs \"\"\"\n",
    "    print(f\"{'input ':>58s} -> {'pred':<5s} {'des.':<5s}\") \n",
    "    for i in range(len(y)):\n",
    "        print(f\"{X[i,:]!s:>58s} -> {'?':<5s} {y[i]:<5.0f}\")   # !s is str ...\n",
    "    \n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5])\n",
    "\n",
    "#\n",
    "# Note that the zeros have become -1's\n",
    "# and the 1's have stayed 1's\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "Iteration 1, loss = 1.16125435\n",
      "Iteration 2, loss = 0.92271363\n",
      "Iteration 3, loss = 0.71396140\n",
      "Iteration 4, loss = 0.57994621\n",
      "Iteration 5, loss = 0.50694908\n",
      "Iteration 6, loss = 0.46844630\n",
      "Iteration 7, loss = 0.44348897\n",
      "Iteration 8, loss = 0.41963898\n",
      "Iteration 9, loss = 0.39208828\n",
      "Iteration 10, loss = 0.36106477\n",
      "Iteration 11, loss = 0.32946945\n",
      "Iteration 12, loss = 0.30076751\n",
      "Iteration 13, loss = 0.27697341\n",
      "Iteration 14, loss = 0.25731869\n",
      "Iteration 15, loss = 0.23888934\n",
      "Iteration 16, loss = 0.21927395\n",
      "Iteration 17, loss = 0.19852258\n",
      "Iteration 18, loss = 0.17839651\n",
      "Iteration 19, loss = 0.16036288\n",
      "Iteration 20, loss = 0.14463957\n",
      "Iteration 21, loss = 0.13067430\n",
      "Iteration 22, loss = 0.11799215\n",
      "Iteration 23, loss = 0.10651455\n",
      "Iteration 24, loss = 0.09634029\n",
      "Iteration 25, loss = 0.08747082\n",
      "Iteration 26, loss = 0.07976420\n",
      "Iteration 27, loss = 0.07303393\n",
      "Iteration 28, loss = 0.06712732\n",
      "Iteration 29, loss = 0.06193571\n",
      "Iteration 30, loss = 0.05737348\n",
      "Iteration 31, loss = 0.05336208\n",
      "Iteration 32, loss = 0.04982666\n",
      "Iteration 33, loss = 0.04669790\n",
      "Iteration 34, loss = 0.04391336\n",
      "Iteration 35, loss = 0.04141749\n",
      "Iteration 36, loss = 0.03916168\n",
      "Iteration 37, loss = 0.03710487\n",
      "Iteration 38, loss = 0.03521441\n",
      "Iteration 39, loss = 0.03346653\n",
      "Iteration 40, loss = 0.03184579\n",
      "Iteration 41, loss = 0.03034371\n",
      "Iteration 42, loss = 0.02895644\n",
      "Iteration 43, loss = 0.02768220\n",
      "Iteration 44, loss = 0.02651883\n",
      "Iteration 45, loss = 0.02546189\n",
      "Iteration 46, loss = 0.02450391\n",
      "Iteration 47, loss = 0.02363458\n",
      "Iteration 48, loss = 0.02284194\n",
      "Iteration 49, loss = 0.02211378\n",
      "Iteration 50, loss = 0.02143902\n",
      "Iteration 51, loss = 0.02080854\n",
      "Iteration 52, loss = 0.02021553\n",
      "Iteration 53, loss = 0.01965531\n",
      "Iteration 54, loss = 0.01912496\n",
      "Iteration 55, loss = 0.01862274\n",
      "Iteration 56, loss = 0.01814750\n",
      "Iteration 57, loss = 0.01769823\n",
      "Iteration 58, loss = 0.01727375\n",
      "Iteration 59, loss = 0.01687256\n",
      "Iteration 60, loss = 0.01649293\n",
      "Iteration 61, loss = 0.01613293\n",
      "Iteration 62, loss = 0.01579067\n",
      "Iteration 63, loss = 0.01546434\n",
      "Iteration 64, loss = 0.01515237\n",
      "Iteration 65, loss = 0.01485345\n",
      "Iteration 66, loss = 0.01456652\n",
      "Iteration 67, loss = 0.01429078\n",
      "Iteration 68, loss = 0.01402557\n",
      "Iteration 69, loss = 0.01377041\n",
      "Iteration 70, loss = 0.01352484\n",
      "Iteration 71, loss = 0.01328844\n",
      "Iteration 72, loss = 0.01306080\n",
      "Iteration 73, loss = 0.01284148\n",
      "Iteration 74, loss = 0.01263004\n",
      "Iteration 75, loss = 0.01242601\n",
      "Iteration 76, loss = 0.01222896\n",
      "Iteration 77, loss = 0.01203845\n",
      "Iteration 78, loss = 0.01185409\n",
      "Iteration 79, loss = 0.01167552\n",
      "Iteration 80, loss = 0.01150243\n",
      "Iteration 81, loss = 0.01133451\n",
      "Iteration 82, loss = 0.01117154\n",
      "Iteration 83, loss = 0.01101326\n",
      "Iteration 84, loss = 0.01085947\n",
      "Iteration 85, loss = 0.01070998\n",
      "Iteration 86, loss = 0.01056460\n",
      "Iteration 87, loss = 0.01042315\n",
      "Iteration 88, loss = 0.01028546\n",
      "Iteration 89, loss = 0.01015136\n",
      "Iteration 90, loss = 0.01002071\n",
      "Iteration 91, loss = 0.00989334\n",
      "Iteration 92, loss = 0.00976913\n",
      "Iteration 93, loss = 0.00964795\n",
      "Iteration 94, loss = 0.00952968\n",
      "Iteration 95, loss = 0.00941420\n",
      "Iteration 96, loss = 0.00930141\n",
      "Iteration 97, loss = 0.00919122\n",
      "Iteration 98, loss = 0.00908353\n",
      "Iteration 99, loss = 0.00897825\n",
      "Iteration 100, loss = 0.00887530\n",
      "Iteration 101, loss = 0.00877461\n",
      "Iteration 102, loss = 0.00867609\n",
      "Iteration 103, loss = 0.00857968\n",
      "Iteration 104, loss = 0.00848530\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 105, loss = 0.00839289\n",
      "Iteration 106, loss = 0.00831482\n",
      "Iteration 107, loss = 0.00824352\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 108, loss = 0.00817825\n",
      "Iteration 109, loss = 0.00812072\n",
      "Iteration 110, loss = 0.00806896\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 111, loss = 0.00802231\n",
      "Iteration 112, loss = 0.00798071\n",
      "Iteration 113, loss = 0.00794337\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 114, loss = 0.00790983\n",
      "Iteration 115, loss = 0.00787980\n",
      "Iteration 116, loss = 0.00785284\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 117, loss = 0.00782864\n",
      "Iteration 118, loss = 0.00780692\n",
      "Iteration 119, loss = 0.00778742\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 120, loss = 0.00776991\n",
      "Iteration 121, loss = 0.00775418\n",
      "Iteration 122, loss = 0.00774004\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 123, loss = 0.00772734\n",
      "Iteration 124, loss = 0.00771593\n",
      "Iteration 125, loss = 0.00770567\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 126, loss = 0.00769644\n",
      "Iteration 127, loss = 0.00768815\n",
      "Iteration 128, loss = 0.00768069\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "The analog prediction error (the loss) is 0.007680694382389224\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#\n",
    "# Here's where you can change the number of hidden layers\n",
    "# and number of neurons!\n",
    "#\n",
    "nn_classifier = MLPClassifier(hidden_layer_sizes=(6,7),  # 3 input -> 6 -> 7 -> 1 output\n",
    "                    max_iter=500,      # how many times to train\n",
    "                    activation=\"tanh\", # the \"activation function\" input -> output\n",
    "                    solver='sgd',      # the algorithm for optimizing weights\n",
    "                    verbose=True,      # False to \"mute\" the training\n",
    "                    shuffle=True,      # reshuffle the training epochs?\n",
    "                    random_state=None, # set for reproduceability\n",
    "                    learning_rate_init=.1,       # learning rate: % of error to backprop\n",
    "                    learning_rate = 'adaptive')  # soften feedback as it converges\n",
    "\n",
    "# documentation:\n",
    "# scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html \n",
    "#     Try verbose / activation \"relu\" / other network sizes ...\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_classifier.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"\\n++++++++++  TRAINING:   end  +++++++++++++++\")\n",
    "print(f\"The analog prediction error (the loss) is {nn_classifier.loss_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      input  ->  pred   des. \n",
      "           [6.  2.2 4.  1. ] ->   1      1      correct \n",
      "           [4.6 3.4 1.4 0.3] ->   0      0      correct \n",
      "           [6.5 3.  5.8 2.2] ->   2      2      correct \n",
      "           [5.2 3.4 1.4 0.2] ->   0      0      correct \n",
      "           [6.2 2.2 4.5 1.5] ->   2      1      incorrect: [9.78693871e-06 8.21858883e-03 9.91771624e-01]\n",
      "           [5.7 3.  4.2 1.2] ->   1      1      correct \n",
      "           [5.6 2.7 4.2 1.3] ->   1      1      correct \n",
      "           [5.7 2.8 4.5 1.3] ->   1      1      correct \n",
      "           [5.8 2.7 5.1 1.9] ->   2      2      correct \n",
      "           [6.4 2.9 4.3 1.3] ->   1      1      correct \n",
      "           [6.9 3.1 5.1 2.3] ->   2      2      correct \n",
      "           [5.5 2.5 4.  1.3] ->   1      1      correct \n",
      "           [5.7 2.6 3.5 1. ] ->   1      1      correct \n",
      "           [5.  3.5 1.3 0.3] ->   0      0      correct \n",
      "           [6.6 2.9 4.6 1.3] ->   1      1      correct \n",
      "           [6.3 2.9 5.6 1.8] ->   2      2      correct \n",
      "           [5.5 2.4 3.7 1. ] ->   1      1      correct \n",
      "           [7.2 3.  5.8 1.6] ->   2      2      correct \n",
      "           [5.  3.3 1.4 0.2] ->   0      0      correct \n",
      "           [5.5 4.2 1.4 0.2] ->   0      0      correct \n",
      "           [4.8 3.4 1.6 0.2] ->   0      0      correct \n",
      "           [6.7 3.  5.  1.7] ->   2      1      incorrect: [1.04559852e-04 9.59761699e-02 9.03919270e-01]\n",
      "           [5.  3.6 1.4 0.2] ->   0      0      correct \n",
      "           [6.  2.7 5.1 1.6] ->   2      1      incorrect: [8.50293425e-06 7.68784851e-03 9.92303649e-01]\n",
      "           [6.3 2.5 4.9 1.5] ->   2      1      incorrect: [1.34367747e-05 1.17682075e-02 9.88218356e-01]\n",
      "           [4.9 2.4 3.3 1. ] ->   1      1      correct \n",
      "           [5.5 2.4 3.8 1.1] ->   1      1      correct \n",
      "           [5.6 2.8 4.9 2. ] ->   2      2      correct \n",
      "           [6.7 3.1 5.6 2.4] ->   2      2      correct \n",
      "\n",
      "correct predictions: 25 out of 29\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# how did it do on the testing data?\n",
    "#\n",
    "\n",
    "#\n",
    "# which one do we want: classifier or regressor?\n",
    "#\n",
    "\n",
    "def ascii_table_for_classifier(Xsc,y,nn,scaler):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(Xsc)            # all predictions\n",
    "    prediction_probs = nn.predict_proba(Xsc) # all prediction probabilities\n",
    "    Xpr = scaler.inverse_transform(Xsc)      # Xpr is the \"X to print\": unscaled data!\n",
    "    # count correct\n",
    "    num_correct = 0\n",
    "    # printing\n",
    "    print(f\"{'input ':>28s} -> {'pred':^6s} {'des.':^6s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        pred_probs = prediction_probs[i,:]\n",
    "        desired = y[i]\n",
    "        if pred != desired: result = \"  incorrect: \" + str(pred_probs)\n",
    "        else: result = \"  correct\"; num_correct += 1\n",
    "        # Xpr = Xsc  # if you want to see the scaled versions\n",
    "        print(f\"{Xpr[i,:]!s:>28s} -> {pred:^6.0f} {desired:^6.0f} {result:^10s}\") \n",
    "    print(f\"\\ncorrect predictions: {num_correct} out of {len(y)}\")\n",
    "    \n",
    "\n",
    "\n",
    "#\n",
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_classifier(X_test_scaled,\n",
    "                           y_test_scaled,\n",
    "                           nn_classifier,\n",
    "                           scaler)   \n",
    "#\n",
    "# other things...\n",
    "#\n",
    "if False:  # do we want to see all of the parameters?\n",
    "    nn = nn_classifier  # less to type?\n",
    "    print(\"\\n\\n+++++ parameters, weights, etc. +++++\\n\")\n",
    "    print(f\"\\nweights/coefficients:\\n\")\n",
    "    for wts in nn.coefs_:\n",
    "        print(wts)\n",
    "    print(f\"\\nintercepts: {nn.intercepts_}\")\n",
    "    print(f\"\\nall parameters: {nn.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input features are [6.7, 3.1, 5.6, 2.4]\n",
      "nn.predict_proba ==  [[1.39045598e-06 1.10663577e-03 9.98891974e-01]]\n",
      "prediction: [2.]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we have a predictive model!  Let's try it out...\n",
    "#\n",
    "\n",
    "def make_prediction( Features, nn, scaler ):\n",
    "    \"\"\" uses nn for predictions \"\"\"\n",
    "    print(\"input features are\", Features)\n",
    "    #  we make sure Features has the right shape (list-of-lists)\n",
    "    row = np.array( [Features] )  # makes an array-row\n",
    "    row = scaler.transform(row)   # scale according to scaler\n",
    "    print(\"nn.predict_proba == \", nn.predict_proba(row))   # probabilities of each\n",
    "    prediction = nn.predict(row)  # max!\n",
    "    return prediction\n",
    "    \n",
    "# our features -- note that the inputs don't have to be bits!\n",
    "Features = [ 6.7, 3.1, 5.6, 2.4 ]      # whatever we'd like to test\n",
    "prediction = make_prediction(Features, nn_classifier, scaler)\n",
    "print(f\"prediction: {prediction}\")     # takes the max (nice to see them all!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Here, we _could_ use cross-validation to find the \"right\"/\"best\" size and shape of the NNet...\n",
    "\n",
    "\n",
    "#### Instead, lets explore two new directions: \n",
    "+ regression and \n",
    "+ exploring feature-predictability\n",
    "+ specifically, next is an example in which we estimate petalwid... \n",
    "+ ... from the other four columns -- including the species!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sepallen': 0, 'sepalwid': 1, 'petallen': 2, 'petalwid': 3, 'irisnum': 4}\n",
      "Index(['sepallen', 'sepalwid', 'petallen', 'petalwid', 'irisnum'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# What shall we predict today?\n",
    "print(COL_INDEX)\n",
    "print(COLUMNS)\n",
    "\n",
    "# Let's predict petal width ('petalwid', column index 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Start of regression experiments... +++\n",
      "\n",
      "y_reg (petalwid!)   is \n",
      " [0.2 0.1 0.2 0.2 0.2 0.2 0.3 0.2 0.3 0.4 0.2 0.2 0.3 0.2 0.1 0.3 0.2 0.2\n",
      " 0.2 0.2 0.3 0.2 0.2 0.2 0.1 0.1 0.1 0.2 0.3 0.4 0.2 0.2 0.2 0.2 0.4 0.2\n",
      " 0.2 0.2 0.4 0.6 0.2 0.5 0.4 0.2 0.3 0.2 0.4 1.4 1.  1.  1.  1.  1.3 1.\n",
      " 1.1 1.4 1.1 1.2 1.3 1.3 1.2 1.  1.3 1.3 1.  1.3 1.2 1.5 1.3 1.2 1.3 1.4\n",
      " 1.4 1.5 1.5 1.3 1.5 1.6 1.5 1.5 1.4 1.5 1.3 1.4 1.2 1.6 1.5 1.8 1.4 1.5\n",
      " 1.5 1.7 1.6 1.7 1.8 1.8 2.  1.8 1.8 2.  1.5 1.9 2.4 1.9 1.8 1.5 2.  2.3\n",
      " 2.  2.3 1.9 2.3 2.3 2.1 1.8 1.8 2.1 1.4 1.8 2.4 2.1 2.2 2.4 2.1 2.5 2.3\n",
      " 2.2 1.8 1.6 2.3 2.1 1.8 2.5 1.9 2.3 1.8 2.  2.1 2.2 2.  2.3]\n",
      "X_reg (just features: first few rows) is \n",
      " [[4.6 3.6 1.  0. ]\n",
      " [4.3 3.  1.1 0. ]\n",
      " [5.  3.2 1.2 0. ]\n",
      " [5.8 4.  1.2 0. ]\n",
      " [4.4 3.  1.3 0. ]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Here we set up for a regression model that will predict 'petalwid'  (column index 3)  using\n",
    "#\n",
    "#   sepal length  'sepallen' (column index 0)\n",
    "#   sepal width   'sepalwid' (column index 1)\n",
    "#   petal length  'petallen' (column index 2)\n",
    "#     and\n",
    "#   species       'irisnum'  (column index 4)\n",
    "\n",
    "#   to be _predicted_:\n",
    "#\n",
    "#   petal width   'petalwid' (column index 3)\n",
    "\n",
    "print(\"+++ Start of regression experiments... +++\\n\")\n",
    "#\n",
    "# Notice!  We're dropping column 3 from the features!\n",
    "#\n",
    "X_all = np.concatenate( (A[:,0:3], A[:,4:]),axis=1)  # columns 0, 1, 2, _not_3, and 4!\n",
    "# sepallen, sepalwid, petallen + species!\n",
    "# The above horizontal-concatenation skips the column indexed at 3\n",
    "\n",
    "#\n",
    "# Notice!  We're using column 3 as the target!!\n",
    "#\n",
    "y_all = A[:,3]             # y (labels) ... is all of column 3 (petalwid), all rows\n",
    "print(f\"y_reg (petalwid!)   is \\n {y_all}\") \n",
    "print(f\"X_reg (just features: first few rows) is \\n {X_all[:5,:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label-_values_\n",
      " [0.2 0.3 0.2 1.5 1.5 2.1 1.  1.3 1.  2.1 1.8 0.2 2.3 1.2 2.  2.3 1.6 0.2\n",
      " 0.2 0.3 1.3 0.1 1.6 1.3 1.2 2.4 1.3 0.4 0.3 2.  1.1 0.2 1.7 0.2 1.  2.4\n",
      " 0.4 1.5 1.2 1.5 0.2 2.  0.2 0.2 1.8 0.1 2.3 0.6 1.3 0.3 2.3 2.5 1.6 1.4\n",
      " 2.2 1.8 1.  0.2 1.3 1.4 0.2 1.8 1.9 1.4 1.5 0.2 1.9 0.4 0.5 1.5 1.4 2.3\n",
      " 2.  0.1 1.2 2.2 1.4 1.  2.  1.7 0.2 0.2 0.2 0.3 1.5 2.3 0.2 2.5 1.5 1.1\n",
      " 0.3 1.8 1.  1.5 0.2 1.8 2.  1.8 0.1 1.8 1.4 1.3 2.3 0.4 1.3 1.4 2.1 1.3\n",
      " 0.2 1.5 2.3 1.3 2.2 0.2 0.4 0.3 2.1 1.2 2.1 1.8 0.2 0.2 0.2 0.2 1.6 1.9\n",
      " 2.4 0.4 1.8 0.2 1.5 1.5 1.  0.1 0.2 1.8 1.4 0.2 1.9 2.1 1.8]\n",
      "\n",
      "features (a few)\n",
      " [[5.8 4.  1.2 0. ]\n",
      " [4.6 3.4 1.4 0. ]\n",
      " [5.2 3.5 1.5 0. ]\n",
      " [6.3 2.8 5.1 2. ]\n",
      " [6.4 3.2 4.5 1. ]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we scramble the data, to give a different TRAIN/TEST split each time...\n",
    "# \n",
    "indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "# we scramble both X and y, necessarily with the same permutation\n",
    "X_all = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "y_all = y_all[indices]              # again...\n",
    "print(\"label-_values_\\n\",y_all)\n",
    "print(\"\\nfeatures (a few)\\n\", X_all[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with 112 rows;  testing with 29 rows\n",
      "\n",
      "Held-out data... (testing data: 29)\n",
      "y_test: [1.3 2.1 0.2 0.2 2.2 0.1 0.1 0.3 1.5 2.  1.3 2.5 0.2 2.4 1.3 2.5 1.  1.5\n",
      " 1.4 2.4 1.3 1.  1.4 2.  1.2 0.4 0.2 0.2 2.3]\n",
      "\n",
      "X_test (few rows): [[6.1 2.8 4.  1. ]\n",
      " [7.1 3.  5.9 2. ]\n",
      " [4.4 3.  1.3 0. ]\n",
      " [4.6 3.1 1.5 0. ]\n",
      " [6.5 3.  5.8 2. ]]\n",
      "\n",
      "Data used for modeling... (training data: 112)\n",
      "y_train: [0.2 1.1 2.  1.5 1.8 1.5 1.6 2.3 1.8 1.6 1.8 0.2 2.  2.3 1.6 0.2 0.2 1.9\n",
      " 2.3 0.3 0.3 1.2 1.7 0.4 1.8 1.5 0.2 1.4 0.3 1.1 0.2 1.  1.5 1.2 0.3 0.2\n",
      " 1.6 2.3 0.2 0.6 1.  1.4 0.2 0.2 1.  1.5 0.4 1.4 2.2 0.1 1.5 0.2 1.8 1.8\n",
      " 1.3 0.2 1.4 0.2 0.2 1.4 1.3 2.  1.2 1.4 0.2 2.3 1.8 0.1 0.3 0.1 0.2 1.8\n",
      " 1.3 0.2 1.9 2.2 0.3 1.3 0.5 0.2 2.3 2.4 0.2 1.  0.2 1.5 2.1 1.8 2.1 0.4\n",
      " 1.8 2.1 1.5 0.4 0.4 2.  1.2 0.2 1.8 1.3 1.8 1.3 2.3 2.1 1.9 2.1 1.7 1.9\n",
      " 0.2 1.5 1.  1.5]\n",
      "\n",
      "X_train (few rows): [[4.6 3.2 1.4 0. ]\n",
      " [5.6 2.5 3.9 1. ]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [5.9 3.  4.2 1. ]\n",
      " [6.4 3.1 5.5 2. ]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We next separate into test data and training data ... \n",
    "#    + We will train on the training data...\n",
    "#    + We will _not_ look at the testing data to build the model\n",
    "#\n",
    "# Then, afterward, we will test on the testing data -- and see how well we do!\n",
    "#\n",
    "\n",
    "#\n",
    "# a common convention:  train on 80%, test on 20%    Let's define the TEST_PERCENT\n",
    "#\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
    "\n",
    "print(f\"training with {len(y_train)} rows;  testing with {len(y_test)} rows\\n\" )\n",
    "\n",
    "print(f\"Held-out data... (testing data: {len(y_test)})\")\n",
    "print(f\"y_test: {y_test}\\n\")\n",
    "print(f\"X_test (few rows): {X_test[0:5,:]}\")  # 5 rows\n",
    "print()\n",
    "print(f\"Data used for modeling... (training data: {len(y_train)})\")\n",
    "print(f\"y_train: {y_train}\\n\")\n",
    "print(f\"X_train (few rows): {X_train[0:5,:]}\")  # 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    input  -> pred  des. \n",
      "         [-1.58097533  0.28578106 -1.34372232 -1.21688419] -> ?     0.2  \n",
      "         [-0.33313011 -1.33778668  0.07498903 -0.01076889] -> ?     1.1  \n",
      "             [2.5369139  1.67741055 1.49370038 1.19534642] -> ?     2.0  \n",
      "         [ 0.04122346 -0.17809544  0.24523439 -0.01076889] -> ?     1.5  \n",
      "             [0.66514607 0.05384281 0.98296429 1.19534642] -> ?     1.8  \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# for NNets, it's important to keep the feature values near 0, say -1. to 1. or so\n",
    "#    This is done through the \"StandardScaler\" in scikit-learn\n",
    "# \n",
    "USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "# we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "if USE_SCALER == True:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)  # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "else:\n",
    "    # this one does no scaling!  We still create it to be consistent:\n",
    "    scaler = StandardScaler(copy=True, with_mean=False, with_std=False) # no scaling\n",
    "    scaler.fit(X_train)  # still need to fit, though it does not change...\n",
    "\n",
    "scaler   # is now defined and ready to use...\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Here are our scaled training and testing sets:\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train) # scale!\n",
    "X_test_scaled = scaler.transform(X_test) # scale!\n",
    "\n",
    "y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "y_test_scaled = y_test  # not using the scaler\n",
    "\n",
    "def ascii_table(X,y):\n",
    "    \"\"\" print a table of binary inputs and outputs \"\"\"\n",
    "    print(f\"{'input ':>58s} -> {'pred':<5s} {'des.':<5s}\") \n",
    "    for i in range(len(y)):\n",
    "        print(f\"{X[i,:]!s:>58s} -> {'?':<5s} {y[i]:<5.1f}\")   # !s is str ...\n",
    "    \n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5])\n",
    "\n",
    "#\n",
    "# Note that the zeros have become -1's\n",
    "# and the 1's have stayed 1's\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "Iteration 1, loss = 1.23704020\n",
      "Iteration 2, loss = 0.32450401\n",
      "Iteration 3, loss = 0.10022771\n",
      "Iteration 4, loss = 0.10685851\n",
      "Iteration 5, loss = 0.10411654\n",
      "Iteration 6, loss = 0.07326363\n",
      "Iteration 7, loss = 0.05759031\n",
      "Iteration 8, loss = 0.06341879\n",
      "Iteration 9, loss = 0.06823872\n",
      "Iteration 10, loss = 0.06209455\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 11, loss = 0.05188344\n",
      "Iteration 12, loss = 0.04752413\n",
      "Iteration 13, loss = 0.04363742\n",
      "Iteration 14, loss = 0.04033667\n",
      "Iteration 15, loss = 0.03761397\n",
      "Iteration 16, loss = 0.03538358\n",
      "Iteration 17, loss = 0.03353140\n",
      "Iteration 18, loss = 0.03195348\n",
      "Iteration 19, loss = 0.03057601\n",
      "Iteration 20, loss = 0.02935818\n",
      "Iteration 21, loss = 0.02828383\n",
      "Iteration 22, loss = 0.02734880\n",
      "Iteration 23, loss = 0.02654968\n",
      "Iteration 24, loss = 0.02587688\n",
      "Iteration 25, loss = 0.02531261\n",
      "Iteration 26, loss = 0.02483291\n",
      "Iteration 27, loss = 0.02441166\n",
      "Iteration 28, loss = 0.02402492\n",
      "Iteration 29, loss = 0.02365415\n",
      "Iteration 30, loss = 0.02328772\n",
      "Iteration 31, loss = 0.02292067\n",
      "Iteration 32, loss = 0.02255321\n",
      "Iteration 33, loss = 0.02218858\n",
      "Iteration 34, loss = 0.02183103\n",
      "Iteration 35, loss = 0.02148418\n",
      "Iteration 36, loss = 0.02115017\n",
      "Iteration 37, loss = 0.02082949\n",
      "Iteration 38, loss = 0.02052127\n",
      "Iteration 39, loss = 0.02022397\n",
      "Iteration 40, loss = 0.01993592\n",
      "Iteration 41, loss = 0.01965591\n",
      "Iteration 42, loss = 0.01938339\n",
      "Iteration 43, loss = 0.01911849\n",
      "Iteration 44, loss = 0.01886189\n",
      "Iteration 45, loss = 0.01861457\n",
      "Iteration 46, loss = 0.01837751\n",
      "Iteration 47, loss = 0.01815149\n",
      "Iteration 48, loss = 0.01793696\n",
      "Iteration 49, loss = 0.01773401\n",
      "Iteration 50, loss = 0.01754239\n",
      "Iteration 51, loss = 0.01736159\n",
      "Iteration 52, loss = 0.01719095\n",
      "Iteration 53, loss = 0.01702971\n",
      "Iteration 54, loss = 0.01687711\n",
      "Iteration 55, loss = 0.01673239\n",
      "Iteration 56, loss = 0.01659484\n",
      "Iteration 57, loss = 0.01646380\n",
      "Iteration 58, loss = 0.01633863\n",
      "Iteration 59, loss = 0.01621872\n",
      "Iteration 60, loss = 0.01610350\n",
      "Iteration 61, loss = 0.01599244\n",
      "Iteration 62, loss = 0.01588505\n",
      "Iteration 63, loss = 0.01578090\n",
      "Iteration 64, loss = 0.01567963\n",
      "Iteration 65, loss = 0.01558093\n",
      "Iteration 66, loss = 0.01548456\n",
      "Iteration 67, loss = 0.01539033\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 68, loss = 0.01529811\n",
      "Iteration 69, loss = 0.01522087\n",
      "Iteration 70, loss = 0.01515058\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 71, loss = 0.01508645\n",
      "Iteration 72, loss = 0.01503025\n",
      "Iteration 73, loss = 0.01497988\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 74, loss = 0.01493466\n",
      "Iteration 75, loss = 0.01489449\n",
      "Iteration 76, loss = 0.01485858\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 77, loss = 0.01482643\n",
      "Iteration 78, loss = 0.01479773\n",
      "Iteration 79, loss = 0.01477204\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 80, loss = 0.01474904\n",
      "Iteration 81, loss = 0.01472846\n",
      "Iteration 82, loss = 0.01471002\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 83, loss = 0.01469349\n",
      "Iteration 84, loss = 0.01467867\n",
      "Iteration 85, loss = 0.01466538\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 86, loss = 0.01465346\n",
      "Iteration 87, loss = 0.01464276\n",
      "Iteration 88, loss = 0.01463315\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "The (squared) prediction error (the loss) is 0.014633150814674605\n",
      "And, its square root: 0.12096756100159499\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# MLPRegressor predicts _floating-point_ outputs\n",
    "#\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "nn_regressor = MLPRegressor(hidden_layer_sizes=(6,7), \n",
    "                    max_iter=200,          # how many training epochs\n",
    "                    activation=\"tanh\",     # the activation function\n",
    "                    solver='sgd',          # the optimizer\n",
    "                    verbose=True,          # do we want to watch as it trains?\n",
    "                    shuffle=True,          # shuffle each epoch?\n",
    "                    random_state=None,     # use for reproducibility\n",
    "                    learning_rate_init=.1, # how much of each error to back-propagate\n",
    "                    learning_rate = 'adaptive')  # how to handle the learning_rate\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_regressor.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"++++++++++  TRAINING:   end  +++++++++++++++\")\n",
    "\n",
    "print(f\"The (squared) prediction error (the loss) is {nn_regressor.loss_}\")\n",
    "print(f\"And, its square root: {nn_regressor.loss_ ** 0.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Note that square-root of the mean-squared-error,  0.11   This is an \"expected error\" in predicting petal width\n",
    "#\n",
    "# admittedly, only one measure of expected error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      input  ->   pred    des.    absdiff  \n",
      "           [6.1 2.8 4.  1. ] ->  +1.432  +1.300    0.132   \n",
      "           [7.1 3.  5.9 2. ] ->  +1.998  +2.100    0.102   \n",
      "           [4.4 3.  1.3 0. ] ->  +0.214  +0.200    0.014   \n",
      "           [4.6 3.1 1.5 0. ] ->  +0.219  +0.200    0.019   \n",
      "           [6.5 3.  5.8 2. ] ->  +1.965  +2.200    0.235   \n",
      "           [4.9 3.1 1.5 0. ] ->  +0.210  +0.100    0.110   \n",
      "           [4.3 3.  1.1 0. ] ->  +0.210  +0.100    0.110   \n",
      "           [4.5 2.3 1.3 0. ] ->  +0.185  +0.300    0.115   \n",
      "           [6.3 2.5 4.9 1. ] ->  +1.456  +1.500    0.044   \n",
      "           [7.7 2.8 6.7 2. ] ->  +2.004  +2.000    0.004   \n",
      "           [6.4 2.9 4.3 1. ] ->  +1.579  +1.300    0.279   \n",
      "           [7.2 3.6 6.1 2. ] ->  +2.018  +2.500    0.482   \n",
      "           [4.9 3.  1.4 0. ] ->  +0.196  +0.200    0.004   \n",
      "           [5.8 2.8 5.1 2. ] ->  +1.864  +2.400    0.536   \n",
      "           [6.6 2.9 4.6 1. ] ->  +1.639  +1.300    0.339   \n",
      "           [6.7 3.3 5.7 2. ] ->  +2.020  +2.500    0.480   \n",
      "           [5.5 2.4 3.7 1. ] ->  +1.052  +1.000    0.052   \n",
      "           [6.9 3.1 4.9 1. ] ->  +1.692  +1.500    0.192   \n",
      "           [5.2 2.7 3.9 1. ] ->  +1.033  +1.400    0.367   \n",
      "           [6.7 3.1 5.6 2. ] ->  +1.991  +2.400    0.409   \n",
      "           [5.6 3.  4.1 1. ] ->  +1.255  +1.300    0.045   \n",
      "           [5.  2.  3.5 1. ] ->  +0.853  +1.000    0.147   \n",
      "           [6.8 2.8 4.8 1. ] ->  +1.652  +1.400    0.252   \n",
      "           [6.5 3.2 5.1 2. ] ->  +1.991  +2.000    0.009   \n",
      "           [5.5 2.6 4.4 1. ] ->  +1.177  +1.200    0.023   \n",
      "           [5.7 4.4 1.5 0. ] ->  +0.438  +0.400    0.038   \n",
      "           [4.7 3.2 1.3 0. ] ->  +0.217  +0.200    0.017   \n",
      "           [4.4 3.2 1.3 0. ] ->  +0.226  +0.200    0.026   \n",
      "           [6.7 3.  5.2 2. ] ->  +1.969  +2.300    0.331   \n",
      "\n",
      "+++++   +++++      +++++   +++++   \n",
      "average abs error: 0.16937665132148275\n",
      "+++++   +++++      +++++   +++++   \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# how did it do? now we're making progress (by regressing)\n",
    "#\n",
    "\n",
    "def ascii_table_for_regressor(Xsc,y,nn,scaler):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(Xsc) # all predictions\n",
    "    Xpr = scaler.inverse_transform(Xsc)  # Xpr is the \"X to print\": unscaled data!\n",
    "    # measure error\n",
    "    error = 0.0\n",
    "    # printing\n",
    "    print(f\"{'input ':>28s} ->  {'pred':^6s}  {'des.':^6s}  {'absdiff':^10s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        desired = y[i]\n",
    "        result = abs(desired - pred)\n",
    "        error += result\n",
    "        # Xpr = Xsc   # if you'd like to see the scaled values\n",
    "        print(f\"{Xpr[i,:]!s:>28s} ->  {pred:<+6.3f}  {desired:<+6.3f}  {result:^10.3f}\") \n",
    "\n",
    "    print(\"\\n\" + \"+++++   +++++      +++++   +++++   \")\n",
    "    print(f\"average abs error: {error/len(y)}\")\n",
    "    print(\"+++++   +++++      +++++   +++++   \")\n",
    "    \n",
    "#\n",
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_regressor(X_test_scaled,\n",
    "                          y_test_scaled,\n",
    "                          nn_regressor,\n",
    "                          scaler)   # this is our own f'n, above\n",
    "#\n",
    "# other things...\n",
    "#\n",
    "if False:  # do we want to see these details?\n",
    "    nn = nn_regressor  # less to type?\n",
    "    print(\"\\n\\n+++++ parameters, weights, etc. +++++\\n\")\n",
    "    print(f\"\\nweights/coefficients:\\n\")\n",
    "    for wts in nn.coefs_:\n",
    "        print(wts)\n",
    "    print(f\"\\nintercepts: {nn.intercepts_}\")\n",
    "    print(f\"\\nall parameters: {nn.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your task!\n",
    "\n",
    "Just as above, \n",
    "choose _another_ botanical feature, one of\n",
    "  + sepallen\n",
    "  + sepalwid\n",
    "  + petallen\n",
    "\n",
    "And model it _with_ the irisnum (the species number)\n",
    "  + This will be largely copy-and-paste-and-edit!\n",
    "\n",
    "THEN, choose the _same_ botanical feature (whichever you chose)\n",
    "And model it _without_ the irisnum (just leave irisnum out)\n",
    "\n",
    "How much better did it do with than without?!\n",
    "\n",
    "  You've estimated how much \"value\" is added, by knowing the flower-species, \n",
    "  when predicting each botanical measurement...\n",
    "\n",
    "\"\"\"\n",
    "print(\"Best wishes with hw6's neural nets!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Start of regression experiments... +++\n",
      "\n",
      "y_reg (petalwid!)   is \n",
      " [1.  1.1 1.2 1.2 1.3 1.3 1.3 1.3 1.3 1.3 1.3 1.4 1.4 1.4 1.4 1.4 1.4 1.4\n",
      " 1.4 1.4 1.4 1.4 1.4 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.6\n",
      " 1.6 1.6 1.6 1.6 1.6 1.7 1.7 1.7 1.7 1.9 1.9 4.7 3.3 3.3 3.5 3.5 3.6 3.7\n",
      " 3.8 3.9 3.9 3.9 4.  4.  4.  4.  4.  4.1 4.1 4.2 4.2 4.2 4.3 4.4 4.4 4.4\n",
      " 4.4 4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.6 4.6 4.6 4.7 4.7 4.7 4.7 4.8 4.8 4.9\n",
      " 4.9 5.  5.1 4.5 4.8 4.8 4.9 4.9 4.9 5.  5.  5.  5.1 5.1 5.1 5.1 5.1 5.1\n",
      " 5.2 5.2 5.3 5.3 5.4 5.4 5.5 5.5 5.5 5.6 5.6 5.6 5.6 5.6 5.6 5.7 5.7 5.7\n",
      " 5.8 5.8 5.8 5.9 5.9 6.  6.1 6.1 6.1 6.3 6.4 6.6 6.7 6.7 6.9]\n",
      "X_reg (just features: first few rows) is \n",
      " [[4.6 3.6 0.2 0. ]\n",
      " [4.3 3.  0.1 0. ]\n",
      " [5.  3.2 0.2 0. ]\n",
      " [5.8 4.  0.2 0. ]\n",
      " [4.4 3.  0.2 0. ]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Here we set up for a regression model that will predict 'petalwid'  (column index 3)  using\n",
    "#\n",
    "#   sepal length  'sepallen' (column index 0)\n",
    "#   sepal width   'sepalwid' (column index 1)\n",
    "#   petal length  'petalwid' (column index 3)\n",
    "#     and\n",
    "#   species       'irisnum'  (column index 4)\n",
    "\n",
    "#   to be _predicted_:\n",
    "#\n",
    "#   petal width   'petallen' (column index 2)\n",
    "\n",
    "print(\"+++ Start of regression experiments... +++\\n\")\n",
    "#\n",
    "# Notice!  We're dropping column 2 from the features!\n",
    "#\n",
    "X_all = np.concatenate( (A[:,0:2], A[:,3:5]),axis=1)  # columns 0, 1, _not_2, 3, and 4!\n",
    "# sepallen, sepalwid, petalwid + species!\n",
    "# The above horizontal-concatenation skips the column indexed at 2\n",
    "\n",
    "#\n",
    "# Notice!  We're using column 2 as the target!!\n",
    "#\n",
    "y_all = A[:,2]             # y (labels) ... is all of column 2 (petallen), all rows\n",
    "print(f\"y_reg (petalwid!)   is \\n {y_all}\") \n",
    "print(f\"X_reg (just features: first few rows) is \\n {X_all[:5,:]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label-_values_\n",
      " [5.5 1.6 6.1 4.8 5.5 5.2 1.4 5.8 4.9 5.7 4.4 1.5 4.1 1.4 1.  5.8 5.1 1.5\n",
      " 1.1 1.3 5.5 3.3 4.2 5.6 6.9 5.9 4.3 1.5 4.5 1.4 4.9 1.3 1.9 1.5 4.4 5.9\n",
      " 4.5 4.8 6.7 1.7 1.7 5.3 1.9 4.6 5.1 1.6 1.3 1.4 6.1 1.6 1.5 1.3 4.9 1.4\n",
      " 5.7 5.1 1.3 5.8 1.6 4.2 1.4 4.5 4.  1.4 3.6 1.5 4.  1.2 4.4 1.4 5.6 4.7\n",
      " 6.3 3.3 5.  5.2 4.8 4.5 1.2 1.5 5.  1.4 6.7 5.1 1.6 5.1 5.6 4.  5.1 4.6\n",
      " 4.  4.5 1.4 4.9 3.5 1.5 1.5 4.5 4.7 1.3 1.7 5.7 4.5 1.4 4.7 6.6 4.1 4.7\n",
      " 4.5 1.5 1.7 3.5 5.1 5.6 3.8 3.9 5.3 4.8 5.4 1.5 3.9 6.  4.2 1.3 3.9 1.5\n",
      " 5.  5.  4.4 1.4 6.1 4.7 6.4 4.9 5.6 3.7 5.4 4.  4.6 5.6 1.6]\n",
      "\n",
      "features (a few)\n",
      " [[6.5 3.  1.8 2. ]\n",
      " [5.  3.5 0.6 0. ]\n",
      " [7.2 3.6 2.5 2. ]\n",
      " [6.8 2.8 1.4 1. ]\n",
      " [6.8 3.  2.1 2. ]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we scramble the data, to give a different TRAIN/TEST split each time...\n",
    "# \n",
    "indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "# we scramble both X and y, necessarily with the same permutation\n",
    "X_all = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "y_all = y_all[indices]              # again...\n",
    "print(\"label-_values_\\n\",y_all)\n",
    "print(\"\\nfeatures (a few)\\n\", X_all[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with 112 rows;  testing with 29 rows\n",
      "\n",
      "Held-out data... (testing data: 29)\n",
      "y_test: [1.2 1.6 1.4 5.5 4.2 3.9 5.8 1.5 4.6 6.4 3.7 1.4 5.7 1.5 4.6 4.5 4.5 1.6\n",
      " 4.9 1.4 1.5 5.  1.4 1.7 1.4 5.7 3.3 5.2 1.7]\n",
      "\n",
      "X_test (few rows): [[5.  3.2 0.2 0. ]\n",
      " [5.  3.5 0.6 0. ]\n",
      " [4.6 3.4 0.3 0. ]\n",
      " [6.4 3.1 1.8 2. ]\n",
      " [5.7 3.  1.2 1. ]]\n",
      "\n",
      "Data used for modeling... (training data: 112)\n",
      "y_train: [1.4 5.1 5.1 4.8 4.1 4.4 4.8 1.5 4.2 6.7 1.5 3.9 4.7 6.7 4.4 1.  1.5 1.4\n",
      " 5.6 3.8 6.9 6.1 5.  4.  1.3 1.3 3.9 5.1 5.9 6.3 4.7 3.6 4.1 4.3 1.2 1.9\n",
      " 4.5 1.7 4.8 1.6 6.1 6.  1.5 4.2 6.1 1.7 5.2 4.9 1.5 3.3 6.6 4.5 1.3 5.4\n",
      " 1.3 4.9 5.6 5.6 5.3 1.6 4.9 4.5 1.5 1.3 4.5 4.7 5.1 5.  1.5 4.  4.5 5.5\n",
      " 4.7 3.5 5.9 5.5 5.8 1.5 1.1 5.4 1.5 1.9 4.7 5.1 4.  1.3 1.6 4.6 1.3 5.3\n",
      " 1.4 5.1 5.6 4.4 4.  4.5 4.8 5.6 1.4 5.7 4.  3.5 5.  5.6 5.8 4.9 1.4 4.4\n",
      " 1.4 1.6 1.4 5.1]\n",
      "\n",
      "X_train (few rows): [[5.2 3.4 0.2 0. ]\n",
      " [6.5 3.2 2.  2. ]\n",
      " [6.3 2.8 1.5 2. ]\n",
      " [6.2 2.8 1.8 2. ]\n",
      " [5.6 3.  1.3 1. ]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We next separate into test data and training data ... \n",
    "#    + We will train on the training data...\n",
    "#    + We will _not_ look at the testing data to build the model\n",
    "#\n",
    "# Then, afterward, we will test on the testing data -- and see how well we do!\n",
    "#\n",
    "\n",
    "#\n",
    "# a common convention:  train on 80%, test on 20%    Let's define the TEST_PERCENT\n",
    "#\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
    "\n",
    "print(f\"training with {len(y_train)} rows;  testing with {len(y_test)} rows\\n\" )\n",
    "\n",
    "print(f\"Held-out data... (testing data: {len(y_test)})\")\n",
    "print(f\"y_test: {y_test}\\n\")\n",
    "print(f\"X_test (few rows): {X_test[0:5,:]}\")  # 5 rows\n",
    "print()\n",
    "print(f\"Data used for modeling... (training data: {len(y_train)})\")\n",
    "print(f\"y_train: {y_train}\\n\")\n",
    "print(f\"X_train (few rows): {X_train[0:5,:]}\")  # 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    input  -> pred  des. \n",
      "         [-0.82524125  0.89798685 -1.38793554 -1.2989798 ] -> ?     1.4  \n",
      "             [0.72915882 0.42910793 0.99138253 1.16688016] -> ?     5.1  \n",
      "         [ 0.49002035 -0.50864989  0.33046084  1.16688016] -> ?     5.1  \n",
      "         [ 0.37045112 -0.50864989  0.72701385  1.16688016] -> ?     4.8  \n",
      "         [-0.3469643  -0.03977098  0.06609217 -0.06604982] -> ?     4.1  \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# for NNets, it's important to keep the feature values near 0, say -1. to 1. or so\n",
    "#    This is done through the \"StandardScaler\" in scikit-learn\n",
    "# \n",
    "USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "# we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "if USE_SCALER == True:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)  # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "else:\n",
    "    # this one does no scaling!  We still create it to be consistent:\n",
    "    scaler = StandardScaler(copy=True, with_mean=False, with_std=False) # no scaling\n",
    "    scaler.fit(X_train)  # still need to fit, though it does not change...\n",
    "\n",
    "scaler   # is now defined and ready to use...\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Here are our scaled training and testing sets:\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train) # scale!\n",
    "X_test_scaled = scaler.transform(X_test) # scale!\n",
    "\n",
    "y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "y_test_scaled = y_test  # not using the scaler\n",
    "\n",
    "def ascii_table(X,y):\n",
    "    \"\"\" print a table of binary inputs and outputs \"\"\"\n",
    "    print(f\"{'input ':>58s} -> {'pred':<5s} {'des.':<5s}\") \n",
    "    for i in range(len(y)):\n",
    "        print(f\"{X[i,:]!s:>58s} -> {'?':<5s} {y[i]:<5.1f}\")   # !s is str ...\n",
    "    \n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5])\n",
    "\n",
    "#\n",
    "# Note that the zeros have become -1's\n",
    "# and the 1's have stayed 1's\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "Iteration 1, loss = 10.18758426\n",
      "Iteration 2, loss = 1.33251266\n",
      "Iteration 3, loss = 0.33605402\n",
      "Iteration 4, loss = 0.96010163\n",
      "Iteration 5, loss = 0.51049782\n",
      "Iteration 6, loss = 0.21878822\n",
      "Iteration 7, loss = 0.23312098\n",
      "Iteration 8, loss = 0.13386747\n",
      "Iteration 9, loss = 0.12487378\n",
      "Iteration 10, loss = 0.12180848\n",
      "Iteration 11, loss = 0.11500245\n",
      "Iteration 12, loss = 0.10547560\n",
      "Iteration 13, loss = 0.09475887\n",
      "Iteration 14, loss = 0.08445208\n",
      "Iteration 15, loss = 0.07527890\n",
      "Iteration 16, loss = 0.06748197\n",
      "Iteration 17, loss = 0.06118883\n",
      "Iteration 18, loss = 0.05643113\n",
      "Iteration 19, loss = 0.05299961\n",
      "Iteration 20, loss = 0.05047103\n",
      "Iteration 21, loss = 0.04841666\n",
      "Iteration 22, loss = 0.04655360\n",
      "Iteration 23, loss = 0.04474815\n",
      "Iteration 24, loss = 0.04295599\n",
      "Iteration 25, loss = 0.04117659\n",
      "Iteration 26, loss = 0.03943872\n",
      "Iteration 27, loss = 0.03779852\n",
      "Iteration 28, loss = 0.03632897\n",
      "Iteration 29, loss = 0.03509597\n",
      "Iteration 30, loss = 0.03413297\n",
      "Iteration 31, loss = 0.03342910\n",
      "Iteration 32, loss = 0.03293592\n",
      "Iteration 33, loss = 0.03258654\n",
      "Iteration 34, loss = 0.03231612\n",
      "Iteration 35, loss = 0.03207537\n",
      "Iteration 36, loss = 0.03183486\n",
      "Iteration 37, loss = 0.03158234\n",
      "Iteration 38, loss = 0.03131711\n",
      "Iteration 39, loss = 0.03104471\n",
      "Iteration 40, loss = 0.03077314\n",
      "Iteration 41, loss = 0.03051043\n",
      "Iteration 42, loss = 0.03026313\n",
      "Iteration 43, loss = 0.03003539\n",
      "Iteration 44, loss = 0.02982865\n",
      "Iteration 45, loss = 0.02964193\n",
      "Iteration 46, loss = 0.02947274\n",
      "Iteration 47, loss = 0.02931811\n",
      "Iteration 48, loss = 0.02917547\n",
      "Iteration 49, loss = 0.02904306\n",
      "Iteration 50, loss = 0.02891993\n",
      "Iteration 51, loss = 0.02880564\n",
      "Iteration 52, loss = 0.02869982\n",
      "Iteration 53, loss = 0.02860188\n",
      "Iteration 54, loss = 0.02851092\n",
      "Iteration 55, loss = 0.02842572\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 56, loss = 0.02834494\n",
      "Iteration 57, loss = 0.02828124\n",
      "Iteration 58, loss = 0.02822529\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 59, loss = 0.02817557\n",
      "Iteration 60, loss = 0.02813392\n",
      "Iteration 61, loss = 0.02809771\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 62, loss = 0.02806606\n",
      "Iteration 63, loss = 0.02803891\n",
      "Iteration 64, loss = 0.02801529\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 65, loss = 0.02799467\n",
      "Iteration 66, loss = 0.02797676\n",
      "Iteration 67, loss = 0.02796112\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 68, loss = 0.02794741\n",
      "Iteration 69, loss = 0.02793542\n",
      "Iteration 70, loss = 0.02792488\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 71, loss = 0.02791561\n",
      "Iteration 72, loss = 0.02790744\n",
      "Iteration 73, loss = 0.02790024\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 74, loss = 0.02789386\n",
      "Iteration 75, loss = 0.02788822\n",
      "Iteration 76, loss = 0.02788322\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 77, loss = 0.02787878\n",
      "Iteration 78, loss = 0.02787484\n",
      "Iteration 79, loss = 0.02787133\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "The (squared) prediction error (the loss) is 0.027871329324824695\n",
      "And, its square root: 0.166947085403803\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# MLPRegressor predicts _floating-point_ outputs\n",
    "#\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "nn_regressor = MLPRegressor(hidden_layer_sizes=(6,7), \n",
    "                    max_iter=200,          # how many training epochs\n",
    "                    activation=\"tanh\",     # the activation function\n",
    "                    solver='sgd',          # the optimizer\n",
    "                    verbose=True,          # do we want to watch as it trains?\n",
    "                    shuffle=True,          # shuffle each epoch?\n",
    "                    random_state=None,     # use for reproducibility\n",
    "                    learning_rate_init=.1, # how much of each error to back-propagate\n",
    "                    learning_rate = 'adaptive')  # how to handle the learning_rate\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_regressor.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"++++++++++  TRAINING:   end  +++++++++++++++\")\n",
    "\n",
    "print(f\"The (squared) prediction error (the loss) is {nn_regressor.loss_}\")\n",
    "print(f\"And, its square root: {nn_regressor.loss_ ** 0.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      input  ->   pred    des.    absdiff  \n",
      "           [5.  3.2 0.2 0. ] ->  +1.494  +1.200    0.294   \n",
      "           [5.  3.5 0.6 0. ] ->  +1.700  +1.600    0.100   \n",
      "           [4.6 3.4 0.3 0. ] ->  +1.477  +1.400    0.077   \n",
      "           [6.4 3.1 1.8 2. ] ->  +5.326  +5.500    0.174   \n",
      "           [5.7 3.  1.2 1. ] ->  +4.135  +4.200    0.065   \n",
      "           [5.6 2.5 1.1 1. ] ->  +3.956  +3.900    0.056   \n",
      "           [7.2 3.  1.6 2. ] ->  +5.894  +5.800    0.094   \n",
      "           [5.1 3.7 0.4 0. ] ->  +1.555  +1.500    0.055   \n",
      "           [6.5 2.8 1.5 1. ] ->  +4.652  +4.600    0.052   \n",
      "           [7.9 3.8 2.  2. ] ->  +6.413  +6.400    0.013   \n",
      "           [5.5 2.4 1.  1. ] ->  +3.734  +3.700    0.034   \n",
      "           [4.8 3.  0.1 0. ] ->  +1.407  +1.400    0.007   \n",
      "           [6.7 3.3 2.1 2. ] ->  +5.607  +5.700    0.093   \n",
      "           [5.4 3.7 0.2 0. ] ->  +1.426  +1.500    0.074   \n",
      "           [6.6 2.9 1.3 1. ] ->  +4.636  +4.600    0.036   \n",
      "           [6.4 3.2 1.5 1. ] ->  +4.624  +4.500    0.124   \n",
      "           [4.9 2.5 1.7 2. ] ->  +4.134  +4.500    0.366   \n",
      "           [4.8 3.4 0.2 0. ] ->  +1.429  +1.600    0.171   \n",
      "           [6.1 3.  1.8 2. ] ->  +5.117  +4.900    0.217   \n",
      "           [5.1 3.5 0.3 0. ] ->  +1.497  +1.400    0.097   \n",
      "           [5.2 3.5 0.2 0. ] ->  +1.442  +1.500    0.058   \n",
      "           [6.7 3.  1.7 1. ] ->  +4.772  +5.000    0.228   \n",
      "           [5.5 4.2 0.2 0. ] ->  +1.452  +1.400    0.052   \n",
      "           [5.4 3.4 0.2 0. ] ->  +1.468  +1.700    0.232   \n",
      "           [5.1 3.5 0.2 0. ] ->  +1.438  +1.400    0.038   \n",
      "           [6.7 3.3 2.5 2. ] ->  +5.701  +5.700    0.001   \n",
      "           [4.9 2.4 1.  1. ] ->  +3.111  +3.300    0.189   \n",
      "           [6.7 3.  2.3 2. ] ->  +5.703  +5.200    0.503   \n",
      "           [5.1 3.3 0.5 0. ] ->  +1.649  +1.700    0.051   \n",
      "\n",
      "+++++   +++++      +++++   +++++   \n",
      "average abs error: 0.12241413800110347\n",
      "+++++   +++++      +++++   +++++   \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# how did it do? now we're making progress (by regressing)\n",
    "#\n",
    "\n",
    "def ascii_table_for_regressor(Xsc,y,nn,scaler):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(Xsc) # all predictions\n",
    "    Xpr = scaler.inverse_transform(Xsc)  # Xpr is the \"X to print\": unscaled data!\n",
    "    # measure error\n",
    "    error = 0.0\n",
    "    # printing\n",
    "    print(f\"{'input ':>28s} ->  {'pred':^6s}  {'des.':^6s}  {'absdiff':^10s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        desired = y[i]\n",
    "        result = abs(desired - pred)\n",
    "        error += result\n",
    "        # Xpr = Xsc   # if you'd like to see the scaled values\n",
    "        print(f\"{Xpr[i,:]!s:>28s} ->  {pred:<+6.3f}  {desired:<+6.3f}  {result:^10.3f}\") \n",
    "\n",
    "    print(\"\\n\" + \"+++++   +++++      +++++   +++++   \")\n",
    "    print(f\"average abs error: {error/len(y)}\")\n",
    "    print(\"+++++   +++++      +++++   +++++   \")\n",
    "    \n",
    "#\n",
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_regressor(X_test_scaled,\n",
    "                          y_test_scaled,\n",
    "                          nn_regressor,\n",
    "                          scaler)   # this is our own f'n, above\n",
    "#\n",
    "# other things...\n",
    "#\n",
    "if False:  # do we want to see these details?\n",
    "    nn = nn_regressor  # less to type?\n",
    "    print(\"\\n\\n+++++ parameters, weights, etc. +++++\\n\")\n",
    "    print(f\"\\nweights/coefficients:\\n\")\n",
    "    for wts in nn.coefs_:\n",
    "        print(wts)\n",
    "    print(f\"\\nintercepts: {nn.intercepts_}\")\n",
    "    print(f\"\\nall parameters: {nn.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Start of regression experiments... +++\n",
      "\n",
      "y_reg (petalwid!)   is \n",
      " [1.  1.1 1.2 1.2 1.3 1.3 1.3 1.3 1.3 1.3 1.3 1.4 1.4 1.4 1.4 1.4 1.4 1.4\n",
      " 1.4 1.4 1.4 1.4 1.4 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.6\n",
      " 1.6 1.6 1.6 1.6 1.6 1.7 1.7 1.7 1.7 1.9 1.9 4.7 3.3 3.3 3.5 3.5 3.6 3.7\n",
      " 3.8 3.9 3.9 3.9 4.  4.  4.  4.  4.  4.1 4.1 4.2 4.2 4.2 4.3 4.4 4.4 4.4\n",
      " 4.4 4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.6 4.6 4.6 4.7 4.7 4.7 4.7 4.8 4.8 4.9\n",
      " 4.9 5.  5.1 4.5 4.8 4.8 4.9 4.9 4.9 5.  5.  5.  5.1 5.1 5.1 5.1 5.1 5.1\n",
      " 5.2 5.2 5.3 5.3 5.4 5.4 5.5 5.5 5.5 5.6 5.6 5.6 5.6 5.6 5.6 5.7 5.7 5.7\n",
      " 5.8 5.8 5.8 5.9 5.9 6.  6.1 6.1 6.1 6.3 6.4 6.6 6.7 6.7 6.9]\n",
      "X_reg (just features: first few rows) is \n",
      " [[4.6 3.6 0.2]\n",
      " [4.3 3.  0.1]\n",
      " [5.  3.2 0.2]\n",
      " [5.8 4.  0.2]\n",
      " [4.4 3.  0.2]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Here we set up for a regression model that will predict 'petalwid'  (column index 3)  using\n",
    "#\n",
    "#   sepal length  'sepallen' (column index 0)\n",
    "#   sepal width   'sepalwid' (column index 1)\n",
    "#   petal length  'petalwid' (column index 3)\n",
    "\n",
    "#   to be _predicted_:\n",
    "#\n",
    "#   petal width   'petallen' (column index 2)\n",
    "\n",
    "print(\"+++ Start of regression experiments... +++\\n\")\n",
    "#\n",
    "# Notice!  We're dropping column 2 from the features!\n",
    "#\n",
    "X_all = np.concatenate( (A[:,0:2], A[:,3:4]),axis=1)  # columns 0, 1, _not_2, and 3\n",
    "# sepallen, sepalwid, petalwid + species!\n",
    "# The above horizontal-concatenation skips the column indexed at 2\n",
    "\n",
    "#\n",
    "# Notice!  We're using column 2 as the target!!\n",
    "#\n",
    "y_all = A[:,2]             # y (labels) ... is all of column 2 (petallen), all rows\n",
    "print(f\"y_reg (petalwid!)   is \\n {y_all}\") \n",
    "print(f\"X_reg (just features: first few rows) is \\n {X_all[:5,:]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label-_values_\n",
      " [3.9 1.7 6.1 5.1 5.1 1.5 1.5 5.  5.4 1.1 5.9 4.5 1.4 5.8 5.1 5.5 1.4 4.6\n",
      " 1.6 4.7 5.1 4.8 6.4 4.4 1.6 1.4 1.  1.4 4.3 5.9 4.7 1.9 1.4 1.5 4.4 1.4\n",
      " 1.3 5.8 5.6 4.9 3.7 1.4 4.8 4.4 1.5 4.6 4.9 1.5 1.5 4.5 6.1 5.2 1.3 6.6\n",
      " 1.4 4.5 5.3 4.7 3.6 5.2 1.4 5.  6.7 1.5 1.3 1.2 3.5 4.2 5.1 3.9 4.2 1.5\n",
      " 4.4 1.3 1.4 3.9 4.  1.4 4.5 1.3 5.6 4.9 4.9 4.5 1.5 4.5 1.4 4.1 4.  1.9\n",
      " 6.  4.8 5.7 3.5 5.  4.1 1.6 1.2 5.4 1.6 4.5 5.3 4.7 1.5 5.1 5.7 4.7 6.9\n",
      " 6.3 5.6 5.1 1.7 4.6 1.3 3.3 5.7 4.9 5.6 5.5 1.6 4.8 5.8 1.5 4.  1.3 4.2\n",
      " 1.7 5.6 1.6 3.8 5.5 6.7 1.7 1.5 4.  3.3 4.  5.6 4.5 6.1 5. ]\n",
      "\n",
      "features (a few)\n",
      " [[5.8 2.7 1.2]\n",
      " [5.4 3.4 0.2]\n",
      " [7.7 3.  2.3]\n",
      " [5.8 2.7 1.9]\n",
      " [6.  2.7 1.6]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we scramble the data, to give a different TRAIN/TEST split each time...\n",
    "# \n",
    "indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "# we scramble both X and y, necessarily with the same permutation\n",
    "X_all = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "y_all = y_all[indices]              # again...\n",
    "print(\"label-_values_\\n\",y_all)\n",
    "print(\"\\nfeatures (a few)\\n\", X_all[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with 112 rows;  testing with 29 rows\n",
      "\n",
      "Held-out data... (testing data: 29)\n",
      "y_test: [5.9 5.7 1.3 4.8 4.8 4.7 1.5 3.7 1.  5.1 4.4 3.9 5.5 1.5 1.1 1.3 6.3 1.4\n",
      " 4.8 3.6 5.6 6.4 4.2 4.7 5.  1.9 6.9 1.5 4.5]\n",
      "\n",
      "X_test (few rows): [[6.8 3.2 2.3]\n",
      " [6.7 3.3 2.5]\n",
      " [5.  3.5 0.3]\n",
      " [5.9 3.2 1.8]\n",
      " [6.  3.  1.8]]\n",
      "\n",
      "Data used for modeling... (training data: 112)\n",
      "y_train: [1.4 1.4 1.3 3.5 4.9 1.3 5.6 1.3 6.1 5.8 4.7 6.7 5.  3.9 4.5 4.8 1.4 4.2\n",
      " 4.5 6.1 1.2 4.3 5.1 5.5 5.1 1.5 1.4 1.7 4.4 1.6 5.1 5.8 4.2 4.5 1.6 5.\n",
      " 4.  4.  1.4 5.4 5.4 5.6 1.3 4.9 1.5 1.4 5.  3.5 4.  3.3 1.7 5.2 5.6 6.6\n",
      " 1.5 5.8 4.  1.5 5.6 6.1 4.5 4.5 4.7 3.8 1.4 4.4 5.7 4.6 1.5 4.9 4.7 3.3\n",
      " 3.9 1.2 1.7 1.4 5.5 1.5 1.5 5.1 1.4 1.4 5.3 4.1 4.5 1.4 4.1 4.  1.6 5.7\n",
      " 4.5 6.7 4.9 5.1 4.6 5.6 5.2 1.9 1.5 6.  1.5 4.4 1.3 4.9 5.3 1.7 1.6 5.1\n",
      " 1.6 5.9 1.6 4.6]\n",
      "\n",
      "X_train (few rows): [[4.9 3.  0.2]\n",
      " [5.  3.6 0.2]\n",
      " [4.5 2.3 0.3]\n",
      " [5.7 2.6 1. ]\n",
      " [6.1 3.  1.8]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We next separate into test data and training data ... \n",
    "#    + We will train on the training data...\n",
    "#    + We will _not_ look at the testing data to build the model\n",
    "#\n",
    "# Then, afterward, we will test on the testing data -- and see how well we do!\n",
    "#\n",
    "\n",
    "#\n",
    "# a common convention:  train on 80%, test on 20%    Let's define the TEST_PERCENT\n",
    "#\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
    "\n",
    "print(f\"training with {len(y_train)} rows;  testing with {len(y_test)} rows\\n\" )\n",
    "\n",
    "print(f\"Held-out data... (testing data: {len(y_test)})\")\n",
    "print(f\"y_test: {y_test}\\n\")\n",
    "print(f\"X_test (few rows): {X_test[0:5,:]}\")  # 5 rows\n",
    "print()\n",
    "print(f\"Data used for modeling... (training data: {len(y_train)})\")\n",
    "print(f\"y_train: {y_train}\\n\")\n",
    "print(f\"X_train (few rows): {X_train[0:5,:]}\")  # 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    input  -> pred  des. \n",
      "                     [-1.17506487 -0.12026142 -1.29673065] -> ?     1.4  \n",
      "                     [-1.05286685  1.32287566 -1.29673065] -> ?     1.4  \n",
      "                     [-1.66385694 -1.80392135 -1.16565318] -> ?     1.3  \n",
      "                     [-0.19748073 -1.08235281 -0.24811092] -> ?     3.5  \n",
      "                     [ 0.29131135 -0.12026142  0.80050881] -> ?     4.9  \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# for NNets, it's important to keep the feature values near 0, say -1. to 1. or so\n",
    "#    This is done through the \"StandardScaler\" in scikit-learn\n",
    "# \n",
    "USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "# we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "if USE_SCALER == True:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)  # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "else:\n",
    "    # this one does no scaling!  We still create it to be consistent:\n",
    "    scaler = StandardScaler(copy=True, with_mean=False, with_std=False) # no scaling\n",
    "    scaler.fit(X_train)  # still need to fit, though it does not change...\n",
    "\n",
    "scaler   # is now defined and ready to use...\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Here are our scaled training and testing sets:\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train) # scale!\n",
    "X_test_scaled = scaler.transform(X_test) # scale!\n",
    "\n",
    "y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "y_test_scaled = y_test  # not using the scaler\n",
    "\n",
    "def ascii_table(X,y):\n",
    "    \"\"\" print a table of binary inputs and outputs \"\"\"\n",
    "    print(f\"{'input ':>58s} -> {'pred':<5s} {'des.':<5s}\") \n",
    "    for i in range(len(y)):\n",
    "        print(f\"{X[i,:]!s:>58s} -> {'?':<5s} {y[i]:<5.1f}\")   # !s is str ...\n",
    "    \n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5])\n",
    "\n",
    "#\n",
    "# Note that the zeros have become -1's\n",
    "# and the 1's have stayed 1's\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "Iteration 1, loss = 11.41892786\n",
      "Iteration 2, loss = 0.55681315\n",
      "Iteration 3, loss = 0.88084612\n",
      "Iteration 4, loss = 0.44886213\n",
      "Iteration 5, loss = 0.20536944\n",
      "Iteration 6, loss = 0.18367445\n",
      "Iteration 7, loss = 0.14436923\n",
      "Iteration 8, loss = 0.12609751\n",
      "Iteration 9, loss = 0.11114883\n",
      "Iteration 10, loss = 0.10144925\n",
      "Iteration 11, loss = 0.09871254\n",
      "Iteration 12, loss = 0.09941425\n",
      "Iteration 13, loss = 0.09940170\n",
      "Iteration 14, loss = 0.09624654\n",
      "Iteration 15, loss = 0.09020363\n",
      "Iteration 16, loss = 0.08265710\n",
      "Iteration 17, loss = 0.07466047\n",
      "Iteration 18, loss = 0.06709794\n",
      "Iteration 19, loss = 0.06035038\n",
      "Iteration 20, loss = 0.05525066\n",
      "Iteration 21, loss = 0.05233261\n",
      "Iteration 22, loss = 0.05456909\n",
      "Iteration 23, loss = 0.06976565\n",
      "Iteration 24, loss = 0.12110165\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 25, loss = 0.25483414\n",
      "Iteration 26, loss = 0.21341650\n",
      "Iteration 27, loss = 0.11217877\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 28, loss = 0.05920384\n",
      "Iteration 29, loss = 0.05510253\n",
      "Iteration 30, loss = 0.06307569\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 31, loss = 0.07606125\n",
      "Iteration 32, loss = 0.09235981\n",
      "Iteration 33, loss = 0.10922145\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 34, loss = 0.12522268\n",
      "Iteration 35, loss = 0.14121427\n",
      "Iteration 36, loss = 0.15614387\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 37, loss = 0.16979924\n",
      "Iteration 38, loss = 0.18258076\n",
      "Iteration 39, loss = 0.19424920\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 40, loss = 0.20483068\n",
      "Iteration 41, loss = 0.21449429\n",
      "Iteration 42, loss = 0.22324349\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 43, loss = 0.23114511\n",
      "Iteration 44, loss = 0.23829373\n",
      "Iteration 45, loss = 0.24474195\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 46, loss = 0.25055266\n",
      "Iteration 47, loss = 0.25579055\n",
      "Iteration 48, loss = 0.26050730\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "The (squared) prediction error (the loss) is 0.260507295083789\n",
      "And, its square root: 0.5103991527067703\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# MLPRegressor predicts _floating-point_ outputs\n",
    "#\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "nn_regressor = MLPRegressor(hidden_layer_sizes=(6,7), \n",
    "                    max_iter=200,          # how many training epochs\n",
    "                    activation=\"tanh\",     # the activation function\n",
    "                    solver='sgd',          # the optimizer\n",
    "                    verbose=True,          # do we want to watch as it trains?\n",
    "                    shuffle=True,          # shuffle each epoch?\n",
    "                    random_state=None,     # use for reproducibility\n",
    "                    learning_rate_init=.1, # how much of each error to back-propagate\n",
    "                    learning_rate = 'adaptive')  # how to handle the learning_rate\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_regressor.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"++++++++++  TRAINING:   end  +++++++++++++++\")\n",
    "\n",
    "print(f\"The (squared) prediction error (the loss) is {nn_regressor.loss_}\")\n",
    "print(f\"And, its square root: {nn_regressor.loss_ ** 0.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      input  ->   pred    des.    absdiff  \n",
      "               [6.8 3.2 2.3] ->  +4.783  +5.900    1.117   \n",
      "               [6.7 3.3 2.5] ->  +4.881  +5.700    0.819   \n",
      "               [5.  3.5 0.3] ->  +1.172  +1.300    0.128   \n",
      "               [5.9 3.2 1.8] ->  +4.328  +4.800    0.472   \n",
      "               [6.  3.  1.8] ->  +4.273  +4.800    0.527   \n",
      "               [6.1 2.9 1.4] ->  +4.000  +4.700    0.700   \n",
      "               [5.1 3.7 0.4] ->  +1.215  +1.500    0.285   \n",
      "               [5.5 2.4 1. ] ->  +3.686  +3.700    0.014   \n",
      "               [4.6 3.6 0.2] ->  +0.880  +1.000    0.120   \n",
      "               [5.8 2.7 1.9] ->  +4.201  +5.100    0.899   \n",
      "               [6.7 3.1 1.4] ->  +4.141  +4.400    0.259   \n",
      "               [5.2 2.7 1.4] ->  +3.862  +3.900    0.038   \n",
      "               [6.5 3.  1.8] ->  +4.351  +5.500    1.149   \n",
      "               [5.7 4.4 0.4] ->  +1.536  +1.500    0.036   \n",
      "               [4.3 3.  0.1] ->  +0.983  +1.100    0.117   \n",
      "               [4.4 3.  0.2] ->  +1.104  +1.300    0.196   \n",
      "               [7.3 2.9 1.8] ->  +4.439  +6.300    1.861   \n",
      "               [4.6 3.2 0.2] ->  +1.064  +1.400    0.336   \n",
      "               [6.2 2.8 1.8] ->  +4.228  +4.800    0.572   \n",
      "               [5.6 2.9 1.3] ->  +3.895  +3.600    0.295   \n",
      "               [6.4 2.8 2.2] ->  +4.534  +5.600    1.066   \n",
      "               [7.9 3.8 2. ] ->  +4.922  +6.400    1.478   \n",
      "               [5.6 2.7 1.3] ->  +3.853  +4.200    0.347   \n",
      "               [6.1 2.8 1.2] ->  +3.873  +4.700    0.827   \n",
      "               [6.  2.2 1.5] ->  +3.847  +5.000    1.153   \n",
      "               [4.8 3.4 0.2] ->  +1.062  +1.900    0.838   \n",
      "               [7.7 2.6 2.3] ->  +4.709  +6.900    2.191   \n",
      "               [5.1 3.8 0.3] ->  +1.127  +1.500    0.373   \n",
      "               [6.2 2.2 1.5] ->  +3.862  +4.500    0.638   \n",
      "\n",
      "+++++   +++++      +++++   +++++   \n",
      "average abs error: 0.6500101814379116\n",
      "+++++   +++++      +++++   +++++   \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# how did it do? now we're making progress (by regressing)\n",
    "#\n",
    "\n",
    "def ascii_table_for_regressor(Xsc,y,nn,scaler):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(Xsc) # all predictions\n",
    "    Xpr = scaler.inverse_transform(Xsc)  # Xpr is the \"X to print\": unscaled data!\n",
    "    # measure error\n",
    "    error = 0.0\n",
    "    # printing\n",
    "    print(f\"{'input ':>28s} ->  {'pred':^6s}  {'des.':^6s}  {'absdiff':^10s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        desired = y[i]\n",
    "        result = abs(desired - pred)\n",
    "        error += result\n",
    "        # Xpr = Xsc   # if you'd like to see the scaled values\n",
    "        print(f\"{Xpr[i,:]!s:>28s} ->  {pred:<+6.3f}  {desired:<+6.3f}  {result:^10.3f}\") \n",
    "\n",
    "    print(\"\\n\" + \"+++++   +++++      +++++   +++++   \")\n",
    "    print(f\"average abs error: {error/len(y)}\")\n",
    "    print(\"+++++   +++++      +++++   +++++   \")\n",
    "    \n",
    "#\n",
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_regressor(X_test_scaled,\n",
    "                          y_test_scaled,\n",
    "                          nn_regressor,\n",
    "                          scaler)   # this is our own f'n, above\n",
    "#\n",
    "# other things...\n",
    "#\n",
    "if False:  # do we want to see these details?\n",
    "    nn = nn_regressor  # less to type?\n",
    "    print(\"\\n\\n+++++ parameters, weights, etc. +++++\\n\")\n",
    "    print(f\"\\nweights/coefficients:\\n\")\n",
    "    for wts in nn.coefs_:\n",
    "        print(wts)\n",
    "    print(f\"\\nintercepts: {nn.intercepts_}\")\n",
    "    print(f\"\\nall parameters: {nn.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
