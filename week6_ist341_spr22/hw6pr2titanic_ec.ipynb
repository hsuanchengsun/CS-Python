{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "####   hw6pr2titanic_modeler \n",
    "+ titanic clasification and regression via NNets\n",
    "+ [here is the assignment page](https://docs.google.com/document/d/1oLNNM8jfWdG3xXvmRojXLgDWE-mFNgQUsohZ4dmfPV0/edit)\n",
    "+ [here is the hw6 page, specifically](https://docs.google.com/document/d/1oLNNM8jfWdG3xXvmRojXLgDWE-mFNgQUsohZ4dmfPV0/edit)\n",
    "\n",
    "Feel free to re-use your previous titanic-data-cleaner and/or your titanic_cleaned.csv itself\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Suggestion: copy-paste-and-alter from our previous wk6 classification and regression examples\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries...\n",
    "import numpy as np      # numpy is Python's \"array\" library\n",
    "import pandas as pd     # Pandas is Python's \"data\" library (\"dataframe\" == spreadsheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titanic_cleaned.csv : file read into a pandas dataframe.\n"
     ]
    }
   ],
   "source": [
    "# let's read in our flower data...\n",
    "# \n",
    "# iris_cleaned.csv and hw4pr1iris_cleaner.ipynb should be in this folder\n",
    "# \n",
    "filename = 'titanic_cleaned.csv'\n",
    "df_tidy = pd.read_csv(filename)      # encoding = \"utf-8\", \"latin1\"\n",
    "print(f\"{filename} : file read into a pandas dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_tidy.shape is (1004, 7)\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1004 entries, 0 to 1266\n",
      "Data columns (total 7 columns):\n",
      "pclass      1004 non-null int64\n",
      "survived    1004 non-null int64\n",
      "sex         1004 non-null object\n",
      "age         1004 non-null float64\n",
      "sibsp       1004 non-null int64\n",
      "parch       1004 non-null int64\n",
      "sexnum      1004 non-null int64\n",
      "dtypes: float64(1), int64(5), object(1)\n",
      "memory usage: 62.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>sexnum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>female</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>female</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>45.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>female</td>\n",
       "      <td>14.5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>26.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1004 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pclass  survived     sex      age  sibsp  parch  sexnum\n",
       "0          1         1  female  29.0000      0      0       0\n",
       "1          1         1    male   0.9167      1      2       1\n",
       "2          1         0  female   2.0000      1      2       0\n",
       "3          1         0    male  30.0000      1      2       1\n",
       "4          1         0  female  25.0000      1      2       0\n",
       "...      ...       ...     ...      ...    ...    ...     ...\n",
       "1259       3         0    male  45.5000      0      0       1\n",
       "1262       3         0  female  14.5000      1      0       0\n",
       "1264       3         0    male  26.5000      0      0       1\n",
       "1265       3         0    male  27.0000      0      0       1\n",
       "1266       3         0    male  29.0000      0      0       1\n",
       "\n",
       "[1004 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# different version vary on how to see all rows (adapt to suit your system!)\n",
    "#\n",
    "print(f\"df_tidy.shape is {df_tidy.shape}\\n\")\n",
    "df_tidy.info()  # prints column information\n",
    "\n",
    "# let's print the whole dataframe, too  (adapt # of lines, as desired)\n",
    "# pd.options.display.max_rows = 150   # None for no limit; default: 10\n",
    "# pd.options.display.min_rows = 150   # None for no limit; default: 10\n",
    "# pd.options.display.max_rows = 10   # None for no limit; default: 10\n",
    "# pd.options.display.min_rows = 10   # None for no limit; default: 10\n",
    "df_tidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>sexnum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>45.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14.5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1004 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pclass  survived      age  sibsp  parch  sexnum\n",
       "0          1         1  29.0000      0      0       0\n",
       "1          1         1   0.9167      1      2       1\n",
       "2          1         0   2.0000      1      2       0\n",
       "3          1         0  30.0000      1      2       1\n",
       "4          1         0  25.0000      1      2       0\n",
       "...      ...       ...      ...    ...    ...     ...\n",
       "1259       3         0  45.5000      0      0       1\n",
       "1262       3         0  14.5000      1      0       0\n",
       "1264       3         0  26.5000      0      0       1\n",
       "1265       3         0  27.0000      0      0       1\n",
       "1266       3         0  29.0000      0      0       1\n",
       "\n",
       "[1004 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# All of the columns need to be numeric, we'll drop irisname\n",
    "ROW = 0\n",
    "COLUMN = 1\n",
    "df_model1 = df_tidy.drop( 'sex', axis=COLUMN )\n",
    "df_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMNS is Index(['pclass', 'survived', 'age', 'sibsp', 'parch', 'sexnum'], dtype='object')\n",
      "\n",
      "COLUMNS[0] is pclass\n",
      "\n",
      "COL_INDEX is {'pclass': 0, 'survived': 1, 'age': 2, 'sibsp': 3, 'parch': 4, 'sexnum': 5}\n",
      "\n",
      "\n",
      "not survived maps to 0\n",
      "survived maps to 1\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# once we have all the columns we want, let's create an index of their names...\n",
    "\n",
    "#\n",
    "# Let's make sure we have all of our helpful variables in one place \n",
    "#       To be adapted if we drop/add more columns...\n",
    "#\n",
    "\n",
    "#\n",
    "# let's keep our column names in variables, for reference\n",
    "#\n",
    "COLUMNS = df_model1.columns            # \"list\" of columns\n",
    "print(f\"COLUMNS is {COLUMNS}\\n\")  \n",
    "  # It's a \"pandas\" list, called an Index\n",
    "  # use it just as a Python list of strings:\n",
    "print(f\"COLUMNS[0] is {COLUMNS[0]}\\n\")\n",
    "\n",
    "# let's create a dictionary to look up any column index by name\n",
    "COL_INDEX = {}\n",
    "for i, name in enumerate(COLUMNS):\n",
    "    COL_INDEX[name] = i  # using the name (as key), look up the value (i)\n",
    "print(f\"COL_INDEX is {COL_INDEX}\\n\\n\")\n",
    "\n",
    "\n",
    "#\n",
    "# and our \"species\" names\n",
    "#\n",
    "\n",
    "# all of scikit-learn's ML routines need numbers, not strings\n",
    "#   ... even for categories/classifications (like species!)\n",
    "#   so, we will convert the flower-species to numbers:\n",
    "\n",
    "SPECIES = ['not survived','survived']   # int to str\n",
    "SPECIES_INDEX = {'not survived':0,'survived':1}  # str to int\n",
    "\n",
    "# Let's try it out...\n",
    "for name in SPECIES:\n",
    "    print(f\"{name} maps to {SPECIES_INDEX[name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.      1.     29.      0.      0.      0.    ]\n",
      " [ 1.      1.      0.9167  1.      2.      1.    ]\n",
      " [ 1.      0.      2.      1.      2.      0.    ]\n",
      " ...\n",
      " [ 3.      0.     26.5     0.      0.      1.    ]\n",
      " [ 3.      0.     27.      0.      0.      1.    ]\n",
      " [ 3.      0.     29.      0.      0.      1.    ]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's convert our dataframe to a numpy array, named A\n",
    "#\n",
    "A = df_model1.to_numpy()   \n",
    "A = A.astype('float64')    # many types:  www.tutorialspoint.com/numpy/numpy_data_types.htm\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The dataset has 1004 rows and 6 cols\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# nice to have NUM_ROWS and NUM_COLS around\n",
    "#\n",
    "NUM_ROWS, NUM_COLS = A.shape\n",
    "print(f\"\\nThe dataset has {NUM_ROWS} rows and {NUM_COLS} cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flower #42 is [ 1.  1. 53.  0.  0.  0.]\n",
      "  Its pclass is 1.0\n",
      "  Its survived is 1.0\n",
      "  Its age is 53.0\n",
      "  Its sibsp is 0.0\n",
      "  Its parch is 0.0\n",
      "  Its sexnum is 0.0\n",
      "  Its species is survived (i.e., 1)\n"
     ]
    }
   ],
   "source": [
    "# let's use all of our variables, to reinforce that we have\n",
    "# (1) names...\n",
    "# (2) access and control...\n",
    "\n",
    "# choose a row index, n:\n",
    "n = 42\n",
    "print(f\"flower #{n} is {A[n]}\")\n",
    "\n",
    "for i in range(len(COLUMNS)):\n",
    "    colname = COLUMNS[i]\n",
    "    value = A[n][i]\n",
    "    print(f\"  Its {colname} is {value}\")\n",
    "\n",
    "species_index = COL_INDEX['survived']\n",
    "species_num = int(round(A[n][species_index]))\n",
    "species = SPECIES[species_num]\n",
    "print(f\"  Its species is {species} (i.e., {species_num})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_reg (petalwid!)   is \n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      "X_reg (just features: first few rows) is \n",
      " [[ 1.     29.      0.      0.      0.    ]\n",
      " [ 1.      0.9167  1.      2.      1.    ]\n",
      " [ 1.      2.      1.      2.      0.    ]\n",
      " [ 1.     30.      1.      2.      1.    ]\n",
      " [ 1.     25.      1.      2.      0.    ]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Notice!  We're dropping column 1 from the features!\n",
    "#\n",
    "X_all = np.concatenate( (A[:,0:1], A[:,2:]),axis=1)  \n",
    "\n",
    "\n",
    "#\n",
    "# Notice!  We're using column 1 as the target!!\n",
    "#\n",
    "y_all = A[:,1]             # y (labels) ... is all of column 3 (petalwid), all rows\n",
    "print(f\"y_reg (petalwid!)   is \\n {y_all}\") \n",
    "print(f\"X_reg (just features: first few rows) is \\n {X_all[:5,:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scrambled labels/species are \n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      "The corresponding data rows are \n",
      " [[ 3. 27.  1.  0.  0.]\n",
      " [ 3. 41.  0.  0.  1.]\n",
      " [ 3. 48.  0.  0.  1.]\n",
      " [ 3.  8.  4.  1.  1.]\n",
      " [ 2. 30.  1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we can scramble the data, to remove (potential) dependence on its ordering: \n",
    "# \n",
    "indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "# we scramble both X and y, necessarily with the same permutation\n",
    "X_all = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "y_all = y_all[indices]              # again...\n",
    "print(f\"The scrambled labels/species are \\n {y_all}\")\n",
    "print(f\"The corresponding data rows are \\n {X_all[0:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with 803 rows;  testing with 201 rows\n",
      "\n",
      "Held-out data... (testing data: 201)\n",
      "y_test: [1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.\n",
      " 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n",
      " 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      "\n",
      "X_test (few rows): [[ 2. 33.  0.  2.  0.]\n",
      " [ 2. 70.  0.  0.  1.]\n",
      " [ 2. 24.  1.  2.  0.]\n",
      " [ 2. 24.  1.  0.  0.]\n",
      " [ 1. 24.  1.  0.  1.]]\n",
      "\n",
      "Data used for modeling... (training data: 803)\n",
      "y_train: [0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0.\n",
      " 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.\n",
      " 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0.\n",
      " 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0.\n",
      " 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1.\n",
      " 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1.\n",
      " 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.]\n",
      "\n",
      "X_train (few rows): [[ 3. 48.  1.  3.  0.]\n",
      " [ 3. 32.  1.  1.  0.]\n",
      " [ 3.  9.  3.  2.  0.]\n",
      " [ 3. 11.  0.  0.  1.]\n",
      " [ 3. 22.  1.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We next separate into test data and training data ... \n",
    "#    + We will train on the training data...\n",
    "#    + We will _not_ look at the testing data to build the model\n",
    "#\n",
    "# Then, afterward, we will test on the testing data -- and see how well we do!\n",
    "#\n",
    "\n",
    "#\n",
    "# a common convention:  train on 80%, test on 20%    Let's define the TEST_PERCENT\n",
    "#\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
    "\n",
    "print(f\"training with {len(y_train)} rows;  testing with {len(y_test)} rows\\n\" )\n",
    "\n",
    "print(f\"Held-out data... (testing data: {len(y_test)})\")\n",
    "print(f\"y_test: {y_test}\\n\")\n",
    "print(f\"X_test (few rows): {X_test[0:5,:]}\")  # 5 rows\n",
    "print()\n",
    "print(f\"Data used for modeling... (training data: {len(y_train)})\")\n",
    "print(f\"y_train: {y_train}\\n\")\n",
    "print(f\"X_train (few rows): {X_train[0:5,:]}\")  # 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    input  -> pred  des. \n",
      "[ 0.89746851  1.29872061  0.55815701  3.18411094 -1.2879986 ] -> ?     0    \n",
      "[ 0.89746851  0.17308438  0.55815701  0.73032871 -1.2879986 ] -> ?     0    \n",
      "[ 0.89746851 -1.44501769  2.74984445  1.95721982 -1.2879986 ] -> ?     0    \n",
      "[ 0.89746851 -1.30431316 -0.53768671 -0.49656241  0.77639836] -> ?     0    \n",
      "[ 0.89746851 -0.53043826  0.55815701  0.73032871 -1.2879986 ] -> ?     1    \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# for NNets, it's important to keep the feature values near 0, say -1. to 1. or so\n",
    "#    This is done through the \"StandardScaler\" in scikit-learn\n",
    "# \n",
    "USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "# we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "if USE_SCALER == True:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)  # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "else:\n",
    "    # this one does no scaling!  We still create it to be consistent:\n",
    "    scaler = StandardScaler(copy=True, with_mean=False, with_std=False) # no scaling\n",
    "    scaler.fit(X_train)  # still need to fit, though it does not change...\n",
    "\n",
    "scaler   # is now defined and ready to use...\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Here are our scaled training and testing sets:\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train) # scale!\n",
    "X_test_scaled = scaler.transform(X_test) # scale!\n",
    "\n",
    "y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "y_test_scaled = y_test  # not using the scaler\n",
    "\n",
    "def ascii_table(X,y):\n",
    "    \"\"\" print a table of binary inputs and outputs \"\"\"\n",
    "    print(f\"{'input ':>58s} -> {'pred':<5s} {'des.':<5s}\") \n",
    "    for i in range(len(y)):\n",
    "        print(f\"{X[i,:]!s:>58s} -> {'?':<5s} {y[i]:<5.0f}\")   # !s is str ...\n",
    "    \n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5])\n",
    "\n",
    "#\n",
    "# Note that the zeros have become -1's\n",
    "# and the 1's have stayed 1's\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "Iteration 1, loss = 0.77241889\n",
      "Iteration 2, loss = 0.53380317\n",
      "Iteration 3, loss = 0.51564475\n",
      "Iteration 4, loss = 0.51218455\n",
      "Iteration 5, loss = 0.50289940\n",
      "Iteration 6, loss = 0.48317975\n",
      "Iteration 7, loss = 0.47679222\n",
      "Iteration 8, loss = 0.49113081\n",
      "Iteration 9, loss = 0.48592526\n",
      "Iteration 10, loss = 0.50859530\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 11, loss = 0.49511493\n",
      "Iteration 12, loss = 0.49201172\n",
      "Iteration 13, loss = 0.47542311\n",
      "Iteration 14, loss = 0.46559385\n",
      "Iteration 15, loss = 0.46488317\n",
      "Iteration 16, loss = 0.46519959\n",
      "Iteration 17, loss = 0.46530584\n",
      "Iteration 18, loss = 0.46566956\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 19, loss = 0.46252469\n",
      "Iteration 20, loss = 0.46094109\n",
      "Iteration 21, loss = 0.45926880\n",
      "Iteration 22, loss = 0.45852633\n",
      "Iteration 23, loss = 0.45790825\n",
      "Iteration 24, loss = 0.45751308\n",
      "Iteration 25, loss = 0.45755350\n",
      "Iteration 26, loss = 0.45739843\n",
      "Iteration 27, loss = 0.45726774\n",
      "Iteration 28, loss = 0.45712110\n",
      "Iteration 29, loss = 0.45730432\n",
      "Iteration 30, loss = 0.45769683\n",
      "Iteration 31, loss = 0.45802970\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 32, loss = 0.45789642\n",
      "Iteration 33, loss = 0.45798749\n",
      "Iteration 34, loss = 0.45792512\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 35, loss = 0.45794169\n",
      "Iteration 36, loss = 0.45794500\n",
      "Iteration 37, loss = 0.45795445\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 38, loss = 0.45792157\n",
      "Iteration 39, loss = 0.45791565\n",
      "Iteration 40, loss = 0.45790527\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 41, loss = 0.45790374\n",
      "Iteration 42, loss = 0.45790121\n",
      "Iteration 43, loss = 0.45789832\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 44, loss = 0.45789688\n",
      "Iteration 45, loss = 0.45789536\n",
      "Iteration 46, loss = 0.45789435\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 47, loss = 0.45789411\n",
      "Iteration 48, loss = 0.45789370\n",
      "Iteration 49, loss = 0.45789358\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "The analog prediction error (the loss) is 0.4578935804088757\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#\n",
    "# Here's where you can change the number of hidden layers\n",
    "# and number of neurons!\n",
    "#\n",
    "nn_classifier = MLPClassifier(hidden_layer_sizes=(16,8),  # 3 input -> 6 -> 7 -> 1 output\n",
    "                    max_iter=500,      # how many times to train\n",
    "                    activation=\"tanh\", # the \"activation function\" input -> output\n",
    "                    solver='sgd',      # the algorithm for optimizing weights\n",
    "                    verbose=True,      # False to \"mute\" the training\n",
    "                    shuffle=True,      # reshuffle the training epochs?\n",
    "                    random_state=None, # set for reproduceability\n",
    "                    learning_rate_init=.1,       # learning rate: % of error to backprop\n",
    "                    learning_rate = 'adaptive')  # soften feedback as it converges\n",
    "\n",
    "# documentation:\n",
    "# scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html \n",
    "#     Try verbose / activation \"relu\" / other network sizes ...\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_classifier.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"\\n++++++++++  TRAINING:   end  +++++++++++++++\")\n",
    "print(f\"The analog prediction error (the loss) is {nn_classifier.loss_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      input  ->  pred   des. \n",
      "[ 2.00000000e+00  3.30000000e+01 -5.55111512e-17  2.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "[ 2.00000000e+00  7.00000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "       [ 2. 24.  1.  2.  0.] ->   1      1      correct \n",
      "       [ 2. 24.  1.  0.  0.] ->   1      1      correct \n",
      "       [ 1. 24.  1.  0.  1.] ->   1      0      incorrect: [0.49167552 0.50832448]\n",
      "       [ 1. 23.  1.  0.  0.] ->   1      1      correct \n",
      "[ 2.00000000e+00  3.40000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 3.00000000e+00  2.10000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 3.00000000e+00  3.10000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   0      0      correct \n",
      "[ 2.00000000e+00  5.40000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 3.00000000e+00  1.60000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "[ 1.00000000e+00  5.80000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "       [ 1. 48.  1.  1.  0.] ->   1      1      correct \n",
      "[ 1.00000000e+00  4.60000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 3.00000000e+00  3.60000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 1.00000000e+00  3.60000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 3.00000000e+00  4.70000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 3.00000000e+00  2.90000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 2.00000000e+00  2.80000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "       [ 1. 37.  1.  0.  1.] ->   0      0      correct \n",
      "       [ 1. 48.  1.  0.  1.] ->   0      1      incorrect: [0.78475457 0.21524543]\n",
      "[ 3.00000000e+00  1.85000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      0      incorrect: [0.46125731 0.53874269]\n",
      "       [ 1. 49.  1.  0.  0.] ->   1      1      correct \n",
      "[ 1.00000000e+00  5.60000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "       [ 1. 43.  1.  0.  1.] ->   0      1      incorrect: [0.76374382 0.23625618]\n",
      "[ 3.00000000e+00  2.20000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      1      incorrect: [0.83435873 0.16564127]\n",
      "[ 3.00000000e+00  2.20000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   0      1      incorrect: [0.50515918 0.49484082]\n",
      "[ 3.00000000e+00  1.80000000e+01 -5.55111512e-17  1.00000000e+00\n",
      "  0.00000000e+00] ->   1      0      incorrect: [0.25044473 0.74955527]\n",
      "       [ 3. 15.  1.  0.  0.] ->   1      1      correct \n",
      "[ 3.00000000e+00  2.00000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "            [3. 6. 1. 1. 1.] ->   0      0      correct \n",
      "[ 2.00000000e+00  3.60000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "[ 1.00000000e+00  3.40000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      1      incorrect: [0.67137274 0.32862726]\n",
      "       [ 2. 24.  1.  0.  0.] ->   1      1      correct \n",
      "       [ 2. 31.  1.  1.  0.] ->   1      1      correct \n",
      "[ 1.00000000e+00  3.90000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 1.00000000e+00  6.40000000e+01 -5.55111512e-17  2.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "       [ 1. 30.  1.  2.  1.] ->   1      0      incorrect: [0.27712817 0.72287183]\n",
      "       [ 2. 27.  1.  0.  0.] ->   1      1      correct \n",
      "[ 1.00000000e+00  2.60000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "[ 3.00000000e+00  2.00000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "            [3. 6. 3. 1. 1.] ->   0      0      correct \n",
      "[ 1.00000000e+00  4.70000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 2.00000000e+00  4.00000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "       [ 2. 43.  1.  1.  1.] ->   0      0      correct \n",
      "[ 3.00000000e+00  3.30000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "            [3. 9. 4. 2. 0.] ->   0      0      correct \n",
      "[ 1.00000000e+00  4.20000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      1      incorrect: [0.7430836 0.2569164]\n",
      "[ 2.00000000e+00  2.30000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "       [ 2. 32.  1.  0.  1.] ->   0      1      incorrect: [0.84443763 0.15556237]\n",
      "            [3. 9. 5. 2. 1.] ->   0      0      correct \n",
      "[ 1.00000000e+00  4.50000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "       [ 1. 55.  2.  0.  0.] ->   1      1      correct \n",
      "[ 3.00000000e+00  2.00000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "       [ 1. 35.  1.  0.  0.] ->   1      1      correct \n",
      "            [3. 3. 4. 2. 1.] ->   0      1      incorrect: [0.79681192 0.20318808]\n",
      "[ 1.00000000e+00  5.00000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 3.00000000e+00  3.30000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 1.00000000e+00  4.70000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 3.00000000e+00  2.60000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 1.00000000e+00  2.40000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "       [ 1. 37.  1.  1.  1.] ->   0      1      incorrect: [0.55352088 0.44647912]\n",
      "[ 3.00000000e+00  4.20000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 2.00000000e+00  4.70000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 1.00000000e+00  2.90000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "[ 3.00000000e+00  1.80000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "[ 1.00000000e+00  3.00000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "[ 2.00000000e+00  3.30000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 3.00000000e+00  3.40000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 2.00000000e+00  2.80000000e+01 -5.55111512e-17  1.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 2.00000000e+00  3.00000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "[ 1.00000000e+00  3.10000000e+01 -5.55111512e-17  2.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "       [ 3. 23.  1.  0.  1.] ->   0      0      correct \n",
      "[ 1.00000000e+00  3.60000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 2.00000000e+00  1.20000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "[ 3.00000000e+00  2.60000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 3.00000000e+00  2.00000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "       [ 2. 36.  1.  0.  0.] ->   1      1      correct \n",
      "[ 2.00000000e+00  1.90000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "       [ 3. 30.  1.  0.  0.] ->   0      0      correct \n",
      "       [ 3. 18.  1.  0.  1.] ->   0      0      correct \n",
      "[ 2.00000000e+00  2.60000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "[ 3.00000000e+00  2.60000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   0      1      incorrect: [0.54970413 0.45029587]\n",
      "[ 3.00000000e+00  2.50000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 1.00000000e+00  6.20000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "[ 3.00000000e+00  1.90000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 2.00000000e+00  5.10000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "       [ 2. 25.  1.  2.  1.] ->   0      0      correct \n",
      "[ 2.00000000e+00  3.00000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 3.00000000e+00  2.10000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "       [ 3. 27.  1.  0.  0.] ->   0      0      correct \n",
      "[ 3.00000000e+00  4.20000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "       [ 1. 39.  1.  1.  0.] ->   1      1      correct \n",
      "            [2. 3. 1. 1. 1.] ->   1      1      correct \n",
      "       [ 1. 42.  1.  0.  1.] ->   0      0      correct \n",
      "[ 3.00000000e+00  2.80000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "       [ 3. 24.  1.  0.  1.] ->   0      0      correct \n",
      "       [ 3. 18.  1.  1.  1.] ->   0      0      correct \n",
      "[ 3.00000000e+00  2.20000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 3.00000000e+00  3.30000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 1.00000000e+00  1.90000000e+01 -5.55111512e-17  2.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "[ 3.00000000e+00  1.90000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "       [ 2. 40.  1.  0.  1.] ->   0      0      correct \n",
      "[ 1.00000000e+00  4.60000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 2.00000000e+00  5.00000000e+01 -5.55111512e-17  1.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "[ 3.00000000e+00  3.20000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      1      incorrect: [0.85859443 0.14140557]\n",
      "[ 1.00000000e+00  8.00000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      1      incorrect: [0.79974816 0.20025184]\n",
      "[ 1.00000000e+00  3.90000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "       [ 1. 64.  1.  1.  0.] ->   1      1      correct \n",
      "[ 1.00000000e+00  4.50000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "            [2. 2. 1. 1. 1.] ->   1      1      correct \n",
      "[ 1.00000000e+00  5.30000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      1      incorrect: [0.78399978 0.21600022]\n",
      "       [ 1. 57.  1.  0.  1.] ->   0      0      correct \n",
      "[ 3.00000000e+00  2.60000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 1.00000000e+00  2.70000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      1      incorrect: [0.56017078 0.43982922]\n",
      "[ 1.00000000e+00  3.10000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "[ 1.00000000e+00  2.10000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "[ 2.00000000e+00  3.80000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      0      incorrect: [0.10316064 0.89683936]\n",
      "[ 3.00000000e+00  2.60000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "            [3. 1. 1. 1. 0.] ->   1      1      correct \n",
      "[ 3.00000000e+00  3.70000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   0      0      correct \n",
      "       [ 1. 41.  1.  0.  1.] ->   0      0      correct \n",
      "[ 3.00000000e+00  2.60000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 3.00000000e+00  2.00000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "       [ 3. 25.  1.  0.  1.] ->   0      0      correct \n",
      "       [ 3. 31.  3.  0.  1.] ->   0      0      correct \n",
      "            [2. 3. 1. 1. 1.] ->   1      1      correct \n",
      "[ 3.00000000e+00  2.60000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 2.00000000e+00  3.50000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 1.00000000e+00  4.80000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "[ 3.00000000e+00  1.90000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 2.00000000e+00  3.40000000e+01 -5.55111512e-17  1.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "[ 1.00000000e+00  2.80000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      1      incorrect: [0.57920499 0.42079501]\n",
      "[ 2.00000000e+00  4.80000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "       [ 3. 39.  1.  5.  1.] ->   0      0      correct \n",
      "[ 1.00000000e+00  2.20000000e+01 -5.55111512e-17  2.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "[ 1.00000000e+00  1.90000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "       [ 2. 26.  1.  1.  0.] ->   1      0      incorrect: [0.06160636 0.93839364]\n",
      "[ 3.00000000e+00  1.60000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 2.00000000e+00  1.40000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "       [ 2. 54.  1.  0.  1.] ->   0      0      correct \n",
      "            [3. 7. 4. 1. 1.] ->   0      0      correct \n",
      "[ 3.00000000e+00  1.30000000e+01 -5.55111512e-17  2.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 2.00000000e+00  2.30000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "            [2. 8. 1. 1. 1.] ->   0      1      incorrect: [0.56312472 0.43687528]\n",
      "[ 2.00000000e+00  4.10000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 1.00000000e+00  4.80000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      1      incorrect: [0.77098187 0.22901813]\n",
      "[ 3.00000000e+00  1.80000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "[ 1.00000000e+00  3.30000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "       [ 3. 43.  1.  6.  0.] ->   0      0      correct \n",
      "[ 3.00000000e+00  4.40000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "       [ 2. 21.  1.  0.  1.] ->   0      0      correct \n",
      "[ 3.00000000e+00  2.80000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "  [3.   0.75 2.   1.   0.  ] ->   1      1      correct \n",
      "[ 1.00000000e+00  4.00000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "[ 1.00000000e+00  5.50000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "       [ 1. 49.  1.  1.  1.] ->   0      0      correct \n",
      "[ 3.00000000e+00  7.40000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 3.00000000e+00  4.05000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 3.00000000e+00  2.20000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 3.00000000e+00  3.20000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      1      incorrect: [0.85859443 0.14140557]\n",
      "       [ 1. 37.  1.  1.  1.] ->   0      0      correct \n",
      "[ 2.00000000e+00  1.85000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "       [ 1. 39.  1.  1.  0.] ->   1      1      correct \n",
      "[ 3.00000000e+00  1.60000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "[ 3.00000000e+00  3.60000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 3.00000000e+00  2.50000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 1.00000000e+00  3.60000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "       [ 2. 24.  2.  3.  0.] ->   1      1      correct \n",
      "       [ 3. 16.  1.  3.  1.] ->   0      0      correct \n",
      "[ 1.00000000e+00  6.00000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 3.00000000e+00  3.10000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 3.00000000e+00  3.20000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      1      incorrect: [0.85859443 0.14140557]\n",
      "[ 1.00000000e+00  3.50000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "[ 3.00000000e+00  2.10000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 2.00000000e+00  2.00000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "[ 3.00000000e+00  2.10000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 3.00000000e+00  5.55000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 1.00000000e+00  2.30000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   1      0      incorrect: [0.47535156 0.52464844]\n",
      "[ 3.00000000e+00  2.10000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      1      incorrect: [0.83184296 0.16815704]\n",
      "[ 2.00000000e+00  5.90000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "       [ 1. 54.  1.  0.  0.] ->   1      1      correct \n",
      "       [ 1. 50.  1.  0.  1.] ->   0      0      correct \n",
      "[ 1.00000000e+00  6.10000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 3.00000000e+00  2.70000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   0      1      incorrect: [0.55947287 0.44052713]\n",
      "       [ 3. 13.  4.  2.  1.] ->   0      0      correct \n",
      "       [ 3. 17.  2.  0.  1.] ->   0      0      correct \n",
      "       [ 3. 16.  1.  1.  1.] ->   0      0      correct \n",
      "[ 3.00000000e+00  5.10000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "[ 1.00000000e+00  3.50000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "[ 1.00000000e+00  1.70000000e+01 -5.55111512e-17  2.00000000e+00\n",
      "  1.00000000e+00] ->   1      1      correct \n",
      "       [ 3. 34.  1.  1.  1.] ->   0      0      correct \n",
      "       [ 3. 16.  1.  1.  0.] ->   1      1      correct \n",
      "[ 3.00000000e+00  2.20000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   0      1      incorrect: [0.50515918 0.49484082]\n",
      "[ 3.00000000e+00  5.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "       [ 3. 38.  4.  2.  0.] ->   0      0      correct \n",
      "[ 1.00000000e+00  2.20000000e+01 -5.55111512e-17  1.00000000e+00\n",
      "  0.00000000e+00] ->   1      1      correct \n",
      "       [ 2. 30.  1.  0.  1.] ->   0      0      correct \n",
      "[ 3.00000000e+00  4.40000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "       [ 2. 27.  1.  0.  1.] ->   0      0      correct \n",
      "[ 2.00000000e+00  2.80000000e+01 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->   0      0      correct \n",
      "\n",
      "correct predictions: 172 out of 201\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# how did it do on the testing data?\n",
    "#\n",
    "\n",
    "#\n",
    "# which one do we want: classifier or regressor?\n",
    "#\n",
    "\n",
    "def ascii_table_for_classifier(Xsc,y,nn,scaler):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(Xsc)            # all predictions\n",
    "    prediction_probs = nn.predict_proba(Xsc) # all prediction probabilities\n",
    "    Xpr = scaler.inverse_transform(Xsc)      # Xpr is the \"X to print\": unscaled data!\n",
    "    # count correct\n",
    "    num_correct = 0\n",
    "    # printing\n",
    "    print(f\"{'input ':>28s} -> {'pred':^6s} {'des.':^6s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        pred_probs = prediction_probs[i,:]\n",
    "        desired = y[i]\n",
    "        if pred != desired: result = \"  incorrect: \" + str(pred_probs)\n",
    "        else: result = \"  correct\"; num_correct += 1\n",
    "        # Xpr = Xsc  # if you want to see the scaled versions\n",
    "        print(f\"{Xpr[i,:]!s:>28s} -> {pred:^6.0f} {desired:^6.0f} {result:^10s}\") \n",
    "    print(f\"\\ncorrect predictions: {num_correct} out of {len(y)}\")\n",
    "    \n",
    "\n",
    "\n",
    "#\n",
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_classifier(X_test_scaled,\n",
    "                           y_test_scaled,\n",
    "                           nn_classifier,\n",
    "                           scaler)   \n",
    "#\n",
    "# other things...\n",
    "#\n",
    "if False:  # do we want to see all of the parameters?\n",
    "    nn = nn_classifier  # less to type?\n",
    "    print(\"\\n\\n+++++ parameters, weights, etc. +++++\\n\")\n",
    "    print(f\"\\nweights/coefficients:\\n\")\n",
    "    for wts in nn.coefs_:\n",
    "        print(wts)\n",
    "    print(f\"\\nintercepts: {nn.intercepts_}\")\n",
    "    print(f\"\\nall parameters: {nn.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input features are [3, 27, 1, 0, 1]\n",
      "nn.predict_proba ==  [[0.83738519 0.16261481]]\n",
      "prediction: [0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jason/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we have a predictive model!  Let's try it out...\n",
    "#\n",
    "\n",
    "def make_prediction( Features, nn, scaler ):\n",
    "    \"\"\" uses nn for predictions \"\"\"\n",
    "    print(\"input features are\", Features)\n",
    "    #  we make sure Features has the right shape (list-of-lists)\n",
    "    row = np.array( [Features] )  # makes an array-row\n",
    "    row = scaler.transform(row)   # scale according to scaler\n",
    "    print(\"nn.predict_proba == \", nn.predict_proba(row))   # probabilities of each\n",
    "    prediction = nn.predict(row)  # max!\n",
    "    return prediction\n",
    "    \n",
    "# our features -- note that the inputs don't have to be bits!\n",
    "Features = [ 3, 27, 1, 0, 1 ]      # whatever we'd like to test\n",
    "prediction = make_prediction(Features, nn_classifier, scaler)\n",
    "print(f\"prediction: {prediction}\")     # takes the max (nice to see them all!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Start of regression experiments... +++\n",
      "\n",
      "y_reg (age!)   is \n",
      " [29.      0.9167  2.     ... 26.5    27.     29.    ]\n",
      "X_reg (just features: first few rows) is \n",
      " [[1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 2. 1.]\n",
      " [1. 0. 1. 2. 0.]\n",
      " [1. 0. 1. 2. 1.]\n",
      " [1. 0. 1. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Here we set up for a regression model that will predict 'age'  (column index 2)  using\n",
    "#\n",
    "\n",
    "print(\"+++ Start of regression experiments... +++\\n\")\n",
    "#\n",
    "# Notice!  We're dropping column 3 from the features!\n",
    "#\n",
    "X_all = np.concatenate( (A[:,0:2], A[:,3:]),axis=1)  \n",
    "\n",
    "#\n",
    "# Notice!  We're using column 2 as the target!!\n",
    "#\n",
    "y_all = A[:,2]             # y (labels) ... is all of column 2 (age), all rows\n",
    "print(f\"y_reg (age!)   is \\n {y_all}\") \n",
    "print(f\"X_reg (just features: first few rows) is \\n {X_all[:5,:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label-_values_\n",
      " [20.  31.  15.  ... 19.  30.  30.5]\n",
      "\n",
      "features (a few)\n",
      " [[3. 0. 0. 0. 1.]\n",
      " [2. 0. 0. 0. 1.]\n",
      " [1. 1. 0. 1. 0.]\n",
      " [3. 0. 2. 0. 1.]\n",
      " [2. 1. 2. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we scramble the data, to give a different TRAIN/TEST split each time...\n",
    "# \n",
    "indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "# we scramble both X and y, necessarily with the same permutation\n",
    "X_all = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "y_all = y_all[indices]              # again...\n",
    "print(\"label-_values_\\n\",y_all)\n",
    "print(\"\\nfeatures (a few)\\n\", X_all[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with 803 rows;  testing with 201 rows\n",
      "\n",
      "Held-out data... (testing data: 201)\n",
      "y_test: [30.5    70.5    29.     49.     16.     50.     27.     39.      6.\n",
      "  0.9167 39.     23.     28.     26.      4.     30.     25.     31.\n",
      " 65.     26.     41.     30.      9.     33.     21.     60.     21.\n",
      " 33.     51.     27.      5.     18.     18.     45.     17.     40.\n",
      " 17.      2.     44.     26.     29.      0.8333 23.     30.     56.\n",
      " 32.5    33.     30.     28.     61.     14.     29.     40.     19.\n",
      " 24.     20.     54.      2.     40.     23.     42.     36.     45.\n",
      " 57.     36.     25.     15.     18.5    24.     16.     23.     21.\n",
      " 29.     62.     30.     51.     49.     39.     15.      2.     36.\n",
      " 20.     39.     22.     64.     54.     18.     45.     24.     36.\n",
      " 30.     47.     55.     38.     54.     18.     70.     49.     32.\n",
      " 30.     20.     22.     19.     27.     24.      9.     27.     22.\n",
      " 55.      0.3333 28.     65.     40.     22.     36.     31.     19.\n",
      " 18.     45.     33.     25.     63.     23.     40.     27.     28.5\n",
      " 19.     21.     40.     30.     50.      5.     34.     21.     18.\n",
      " 15.     19.     28.     29.     24.     42.     10.     40.     58.\n",
      " 24.     80.      2.     31.     28.     26.     65.     25.      8.\n",
      " 17.     18.     62.     40.     18.     20.     19.     16.     25.\n",
      " 33.     48.     27.     57.     76.     29.     48.     25.     30.\n",
      "  4.     24.     18.     45.     18.     21.     22.     19.     25.\n",
      " 52.     34.     55.     29.      9.     29.     39.     23.     27.\n",
      " 43.     43.     61.     48.      2.     28.     34.     27.     36.\n",
      " 25.     63.     36.    ]\n",
      "\n",
      "X_test (few rows): [[3. 0. 0. 0. 0.]\n",
      " [3. 0. 0. 0. 1.]\n",
      " [2. 1. 0. 2. 0.]\n",
      " [1. 1. 1. 0. 1.]\n",
      " [3. 0. 0. 0. 1.]]\n",
      "\n",
      "Data used for modeling... (training data: 803)\n",
      "y_train: [23.     25.     17.     58.     44.     24.     17.     21.     19.\n",
      " 45.     30.     22.     28.     42.     24.     25.     59.     33.\n",
      " 33.     32.     24.     50.     19.     34.     28.     40.     38.5\n",
      " 25.     41.     14.     25.     50.     22.     36.     18.     34.\n",
      " 19.     27.     21.      0.75   18.     47.     24.     47.     25.\n",
      " 17.     30.     25.     19.     52.     35.     20.     47.      6.\n",
      " 29.     16.     27.     26.     29.     21.     63.     28.      6.\n",
      " 30.     32.     64.      0.6667 54.     19.     34.     13.     41.\n",
      " 48.     31.     30.     25.      1.     16.     28.     23.     40.\n",
      " 23.     22.     31.     43.     27.     32.     22.     45.     26.\n",
      "  1.     51.     30.     34.     21.     37.     18.5    32.     20.\n",
      " 44.     28.     50.     24.     25.     55.     26.      3.     18.\n",
      " 27.     28.5     1.     30.     47.      0.8333 18.     22.     30.\n",
      " 22.      1.     23.     37.     25.     49.     35.     24.     18.\n",
      " 39.     64.     27.     20.     40.     26.     21.     50.     23.\n",
      "  3.     17.     30.5    56.     20.     42.     36.     40.     21.\n",
      " 58.     16.     43.     22.     22.     21.     24.     44.     26.\n",
      " 19.     32.     26.     32.      9.     28.     30.     63.      1.\n",
      " 11.     26.     22.     39.     25.     35.     24.     45.      8.\n",
      " 67.      6.     24.     18.     28.     45.     43.     31.     29.\n",
      " 18.     39.     40.     18.     47.     21.     35.     60.     21.\n",
      " 48.      9.     21.     21.     21.     32.5    18.     36.     28.\n",
      " 38.     31.     47.     24.     25.     18.     28.     64.     29.\n",
      " 58.      6.     39.     28.     50.     22.     21.     47.     18.\n",
      "  2.      5.     26.     32.5    17.     21.      9.     30.     16.\n",
      " 47.     24.     62.     21.     27.     30.     33.     49.     28.5\n",
      " 15.     26.     28.     31.     14.5    22.     18.     23.     59.\n",
      " 47.     31.     44.     41.     41.     45.     24.5    11.      3.\n",
      " 36.     19.     25.     28.     34.     39.     33.     30.     33.\n",
      " 36.     35.     26.     17.     46.     52.     24.     30.     20.\n",
      " 38.     57.     16.     33.     46.     40.5    30.     36.     51.\n",
      " 19.     38.     29.     50.     45.     38.     60.     25.     45.\n",
      " 30.     11.5    40.     23.     17.     23.     57.     21.     18.\n",
      " 26.     39.      0.9167 36.     45.5    21.     32.     28.      0.1667\n",
      " 55.     36.     27.     71.     22.     16.     22.     43.     13.\n",
      " 26.     38.     42.     58.     33.     61.     38.     14.     21.\n",
      " 49.      4.     29.     42.     33.      2.     18.     33.     42.\n",
      " 30.     23.     46.     45.     31.     30.     46.     52.     30.\n",
      " 28.     14.     29.     31.     26.     21.     40.5    13.     34.\n",
      " 30.     35.     10.     24.     22.     23.     19.     47.     41.\n",
      " 27.     21.     29.     22.     24.     22.     18.     37.     18.\n",
      " 28.     27.     62.     36.     23.     24.     28.     31.     28.\n",
      " 45.     29.     49.     31.     17.     27.     51.     52.      7.\n",
      " 61.     50.     43.     49.     45.     17.     17.     30.     36.\n",
      "  7.     35.     32.     20.      1.     13.     41.     28.     35.\n",
      " 33.     40.      4.     26.     35.     19.     24.     42.     27.\n",
      " 10.     39.     22.     22.     12.     21.     21.     11.     45.\n",
      " 22.     29.     58.     27.     30.     48.     36.5    28.     14.\n",
      " 18.     30.     36.     43.     21.     47.     18.     41.     27.\n",
      " 60.     39.      9.     16.     14.5    44.     28.     34.     33.\n",
      " 24.     25.     30.     24.     32.      2.     22.     25.     27.\n",
      " 22.     36.     33.     32.     16.     61.     25.     31.     32.\n",
      " 15.     34.     32.5    40.     19.     16.     41.     26.5    48.\n",
      " 22.     36.     36.      3.     50.     36.     19.     22.     25.\n",
      " 30.     22.     26.     39.     26.     32.      2.     42.     37.\n",
      " 24.     23.     24.     20.     22.     50.     54.     50.     33.\n",
      " 48.     17.     21.     31.     42.     26.     53.     26.     27.\n",
      " 35.     55.     25.     34.     57.     36.     49.     20.     21.\n",
      "  4.      1.     22.      8.     17.     18.     42.     27.     55.5\n",
      " 19.     11.     38.     22.5    34.5    21.     18.     28.     42.\n",
      " 32.     53.     42.      2.     42.     22.     24.     17.     36.\n",
      " 14.      8.     54.     31.     21.     24.     59.     24.     23.\n",
      "  0.75   33.      0.75   48.      3.     24.     36.      3.     20.\n",
      " 44.     70.     37.     39.     35.     10.     38.     19.     35.\n",
      " 45.     20.     35.     24.     18.     30.      4.     36.     60.5\n",
      " 22.     42.     16.     22.     26.     32.     32.      4.     32.\n",
      " 24.     18.5    30.     18.     51.     21.     29.     45.     33.\n",
      " 24.     66.     35.     32.     46.     21.     23.     34.5    18.\n",
      " 29.     27.     33.     36.5    19.     20.     22.     39.     35.\n",
      " 29.     29.     30.     60.     25.      0.4167 44.     31.     18.\n",
      " 50.     24.     22.     29.     34.     43.     21.     24.     71.\n",
      " 22.     25.     30.     29.     16.     34.     27.     29.     38.\n",
      " 45.     12.     25.     26.     16.     22.      9.     23.     21.\n",
      " 40.     31.     26.      4.     23.5    35.     48.     26.     28.\n",
      "  5.     18.      2.     18.     13.     20.     48.     19.     37.\n",
      " 25.     15.     24.     20.     29.     16.     24.     16.     24.\n",
      " 46.     53.      7.     38.     21.     14.     38.     27.     26.\n",
      " 35.     16.     22.     25.     31.     31.      0.8333 22.      8.\n",
      " 27.     31.     28.     40.     39.     39.     54.     56.     28.\n",
      " 42.     39.     17.     20.     23.     34.     48.     29.     44.\n",
      " 39.     38.     20.     48.     18.     30.     20.5    36.     30.\n",
      " 45.     54.     35.     29.     35.     51.     25.     20.     26.\n",
      " 36.     74.     19.     17.     22.     23.     22.     16.     20.\n",
      "  7.     28.      8.      3.     32.     51.     44.     32.     35.\n",
      " 41.     19.     25.      9.     20.     45.5    14.     32.     29.\n",
      " 26.     50.     55.     47.     31.     55.     21.     37.     45.\n",
      " 27.     48.     41.     38.     42.     17.     35.     21.     24.\n",
      " 22.     30.     47.     37.     17.      5.     56.      2.     32.\n",
      "  9.     24.     35.     37.     52.     29.      6.     19.     30.\n",
      " 24.     60.    ]\n",
      "\n",
      "X_train (few rows): [[1. 1. 0. 1. 0.]\n",
      " [2. 1. 0. 1. 0.]\n",
      " [3. 0. 2. 0. 1.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We next separate into test data and training data ... \n",
    "#    + We will train on the training data...\n",
    "#    + We will _not_ look at the testing data to build the model\n",
    "#\n",
    "# Then, afterward, we will test on the testing data -- and see how well we do!\n",
    "#\n",
    "\n",
    "#\n",
    "# a common convention:  train on 80%, test on 20%    Let's define the TEST_PERCENT\n",
    "#\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
    "\n",
    "print(f\"training with {len(y_train)} rows;  testing with {len(y_test)} rows\\n\" )\n",
    "\n",
    "print(f\"Held-out data... (testing data: {len(y_test)})\")\n",
    "print(f\"y_test: {y_test}\\n\")\n",
    "print(f\"X_test (few rows): {X_test[0:5,:]}\")  # 5 rows\n",
    "print()\n",
    "print(f\"Data used for modeling... (training data: {len(y_train)})\")\n",
    "print(f\"y_train: {y_train}\\n\")\n",
    "print(f\"X_train (few rows): {X_train[0:5,:]}\")  # 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    input  -> pred  des. \n",
      "[-1.46802115  1.25119751 -0.52607307  0.77935143 -1.34450448] -> ?     23.000000\n",
      "[-0.28327644  1.25119751 -0.52607307  0.77935143 -1.34450448] -> ?     25.000000\n",
      "[ 0.90146826 -0.79923433  1.662718   -0.49005871  0.74376844] -> ?     17.000000\n",
      "[-1.46802115  1.25119751 -0.52607307 -0.49005871 -1.34450448] -> ?     58.000000\n",
      "[-1.46802115  1.25119751 -0.52607307  0.77935143 -1.34450448] -> ?     44.000000\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# for NNets, it's important to keep the feature values near 0, say -1. to 1. or so\n",
    "#    This is done through the \"StandardScaler\" in scikit-learn\n",
    "# \n",
    "USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "# we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "if USE_SCALER == True:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)  # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "else:\n",
    "    # this one does no scaling!  We still create it to be consistent:\n",
    "    scaler = StandardScaler(copy=True, with_mean=False, with_std=False) # no scaling\n",
    "    scaler.fit(X_train)  # still need to fit, though it does not change...\n",
    "\n",
    "scaler   # is now defined and ready to use...\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Here are our scaled training and testing sets:\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train) # scale!\n",
    "X_test_scaled = scaler.transform(X_test) # scale!\n",
    "\n",
    "y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "y_test_scaled = y_test  # not using the scaler\n",
    "\n",
    "def ascii_table(X,y):\n",
    "    \"\"\" print a table of binary inputs and outputs \"\"\"\n",
    "    print(f\"{'input ':>58s} -> {'pred':<5s} {'des.':<5s}\") \n",
    "    for i in range(len(y)):\n",
    "        print(f\"{X[i,:]!s:>58s} -> {'?':<5s} {y[i]:<5.6f}\")   # !s is str ...\n",
    "    \n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5])\n",
    "\n",
    "#\n",
    "# Note that the zeros have become -1's\n",
    "# and the 1's have stayed 1's\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "Iteration 1, loss = 282.83439312\n",
      "Iteration 2, loss = 118.18283791\n",
      "Iteration 3, loss = 92.70882061\n",
      "Iteration 4, loss = 150.94855361\n",
      "Iteration 5, loss = 94.92155937\n",
      "Iteration 6, loss = 103.61415605\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 7, loss = 123.04752372\n",
      "Iteration 8, loss = 111.93634876\n",
      "Iteration 9, loss = 102.82551425\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 10, loss = 94.76567826\n",
      "Iteration 11, loss = 95.76690769\n",
      "Iteration 12, loss = 91.61539671\n",
      "Iteration 13, loss = 88.02471833\n",
      "Iteration 14, loss = 87.34603102\n",
      "Iteration 15, loss = 88.51968293\n",
      "Iteration 16, loss = 87.63983196\n",
      "Iteration 17, loss = 87.57983469\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 18, loss = 87.14551342\n",
      "Iteration 19, loss = 87.07954387\n",
      "Iteration 20, loss = 87.10408539\n",
      "Iteration 21, loss = 87.20676009\n",
      "Iteration 22, loss = 87.31210534\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 23, loss = 87.21331371\n",
      "Iteration 24, loss = 87.16100028\n",
      "Iteration 25, loss = 87.12556780\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 26, loss = 87.09293516\n",
      "Iteration 27, loss = 87.07560066\n",
      "Iteration 28, loss = 87.06060434\n",
      "Iteration 29, loss = 87.05318059\n",
      "Iteration 30, loss = 87.04677822\n",
      "Iteration 31, loss = 87.04065870\n",
      "Iteration 32, loss = 87.03814613\n",
      "Iteration 33, loss = 87.03147239\n",
      "Iteration 34, loss = 87.02947899\n",
      "Iteration 35, loss = 87.02536325\n",
      "Iteration 36, loss = 87.02899752\n",
      "Iteration 37, loss = 87.02702130\n",
      "Iteration 38, loss = 87.02321103\n",
      "Iteration 39, loss = 87.02214448\n",
      "Iteration 40, loss = 87.02242297\n",
      "Iteration 41, loss = 87.02261036\n",
      "Iteration 42, loss = 87.02226238\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 43, loss = 87.02404714\n",
      "Iteration 44, loss = 87.02705943\n",
      "Iteration 45, loss = 87.02787243\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 46, loss = 87.02846381\n",
      "Iteration 47, loss = 87.02862717\n",
      "Iteration 48, loss = 87.02873233\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 49, loss = 87.02884077\n",
      "Iteration 50, loss = 87.02879631\n",
      "Iteration 51, loss = 87.02880142\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "The (squared) prediction error (the loss) is 87.02880141668926\n",
      "And, its square root: 9.328922843323834\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# MLPRegressor predicts _floating-point_ outputs\n",
    "#\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "nn_regressor = MLPRegressor(hidden_layer_sizes=(6,7), \n",
    "                    max_iter=200,          # how many training epochs\n",
    "                    activation=\"tanh\",     # the activation function\n",
    "                    solver='sgd',          # the optimizer\n",
    "                    verbose=True,          # do we want to watch as it trains?\n",
    "                    shuffle=True,          # shuffle each epoch?\n",
    "                    random_state=None,     # use for reproducibility\n",
    "                    learning_rate_init=.1, # how much of each error to back-propagate\n",
    "                    learning_rate = 'adaptive')  # how to handle the learning_rate\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_regressor.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"++++++++++  TRAINING:   end  +++++++++++++++\")\n",
    "\n",
    "print(f\"The (squared) prediction error (the loss) is {nn_regressor.loss_}\")\n",
    "print(f\"And, its square root: {nn_regressor.loss_ ** 0.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      input  ->   pred    des.    absdiff  \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +26.365  +30.500    4.135   \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +70.500    37.578  \n",
      "[ 2.00000000e+00  1.00000000e+00 -5.55111512e-17  2.00000000e+00\n",
      "  0.00000000e+00] ->  +26.365  +29.000    2.635   \n",
      "            [1. 1. 1. 0. 1.] ->  +32.922  +49.000    16.078  \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +16.000    16.922  \n",
      "            [2. 0. 1. 0. 1.] ->  +26.365  +50.000    23.635  \n",
      "            [2. 0. 1. 0. 0.] ->  +26.365  +27.000    0.635   \n",
      "[ 1.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +39.000    6.078   \n",
      "[ 1.00000000e+00  1.00000000e+00 -5.55111512e-17  2.00000000e+00\n",
      "  1.00000000e+00] ->  +26.365  +6.000    20.365  \n",
      "            [2. 1. 1. 2. 0.] ->  +26.365  +0.917    25.449  \n",
      "[ 2.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +39.000    6.078   \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +23.000    9.922   \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +28.000    4.922   \n",
      "[ 1.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +32.922  +26.000    6.922   \n",
      "            [3. 1. 1. 1. 1.] ->  +26.365  +4.000    22.365  \n",
      "[ 1.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +32.922  +30.000    2.922   \n",
      "            [1. 1. 1. 0. 0.] ->  +26.365  +25.000    1.365   \n",
      "[ 2.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +31.000    1.922   \n",
      "[ 1.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +65.000    32.078  \n",
      "            [3. 0. 1. 0. 1.] ->  +26.365  +26.000    0.365   \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  5.00000000e+00\n",
      "  0.00000000e+00] ->  +26.365  +41.000    14.635  \n",
      "[ 2.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +32.922  +30.000    2.922   \n",
      "            [3. 0. 5. 2. 1.] ->  +26.365  +9.000    17.365  \n",
      "            [3. 0. 1. 1. 1.] ->  +26.365  +33.000    6.635   \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +21.000    11.922  \n",
      "            [1. 1. 1. 0. 0.] ->  +26.365  +60.000    33.635  \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +21.000    11.922  \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +33.000    0.078   \n",
      "            [1. 1. 1. 0. 0.] ->  +26.365  +51.000    24.635  \n",
      "[ 2.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +27.000    5.922   \n",
      "            [3. 1. 2. 1. 0.] ->  +26.365  +5.000    21.365  \n",
      "[ 3.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +26.365  +18.000    8.365   \n",
      "            [1. 1. 1. 0. 0.] ->  +26.365  +18.000    8.365   \n",
      "[ 1.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +45.000    12.078  \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +17.000    15.922  \n",
      "            [2. 1. 1. 1. 0.] ->  +26.365  +40.000    13.635  \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +17.000    15.922  \n",
      "            [3. 0. 4. 1. 1.] ->  +26.365  +2.000    24.365  \n",
      "[ 1.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +32.922  +44.000    11.078  \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +26.000    6.922   \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +29.000    3.922   \n",
      "[ 2.00000000e+00  1.00000000e+00 -5.55111512e-17  2.00000000e+00\n",
      "  1.00000000e+00] ->  +26.365  +0.833    25.532  \n",
      "[ 2.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +23.000    9.922   \n",
      "[ 1.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +32.922  +30.000    2.922   \n",
      "[ 1.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +56.000    23.078  \n",
      "[ 2.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +32.922  +32.500    0.422   \n",
      "            [2. 1. 1. 2. 0.] ->  +26.365  +33.000    6.635   \n",
      "[ 2.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +26.365  +30.000    3.635   \n",
      "[ 1.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +28.000    4.922   \n",
      "            [1. 0. 1. 3. 1.] ->  +26.365  +61.000    34.635  \n",
      "            [2. 1. 1. 0. 0.] ->  +26.365  +14.000    12.365  \n",
      "            [2. 0. 1. 0. 0.] ->  +26.365  +29.000    2.635   \n",
      "[ 2.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +32.922  +40.000    7.078   \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +19.000    13.922  \n",
      "[ 3.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +26.365  +24.000    2.365   \n",
      "[ 3.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +26.365  +20.000    6.365   \n",
      "            [1. 1. 1. 0. 1.] ->  +32.922  +54.000    21.078  \n",
      "[ 3.00000000e+00  1.00000000e+00 -5.55111512e-17  1.00000000e+00\n",
      "  0.00000000e+00] ->  +26.365  +2.000    24.365  \n",
      "[ 1.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +32.922  +40.000    7.078   \n",
      "[ 1.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +23.000    9.922   \n",
      "            [2. 0. 1. 0. 1.] ->  +26.365  +42.000    15.635  \n",
      "[ 1.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +32.922  +36.000    3.078   \n",
      "[ 2.00000000e+00  1.00000000e+00 -5.55111512e-17  2.00000000e+00\n",
      "  0.00000000e+00] ->  +26.365  +45.000    18.635  \n",
      "            [1. 0. 1. 1. 1.] ->  +26.365  +57.000    30.635  \n",
      "[ 1.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +36.000    3.078   \n",
      "            [2. 1. 1. 1. 0.] ->  +26.365  +25.000    1.365   \n",
      "            [3. 1. 1. 0. 0.] ->  +26.365  +15.000    11.365  \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +18.500    14.422  \n",
      "[ 2.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +24.000    8.922   \n",
      "[ 3.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +26.365  +16.000    10.365  \n",
      "            [3. 0. 1. 0. 1.] ->  +26.365  +23.000    3.365   \n",
      "[ 3.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +26.365  +21.000    5.365   \n",
      "[ 3.00000000e+00  1.00000000e+00 -5.55111512e-17  2.00000000e+00\n",
      "  0.00000000e+00] ->  +26.365  +29.000    2.635   \n",
      "[ 2.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +62.000    29.078  \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +30.000    2.922   \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +51.000    18.078  \n",
      "            [1. 1. 1. 0. 0.] ->  +26.365  +49.000    22.635  \n",
      "            [1. 1. 1. 1. 0.] ->  +26.365  +39.000    12.635  \n",
      "            [3. 0. 1. 1. 1.] ->  +26.365  +15.000    11.365  \n",
      "            [3. 0. 3. 1. 1.] ->  +26.365  +2.000    24.365  \n",
      "[ 1.00000000e+00  1.00000000e+00 -5.55111512e-17  1.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +36.000    3.078   \n",
      "            [3. 1. 1. 1. 1.] ->  +26.365  +20.000    6.365   \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +39.000    6.078   \n",
      "[ 3.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +26.365  +22.000    4.365   \n",
      "[ 1.00000000e+00  1.00000000e+00 -5.55111512e-17  2.00000000e+00\n",
      "  0.00000000e+00] ->  +26.365  +64.000    37.635  \n",
      "            [2. 0. 1. 0. 1.] ->  +26.365  +54.000    27.635  \n",
      "[ 2.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +18.000    14.922  \n",
      "            [1. 0. 1. 0. 1.] ->  +32.922  +45.000    12.078  \n",
      "            [2. 1. 1. 2. 0.] ->  +26.365  +24.000    2.365   \n",
      "            [3. 0. 1. 1. 1.] ->  +26.365  +36.000    9.635   \n",
      "[ 2.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +30.000    2.922   \n",
      "            [1. 1. 1. 0. 0.] ->  +26.365  +47.000    20.635  \n",
      "            [1. 0. 1. 1. 1.] ->  +26.365  +55.000    28.635  \n",
      "[ 1.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +32.922  +38.000    5.078   \n",
      "            [1. 1. 1. 0. 0.] ->  +26.365  +54.000    27.635  \n",
      "            [3. 0. 1. 0. 0.] ->  +26.365  +18.000    8.365   \n",
      "            [1. 0. 1. 1. 1.] ->  +26.365  +70.000    43.635  \n",
      "[ 1.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +49.000    16.078  \n",
      "[ 1.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +32.922  +32.000    0.922   \n",
      "            [3. 0. 1. 0. 0.] ->  +26.365  +30.000    3.635   \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +20.000    12.922  \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +22.000    10.922  \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +19.000    13.922  \n",
      "[ 1.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +27.000    5.922   \n",
      "[ 3.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +26.365  +24.000    2.365   \n",
      "            [3. 0. 2. 2. 0.] ->  +26.365  +9.000    17.365  \n",
      "[ 3.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +26.365  +27.000    0.635   \n",
      "[ 2.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +32.922  +22.000    10.922  \n",
      "[ 1.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +32.922  +55.000    22.078  \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  2.00000000e+00\n",
      "  1.00000000e+00] ->  +26.365  +0.333    26.032  \n",
      "            [2. 1. 1. 0. 0.] ->  +26.365  +28.000    1.635   \n",
      "[ 1.00000000e+00  0.00000000e+00 -5.55111512e-17  1.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +65.000    32.078  \n",
      "[ 2.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +32.922  +40.000    7.078   \n",
      "            [1. 1. 1. 0. 0.] ->  +26.365  +22.000    4.365   \n",
      "            [3. 0. 1. 0. 1.] ->  +26.365  +36.000    9.635   \n",
      "            [1. 1. 1. 0. 0.] ->  +26.365  +31.000    4.635   \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +19.000    13.922  \n",
      "[ 2.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +18.000    14.922  \n",
      "[ 1.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +32.922  +45.000    12.078  \n",
      "[ 2.00000000e+00  1.00000000e+00 -5.55111512e-17  2.00000000e+00\n",
      "  0.00000000e+00] ->  +26.365  +33.000    6.635   \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +25.000    7.922   \n",
      "[ 3.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +26.365  +63.000    36.635  \n",
      "[ 2.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +23.000    9.922   \n",
      "            [3. 0. 1. 0. 0.] ->  +26.365  +40.000    13.635  \n",
      "            [1. 1. 1. 0. 1.] ->  +32.922  +27.000    5.922   \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +28.500    4.422   \n",
      "[ 2.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +19.000    13.922  \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +21.000    11.922  \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +40.000    7.078   \n",
      "            [2. 1. 3. 0. 0.] ->  +26.365  +30.000    3.635   \n",
      "            [1. 0. 1. 1. 1.] ->  +26.365  +50.000    23.635  \n",
      "            [2. 1. 1. 2. 0.] ->  +26.365  +5.000    21.365  \n",
      "            [2. 0. 1. 0. 1.] ->  +26.365  +34.000    7.635   \n",
      "            [2. 0. 1. 0. 1.] ->  +26.365  +21.000    5.365   \n",
      "            [3. 0. 1. 1. 1.] ->  +26.365  +18.000    8.365   \n",
      "[ 1.00000000e+00  1.00000000e+00 -5.55111512e-17  1.00000000e+00\n",
      "  0.00000000e+00] ->  +32.922  +15.000    17.922  \n",
      "            [1. 1. 1. 0. 0.] ->  +26.365  +19.000    7.365   \n",
      "[ 2.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +28.000    4.922   \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +26.365  +29.000    2.635   \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +24.000    8.922   \n",
      "[ 2.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +42.000    9.078   \n",
      "            [3. 0. 5. 2. 0.] ->  +26.365  +10.000    16.365  \n",
      "            [2. 0. 1. 0. 1.] ->  +26.365  +40.000    13.635  \n",
      "[ 1.00000000e+00  1.00000000e+00 -5.55111512e-17  1.00000000e+00\n",
      "  0.00000000e+00] ->  +32.922  +58.000    25.078  \n",
      "[ 2.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +32.922  +24.000    8.922   \n",
      "[ 1.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +80.000    47.078  \n",
      "            [3. 0. 4. 2. 0.] ->  +26.365  +2.000    24.365  \n",
      "            [3. 0. 1. 0. 0.] ->  +26.365  +31.000    4.635   \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +28.000    4.922   \n",
      "[ 3.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +26.365  +26.000    0.365   \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +65.000    32.078  \n",
      "[ 3.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +26.365  +25.000    1.365   \n",
      "            [3. 0. 3. 1. 0.] ->  +26.365  +8.000    18.365  \n",
      "            [3. 0. 1. 0. 1.] ->  +26.365  +17.000    9.365   \n",
      "            [1. 0. 1. 0. 1.] ->  +32.922  +18.000    14.922  \n",
      "[ 2.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +62.000    29.078  \n",
      "[ 1.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +40.000    7.078   \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +18.000    14.922  \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +26.365  +20.000    6.365   \n",
      "[ 2.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +19.000    13.922  \n",
      "[ 2.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +16.000    16.922  \n",
      "            [1. 0. 1. 2. 0.] ->  +26.365  +25.000    1.365   \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +33.000    0.078   \n",
      "            [1. 1. 1. 3. 0.] ->  +26.365  +48.000    21.635  \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +26.365  +27.000    0.635   \n",
      "            [1. 0. 1. 0. 1.] ->  +32.922  +57.000    24.078  \n",
      "            [1. 1. 1. 0. 0.] ->  +26.365  +76.000    49.635  \n",
      "[ 2.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +32.922  +29.000    3.922   \n",
      "            [1. 1. 1. 0. 1.] ->  +32.922  +48.000    15.078  \n",
      "[ 3.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +26.365  +25.000    1.365   \n",
      "[ 2.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +32.922  +30.000    2.922   \n",
      "[ 3.00000000e+00  1.00000000e+00 -5.55111512e-17  2.00000000e+00\n",
      "  0.00000000e+00] ->  +26.365  +4.000    22.365  \n",
      "[ 1.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +32.922  +24.000    8.922   \n",
      "[ 2.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +18.000    14.922  \n",
      "[ 1.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +45.000    12.078  \n",
      "[ 2.00000000e+00  1.00000000e+00 -5.55111512e-17  2.00000000e+00\n",
      "  0.00000000e+00] ->  +26.365  +18.000    8.365   \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +21.000    11.922  \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +22.000    10.922  \n",
      "            [2. 1. 1. 0. 0.] ->  +26.365  +19.000    7.365   \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +25.000    7.922   \n",
      "[ 2.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +52.000    19.078  \n",
      "            [2. 0. 1. 0. 1.] ->  +26.365  +34.000    7.635   \n",
      "            [1. 0. 1. 0. 1.] ->  +32.922  +55.000    22.078  \n",
      "            [2. 1. 1. 0. 0.] ->  +26.365  +29.000    2.635   \n",
      "[ 3.00000000e+00  1.00000000e+00 -5.55111512e-17  1.00000000e+00\n",
      "  1.00000000e+00] ->  +26.365  +9.000    17.365  \n",
      "[ 3.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +26.365  +29.000    2.635   \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  5.00000000e+00\n",
      "  0.00000000e+00] ->  +26.365  +39.000    12.635  \n",
      "[ 3.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  0.00000000e+00] ->  +26.365  +23.000    3.365   \n",
      "[ 3.00000000e+00  1.00000000e+00 -5.55111512e-17  1.00000000e+00\n",
      "  0.00000000e+00] ->  +26.365  +27.000    0.635   \n",
      "            [3. 0. 1. 6. 0.] ->  +26.365  +43.000    16.635  \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +43.000    10.078  \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +61.000    28.078  \n",
      "            [2. 1. 1. 2. 0.] ->  +26.365  +48.000    21.635  \n",
      "            [3. 0. 4. 1. 1.] ->  +26.365  +2.000    24.365  \n",
      "            [2. 1. 1. 0. 0.] ->  +26.365  +28.000    1.635   \n",
      "            [3. 0. 1. 1. 1.] ->  +26.365  +34.000    7.635   \n",
      "[ 1.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +27.000    5.922   \n",
      "            [1. 1. 1. 2. 0.] ->  +26.365  +36.000    9.635   \n",
      "[ 3.00000000e+00  1.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +26.365  +25.000    1.365   \n",
      "            [2. 0. 1. 0. 1.] ->  +26.365  +63.000    36.635  \n",
      "[ 3.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00\n",
      "  1.00000000e+00] ->  +32.922  +36.000    3.078   \n",
      "\n",
      "+++++   +++++      +++++   +++++   \n",
      "average abs error: 12.521325561167675\n",
      "+++++   +++++      +++++   +++++   \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# how did it do? now we're making progress (by regressing)\n",
    "#\n",
    "\n",
    "def ascii_table_for_regressor(Xsc,y,nn,scaler):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(Xsc) # all predictions\n",
    "    Xpr = scaler.inverse_transform(Xsc)  # Xpr is the \"X to print\": unscaled data!\n",
    "    # measure error\n",
    "    error = 0.0\n",
    "    # printing\n",
    "    print(f\"{'input ':>28s} ->  {'pred':^6s}  {'des.':^6s}  {'absdiff':^10s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        desired = y[i]\n",
    "        result = abs(desired - pred)\n",
    "        error += result\n",
    "        # Xpr = Xsc   # if you'd like to see the scaled values\n",
    "        print(f\"{Xpr[i,:]!s:>28s} ->  {pred:<+6.3f}  {desired:<+6.3f}  {result:^10.3f}\") \n",
    "\n",
    "    print(\"\\n\" + \"+++++   +++++      +++++   +++++   \")\n",
    "    print(f\"average abs error: {error/len(y)}\")\n",
    "    print(\"+++++   +++++      +++++   +++++   \")\n",
    "    \n",
    "#\n",
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_regressor(X_test_scaled,\n",
    "                          y_test_scaled,\n",
    "                          nn_regressor,\n",
    "                          scaler)   # this is our own f'n, above\n",
    "#\n",
    "# other things...\n",
    "#\n",
    "if False:  # do we want to see these details?\n",
    "    nn = nn_regressor  # less to type?\n",
    "    print(\"\\n\\n+++++ parameters, weights, etc. +++++\\n\")\n",
    "    print(f\"\\nweights/coefficients:\\n\")\n",
    "    for wts in nn.coefs_:\n",
    "        print(wts)\n",
    "    print(f\"\\nintercepts: {nn.intercepts_}\")\n",
    "    print(f\"\\nall parameters: {nn.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
