{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "####  Modeling with both proximity and conditionals:  Neural Nets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   We demonstrate _both_ clasification + regression for bitwise functions:\n",
    "\n",
    "#     + function #1:  MAJ, the \"majority\" function\n",
    "#                     three bits input, the most-appearing bit is the output \n",
    "\n",
    "#     + function #2:  XOR, the \"xor\" or \"odd # of 1's\" function \n",
    "#                     three bits input, output is their sum%2 \n",
    "#                     that is, 1 if there is an odd # of 1's, 0 if an even # of 1's\n",
    "#   \n",
    "#   From here, we'll use NNets for the births and iris datasets\n",
    "#     + births is complete\n",
    "#     + iris has been started.  Your task: to complete its analysis.\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries!\n",
    "import numpy as np      # numpy is Python's \"array\" library\n",
    "import pandas as pd     # Pandas is Python's \"data\" library (\"dataframe\" == spreadsheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's read in our data...\n",
    "# \n",
    "# for read_csv, use header=0 when row 0 is a header row\n",
    "# \n",
    "filename = 'xor_cleaned.csv'\n",
    "filename = 'maj_cleaned.csv'\n",
    "df = pd.read_csv(filename, header=0)   # encoding=\"latin1\" et al.\n",
    "print(f\"{filename} : file read into a pandas dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# let's look at our pandas dataframe  \n",
    "#\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# let's keep our column names in variables, for reference\n",
    "#\n",
    "COLUMNS = df.columns            # \"list\" of columns\n",
    "print(f\"COLUMNS is {COLUMNS}\\n\")  \n",
    "  # It's a \"pandas\" list, called an Index\n",
    "  # use it just as a Python list of strings:\n",
    "print(f\"COLUMNS[0] is {COLUMNS[0]}\\n\")\n",
    "\n",
    "# let's create a dictionary to look up any column index by name\n",
    "COL_INDEX = {}\n",
    "for i, name in enumerate(COLUMNS):\n",
    "    COL_INDEX[name] = i  # using the name (as key), look up the value (i)\n",
    "print(f\"COL_INDEX is {COL_INDEX}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all of scikit-learn's ML routines need numbers, not strings\n",
    "#   ... even for categories/classifications (like species!)\n",
    "#   so, we will convert the flower-species to numbers:\n",
    "\n",
    "SPECIES = ['zero','one']   # int to str\n",
    "SPECIES_INDEX = {'zero':0,'one':1}  # str to int\n",
    "\n",
    "def convert_species(speciesname):\n",
    "    \"\"\" return the species index (a unique integer/category) \"\"\"\n",
    "    #print(f\"converting {speciesname}...\")\n",
    "    return SPECIES_INDEX[speciesname]\n",
    "\n",
    "# Let's try it out...\n",
    "for name in SPECIES:\n",
    "    print(f\"{name} maps to {convert_species(name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# let's convert our dataframe to a numpy array, named A\n",
    "#    Our ML library, scikit-learn operates entirely on numpy arrays.\n",
    "#\n",
    "A = df.to_numpy()    \n",
    "A = A.astype('float64')   # and make things floating-point\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This is a small table of data... \n",
    "# \n",
    "# Alternaively, we could just define any bitwise function, by hand! :-)\n",
    "#\n",
    "\n",
    "\n",
    "# print(\"+++ Start of MAJ (majority) example +++\\n\")\n",
    "\n",
    "# A = np.asarray( [ \n",
    "#                     [0,0,0,  0],  # three input bits, one output bit (MAJ)\n",
    "#                     [0,0,1,  0],   \n",
    "#                     [0,1,0,  0],  \n",
    "#                     [0,1,1,  1],   \n",
    "#                     [1,0,0,  0],  \n",
    "#                     [1,0,1,  1],   \n",
    "#                     [1,1,0,  1],  \n",
    "#                     [1,1,1,  1],\n",
    "#                 ])\n",
    "\n",
    "\n",
    "# print(\"+++ Start of XOR (exclusive or == odd #of 1's) example +++\\n\")\n",
    "\n",
    "# A = np.asarray( [ \n",
    "#                     [0,0,0,  0],  # three input bits, one output bit (XOR) odd # of 1's\n",
    "#                     [0,0,1,  1],   \n",
    "#                     [0,1,0,  1],  \n",
    "#                     [0,1,1,  0],   \n",
    "#                     [1,0,0,  1],  \n",
    "#                     [1,0,1,  0],   \n",
    "#                     [1,1,0,  0],  \n",
    "#                     [1,1,1,  1],\n",
    "#                 ])\n",
    "\n",
    "A = A.astype('float64')   # and make things floating-point\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Let's split into features and labels/categories\n",
    "#\n",
    "\n",
    "# Here, we call it X_def and y_def, because it's data \"from definition,\" not observation\n",
    "\n",
    "X_def = A[:,0:3].copy()   # We make a copy so we don't change A\n",
    "y_def = A[:,3].copy()\n",
    "\n",
    "def ascii_table(X,y):\n",
    "    \"\"\" print a table of binary inputs and outputs \"\"\"\n",
    "    print(f\"{'input ':>18s} -> {'pred':<5s} {'des.':<5s}\") \n",
    "    for i in range(len(y)):\n",
    "        print(f\"{X[i,:]!s:>18s} -> {'?':<5s} {y[i]:<5.0f}\")   # !s is str ...\n",
    "        \n",
    "ascii_table(X_def,y_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# we can scramble the remaining data if we want to...\n",
    "# \n",
    "\n",
    "# Here, we switch to X_all (features, inputs) and y_all (output / species)\n",
    "#       to match our other modeling workflows\n",
    "\n",
    "SCRAMBLE = False\n",
    "if SCRAMBLE == True:\n",
    "    NUM_ROWS = len(y_def)\n",
    "    indices = np.random.permutation(NUM_ROWS)  # this scrambles the data each time\n",
    "    X_all = X_def[indices]\n",
    "    y_all = y_def[indices]\n",
    "else:\n",
    "    X_all = X_def  # don't scramble\n",
    "    y_all = y_def\n",
    "\n",
    "ascii_table(X_all,y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# here, we _cheat_ by letting the full dataset \n",
    "# be _both_ the training and testing sets.  (There are too few otherwise!)\n",
    "#\n",
    "X_train = X_all.copy()\n",
    "y_train = y_all.copy()\n",
    "\n",
    "X_test = X_all.copy()\n",
    "y_test = y_all.copy()\n",
    "\n",
    "ascii_table(X_train,y_train)    # same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# for NNets, it's important to keep the feature values near 0, say -1. to 1. or so\n",
    "#    This is done through the \"StandardScaler\" in scikit-learn\n",
    "# \n",
    "USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "# we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "if USE_SCALER == True:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)  # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "else:\n",
    "    # this one does no scaling!  We still create it to be consistent:\n",
    "    scaler = StandardScaler(copy=True, with_mean=False, with_std=False)\n",
    "    scaler.fit(X_train)  # still need to fit, though it does not change...\n",
    "\n",
    "scaler   # is now defined and ready to use...\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Here are our scaled training and testing sets:\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train) # scale!\n",
    "X_test_scaled = scaler.transform(X_test) # scale!\n",
    "\n",
    "y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "y_test_scaled = y_test  # not using the scaler\n",
    "    \n",
    "ascii_table(X_train_scaled,y_train_scaled)\n",
    "\n",
    "#\n",
    "# Note that the zeros have become -1's\n",
    "# and the 1's have stayed 1's\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# import our NNet library (within scikit-learn)\n",
    "#\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#\n",
    "# Here's where you can change the number of layers, neurons, and other parameters:\n",
    "#\n",
    "nn_classifier = MLPClassifier(hidden_layer_sizes=(6,7),  # 3 input -> 6 -> 7 -> 1 output\n",
    "                    max_iter=500,      # how many times to train\n",
    "                    activation=\"tanh\", # the \"activation function\" input -> output\n",
    "                    solver='sgd',      # the algorithm for optimizing weights\n",
    "                    verbose=True,      # False to \"mute\" the training\n",
    "                    shuffle=True,      # reshuffle the training epochs?\n",
    "                    random_state=None, # set for reproduceability\n",
    "                    learning_rate_init=.1,       # learning rate: % of error to backprop\n",
    "                    learning_rate = 'adaptive')  # soften feedback as it converges\n",
    "\n",
    "# documentation:\n",
    "# scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html \n",
    "#     Try verbose / activation \"relu\" / other network sizes ...\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_classifier.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"\\n++++++++++  TRAINING:   end  +++++++++++++++\")\n",
    "print(f\"The analog prediction error (the loss) is {nn_classifier.loss_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# what can we see about our nnet?\n",
    "#\n",
    "#\n",
    "# how did it do on the training data?   (It's the same as the testing data, in this case!)\n",
    "#\n",
    "\n",
    "#\n",
    "# which one do we want: classifier or regressor?\n",
    "#\n",
    "\n",
    "def ascii_table_for_classifier(Xsc,y,nn,scaler):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(Xsc)            # all predictions\n",
    "    prediction_probs = nn.predict_proba(Xsc) # all prediction probabilities\n",
    "    Xpr = scaler.inverse_transform(Xsc)      # Xpr is the \"X to print\": unscaled data!\n",
    "    # count correct\n",
    "    num_correct = 0\n",
    "    # printing\n",
    "    print(f\"{'input ':>18s} -> {'pred':^6s} {'des.':^6s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        pred_probs = prediction_probs[i,:]\n",
    "        desired = y[i]\n",
    "        if pred != desired: result = \"  incorrect: \" + str(pred_probs)\n",
    "        else: result = \"  correct\"; num_correct += 1\n",
    "        # Xpr = Xsc  # if you want to see the scaled versions\n",
    "        print(f\"{Xpr[i,:]!s:>18s} -> {pred:^6.0f} {desired:^6.0f} {result:^10s}\") \n",
    "    print(f\"\\ncorrect predictions: {num_correct} out of {len(y)}\")\n",
    "    \n",
    "\n",
    "\n",
    "#\n",
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_classifier(X_test_scaled,\n",
    "                           y_test_scaled,\n",
    "                           nn_classifier,\n",
    "                           scaler)   \n",
    "#\n",
    "# other things...\n",
    "#\n",
    "nn = nn_classifier  # less to type?\n",
    "print(\"\\n\\n+++++ parameters, weights, etc. +++++\\n\")\n",
    "print(f\"\\nweights/coefficients:\\n\")\n",
    "for wts in nn.coefs_:\n",
    "    print(wts)\n",
    "print(f\"\\nintercepts: {nn.intercepts_}\")\n",
    "print(f\"\\nall parameters: {nn.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# we have a predictive model!  Let's try it out...\n",
    "#\n",
    "\n",
    "def make_prediction( Features, nn, scaler ):\n",
    "    \"\"\" uses nn for predictions \"\"\"\n",
    "    print(\"input features are\", Features)\n",
    "    #  we make sure Features has the right shape (list-of-lists)\n",
    "    row = np.array( [Features] )  # makes an array-row\n",
    "    row = scaler.transform(row)   # scale according to scaler\n",
    "    print(\"nn.predict_proba == \", nn.predict_proba(row))   # probabilities of each\n",
    "    prediction = nn.predict(row)  # max!\n",
    "    return prediction\n",
    "    \n",
    "# our features -- note that the inputs don't have to be bits!\n",
    "Features = [ 1, 0, 1 ]      # whatever we'd like to test\n",
    "prediction = make_prediction(Features, nn_classifier, scaler)\n",
    "print(f\"prediction: {prediction}\")   # just takes the max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### From classification to regression\n",
    "+ NNets are more natural regressors than classifiers...\n",
    "+ That is, they naturally output continuous, floating-point values\n",
    "+ ... instead of a category or choice-among-labels.\n",
    "+ So, let's try to predict our binary function as a floating point output instead.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# MLPRegressor predicts _floating-point_ outputs\n",
    "#\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "nn_regressor = MLPRegressor(hidden_layer_sizes=(6,7), \n",
    "                    max_iter=200,          # how many training epochs\n",
    "                    activation=\"tanh\",     # the activation function\n",
    "                    solver='sgd',          # the optimizer\n",
    "                    verbose=True,          # do we want to watch as it trains?\n",
    "                    shuffle=True,          # shuffle each epoch?\n",
    "                    random_state=None,     # use for reproducibility\n",
    "                    learning_rate_init=.1, # how much of each error to back-propagate\n",
    "                    learning_rate = 'adaptive')  # how to handle the learning_rate\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_regressor.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"++++++++++  TRAINING:   end  +++++++++++++++\")\n",
    "\n",
    "print(f\"The (squared) prediction error (the loss) is {nn_regressor.loss_}\")\n",
    "print(f\"And, its square root: {nn_regressor.loss_ ** 0.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# how did it do? now we're making progress (by regressing)\n",
    "#\n",
    "\n",
    "def ascii_table_for_regressor(Xsc,y,nn,scaler):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(Xsc) # all predictions\n",
    "    Xpr = scaler.inverse_transform(Xsc)  # Xpr is the \"X to print\": unscaled data!\n",
    "    # measure error\n",
    "    error = 0.0\n",
    "    # printing\n",
    "    print(f\"{'input ':>18s} ->  {'pred':^6s}  {'desr':^6s}  {'absdiff':^10s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        desired = y[i]\n",
    "        result = abs(desired - pred)\n",
    "        error += result\n",
    "        # Xpr = Xsc   # if you'd like to see the scaled values\n",
    "        print(f\"{Xpr[i,:]!s:>18s} ->  {pred:<+6.3f}  {desired:<+6.3f}  {result:^10.3f}\") \n",
    "    print(f\"\\naverage abs error: {error/len(y)}\")\n",
    "    \n",
    "#\n",
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_regressor(X_test_scaled,\n",
    "                          y_test_scaled,\n",
    "                          nn_regressor,\n",
    "                          scaler)   # this is our own f'n, above\n",
    "#\n",
    "# other things...\n",
    "#\n",
    "nn = nn_regressor  # less to type?\n",
    "print(\"\\n\\n+++++ parameters, weights, etc. +++++\\n\")\n",
    "print(f\"\\nweights/coefficients:\\n\")\n",
    "for wts in nn.coefs_:\n",
    "    print(wts)\n",
    "print(f\"\\nintercepts: {nn.intercepts_}\")\n",
    "print(f\"\\nall parameters: {nn.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# again, we have a predictive model, now a regressor.  Let's try it out...\n",
    "#\n",
    "\n",
    "def make_prediction( Features, nn, scaler ):\n",
    "    \"\"\" use a NNet regressor to make a prediction \"\"\"\n",
    "    print(\"input features are\", Features)\n",
    "    row = np.array( [Features] )  # a list-of-lists-style input is needed\n",
    "    row = scaler.transform(row)   # scale!\n",
    "    prediction = nn.predict(row)\n",
    "    print(\"nn.predict(row) == \", prediction)\n",
    "    return prediction\n",
    "    \n",
    "# our features\n",
    "Features = [ 1.0, 0.0, 1.0 ]\n",
    "prediction = make_prediction(Features, nn_regressor, scaler)\n",
    "print(f\"prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Welcome to NNets!  \n",
    "# \n",
    "# Onward to the births and iris data.... and beyond!\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
